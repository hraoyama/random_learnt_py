{"cells":[{"cell_type":"markdown","metadata":{"id":"BErSeefeQwQi"},"source":["### Setup packages "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164251,"status":"ok","timestamp":1655373220159,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"L8DGCgVxR2AB","outputId":"ba9caa20-37e0-4aef-edf0-deca3e33815e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12736,"status":"ok","timestamp":1655373232889,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"3xIx5C6UQn4u","outputId":"16bb5d75-6fe9-4969-ae24-63cd67c577ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting progressbar\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","Building wheels for collected packages: progressbar\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=717a8f58bfc055f82b5299e86c4c2f7e313be3f22e33bf6a5275d416d236120e\n","  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n","Successfully built progressbar\n","Installing collected packages: progressbar\n","Successfully installed progressbar-2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.2.0)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.3.1\n","time: 142 µs (started: 2022-06-16 09:53:52 +00:00)\n"]}],"source":["%pip install progressbar\n","%pip install plotnine\n","%pip install torch\n","%pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1912,"status":"ok","timestamp":1655373234796,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"EfIU_eNp3Zio","outputId":"cb735600-533b-4de8-e89b-247e4aaadf5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.76 s (started: 2022-06-16 09:53:52 +00:00)\n"]}],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2661,"status":"ok","timestamp":1655373237452,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ZmUjYbArAuQT","outputId":"5164faa9-5bb9-40c6-8df7-7432c2d0ef4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.66 s (started: 2022-06-16 09:53:54 +00:00)\n"]}],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655373237453,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"BI4p7ZKb0Qz2","outputId":"49461b44-1f53-467b-adac-5fe36827948f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 898 µs (started: 2022-06-16 09:53:57 +00:00)\n"]}],"source":["paper_name = \"dgm_hjb\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1655373238047,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"433z6V3T2rB2","outputId":"b424f74a-70fa-4e98-d5de-7b4fe306ddc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 580 ms (started: 2022-06-16 09:53:57 +00:00)\n"]}],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655373238047,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"uat0pG8aR3Rh","outputId":"e27ba2a0-019f-45a9-dbf8-5d0b75120ca3"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 8.46 ms (started: 2022-06-16 09:53:57 +00:00)\n"]}],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3194,"status":"ok","timestamp":1655373241239,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"KpFjo3MkLus9","outputId":"31f25814-1ed1-4c49-eb89-5077f168a9d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.92 s (started: 2022-06-16 09:53:57 +00:00)\n"]}],"source":["import torch \n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from scipy.stats import norm\n","from matplotlib import cm\n","import pdb\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1862,"status":"ok","timestamp":1655373243098,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"CbfN42gpGZhC","outputId":"42c69b81-20fe-4735-de95-9c4fd18c5ec1"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.07 s (started: 2022-06-16 09:54:00 +00:00)\n"]}],"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","from pprint import pprint as pp"]},{"cell_type":"markdown","metadata":{"id":"bvy0WvxDGCxk"},"source":["### Shared functions across models"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1655373266266,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"cpVaz5dwXZNq","outputId":"5ab32354-30e7-4ed5-f44c-0042a6e9290c"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 22.1 ms (started: 2022-06-16 09:54:25 +00:00)\n"]}],"source":["import pandas as pd\n","\n","def plot_report(train_instance):\n","        \n","    history_tl_cpu = [ x for x in train_instance.history_tl ]\n","    history_internal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_internal_cpu ]\n","    history_terminal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_terminal ]\n","    history_initial_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_initial ]\n","    history_nonzero_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_nonzero ]\n","\n","    obs_data = pd.DataFrame({\"Epochs\" : [ (x+1)*train_instance.hook_interval for x in range(len(history_initial_cpu))], \n","                             \"AvgLogLoss\": np.log(history_tl_cpu), \n","                             \"TerminalLogLoss\" :  np.log(history_terminal_cpu),\n","                             \"InternalLogLoss\" :  np.log(history_internal_cpu),\n","                             \"InitialLogLoss\" : np.log(history_initial_cpu),\n","                             \"NonZeroLogLoss\" : np.log(history_nonzero_cpu),\n","                             })\n","\n","    return (ggplot(obs_data, aes(\"Epochs\",\"AvgLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"TerminalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InternalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InitialLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"NonZeroLogLoss\")) + geom_line() + geom_point(),\n","            )\n","\n","def plot_activation_mean(train_instance):\n","    \n","    # pdb.set_trace()\n","\n","    if train_instance.debug == False:\n","        print( 'error: debug is off , turn it on and train again ' )\n","    else:\n","        history = np.array(train_instance.history_mean_hooks)\n","        jet= plt.get_cmap('jet')\n","        colors = iter(jet(np.linspace(0,1,10)))\n","        fig, ax = plt.subplots()\n","        for i in range(history.shape[1]):\n","            ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n","        fig.suptitle('Layers activation mean value', fontsize=10)\n","        leg = ax.legend();\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655373266802,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"QMAuMqdgU9kL","outputId":"d18d9448-e2bb-491b-8913-199208932cc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.03 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["# plot_report(train)\n","# plot_activation_mean(train)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655373266803,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"sCV-yFDXUV4J","outputId":"470b27bc-e395-409c-edcc-4dfc01ad2580"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 862 µs (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["# print( 'Value at 0' , net( torch.tensor( [ 0. , 1. , 1. , 1. ] ).cuda() ) )\n","# #%% save\n","# torch.save(net.state_dict(), './model3Assets')\n","# #%%\n","# net = TheModelClass(*args, **kwargs)\n","# net.load_state_dict(torch.load('./modelmodel3Assets'))\n","# net.eval()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655373266803,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ONB5NopRa3fD","outputId":"21cb8020-1426-49fc-b46a-17264df5c869"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 31.9 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n","from scipy.optimize import LinearConstraint, NonlinearConstraint\n","from scipy.optimize import Bounds\n","from functools import partial\n","from scipy.optimize import minimize\n","from functools import wraps\n","\n","def negative(f):\n","    @wraps(f)\n","    def g(*args,**kwargs):\n","        return - f(*args,**kwargs)\n","    # g.__name__ = f'negative({f.__name__})'\n","    return g\n","# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n","\n","# we can add more minimization functions here later (e.g. SS diff)\n","def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n","  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n","  input = torch.tensor(varX*loss_terms)\n","  loss_pointwise = target * (torch.log(target) - torch.log(input))\n","  if reduction == \"mean\":  # default\n","      loss = loss_pointwise.mean()\n","  elif reduction == \"batchmean\":  # mathematically correct\n","      loss = loss_pointwise.sum() / input.size(0)\n","  elif reduction == \"sum\":\n","      loss = loss_pointwise.sum()\n","  else:  # reduction == \"none\"\n","      loss = loss_pointwise  \n","  return loss\n","\n","  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n","\n","def minimize_weights_st(loss_terms, loss_func):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  x0 = [0.25]*len(loss_terms)\n","  res = minimize( partial(loss_func, loss_terms=loss_terms), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n","\n","def maximizeloss_weights_st(loss_terms, loss_func, eps):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","  # even though zero is the KL minimum it helps to put a negative number here to explore\n","\n","  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n","  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint, nonlinear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":497,"status":"ok","timestamp":1655373267297,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"RM0IVdZ_TXW3","outputId":"617cc21a-20ea-4b18-a7bb-bea8b7862d8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.33334757 0.33333761 0.33331482]\n","time: 74.4 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n","print(r1.x)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655373267298,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ewko67bDIcz9","outputId":"602f924b-d09f-4a41-ddce-69ae5b48a111"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.61 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["### There is an issue getting this to work because of nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","\n","    # def calculateLossAdaptWeights(self , size = 2**8 , train = True, min_max = True):\n","    #     '''\n","    #     Helper function that Sample and Calculate loss,\n","    #     This is adapted in that it changes the weights on the losses to maximize the loss provided\n","    #     the KL distance of the new weighting is within self.eps of the previous distribution (starting at equally weighted)\n","    #     '''        \n","    #     x , x_terminal , x_boundary = self.sample(size)\n","    #     x = Variable( x , requires_grad=True)\n","    #     Ls = self.criterion( x , x_terminal , x_boundary )\n","    #     DO , TC , BC = Ls\n","    #     DOm = torch.mean(DO).detach().cpu().float().item()\n","    #     TCm = torch.mean(TC).detach().cpu().float().item()\n","    #     BCm = torch.mean(BC).detach().cpu().float().item()\n","\n","    #     losses_for_reweighting = [ torch.mean(lv).detach().cpu().float().item() for lv in Ls if list(lv.size())] \n","    #     mask_for_available_losses = [ True if list(lv.size()) else False for lv in Ls ]\n","\n","    #     # print([ DOm, TCm, BCm])\n","    #     # if is.nan(DOm):\n","    #     #   print(DO)\n","\n","    #     if self.weights is None:\n","    #       self.weights = torch.ones(1,len(Ls))/len(Ls)\n","\n","    #     # pdb.set_trace()\n","\n","    #     if min_max:\n","    #         r1 = maximizeloss_weights_st( losses_for_reweighting , KLDiffHere, self.eps)\n","    #         candidate_weigths = torch.zeros_like(self.weights).to(torch.device(\"cuda:0\"))\n","    #         candidate_weigths[0][mask_for_available_losses] = torch.tensor(r1.x).to(torch.device(\"cuda:0\")).float()\n","    #         self.weights = candidate_weigths.to(torch.device(\"cuda:0\"))\n","    #         self.weights_tbl.append(self.weights.detach().cpu().numpy())\n","\n","    #     numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","    #     if train == True:\n","    #         return  (self.weights[0,0]*torch.mean(DO) + \n","    #                  self.weights[0,1]*torch.mean(TC) + \n","    #                  self.weights[0,2]*torch.mean(BC)) , \\\n","    #                  self.weights[0,0]*torch.mean(DO) , \\\n","    #                  self.weights[0,1]*torch.mean(TC) , \\\n","    #                  self.weights[0,2]*torch.mean(BC) , \\\n","    #                  (1./numActive*torch.mean(DO) + \n","    #                  1./numActive*torch.mean(TC) + \n","    #                  1./numActive*torch.mean(BC))             \n","    #     else:\n","    #         return  DO , TC , BC\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655373267298,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"iyacROFeXgNp","outputId":"005eba89-44ca-485d-85ad-5eecea2ca347"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 7.81 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["import torch\n","from torch.distributions import Normal\n","\n","std_norm_cdf = Normal(0, 1).cdf\n","std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n","\n","def bs_price(right, K, S, T, sigma, r):\n","    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n","    d_2 = d_1 - sigma * torch.sqrt(T)\n","    \n","    if right == \"C\":\n","        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n","        return C\n","        \n","    elif right == \"P\":\n","        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n","        return P"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655373267298,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"sLsA5AvqpMM7","outputId":"5e64e7ed-5002-4846-94c6-ccd883c67d65"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.8 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["import torch\n","\n","def to_cpu_detach(x):\n","  if isinstance(x, list):\n","    return [ y.detach().cpu().item() for y in x ]\n","  else:\n","    return x.detach().cpu().item()"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655373267299,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"PC-E2SeX46A9","outputId":"95fbff3d-45d9-4ad6-ea0b-e6d58dc4d6e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.2 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["def huber_loss_zero_target(x, delta = 1.0):\n","  loss_function = torch.nn.HuberLoss(delta=delta)\n","  return loss_function(x, torch.zeros_like(x))\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655373267299,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"MNYJyHWpeL66","outputId":"8d6839fa-f047-4c3c-805e-7275a616b928"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 20.3 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_wgamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655373267300,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ipogSsVTbv0k","outputId":"0258b6eb-aff4-411d-840c-28655413d59d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 26.5 ms (started: 2022-06-16 09:54:26 +00:00)\n"]}],"source":["def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        xbreaks = getattr(eqObject,\"xbreaks\")\n","        xbreaks_str = str(len(xbreaks))\n","        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        tbreaks = getattr(eqObject,\"tbreaks\")\n","        tbreaks_str = str(len(tbreaks))\n","        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"markdown","metadata":{"id":"Tz5tUJuYaXKu"},"source":["### Merton Invest-Consumption Problem - Equation HJB optimization\n","\n","[Extensions of the Deep Galerkin Method](https://arxiv.org/pdf/1912.01455v3.pdf)"]},{"cell_type":"markdown","metadata":{"id":"N-GO35FcJPP6"},"source":["##### Closed form terminal utility functions"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655373267587,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"0cWoRXs02PoF","outputId":"4aad7956-5a25-4511-c445-0c3373276835"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.23 ms (started: 2022-06-16 09:54:27 +00:00)\n"]}],"source":["def expTerminalUtilityOfWealth(x, gamma_discount = 1):\n","  return(-1*torch.exp(-gamma_discount*x))\n","\n","def expTerminalUtilityOfWealth_np(x, gamma_discount = 1):\n","  return(-np.exp(-gamma_discount*x))\n","\n","from functools import partial\n","\n","# should give a closed form solution for the control => PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))"]},{"cell_type":"markdown","metadata":{"id":"HrivvbmubiiY"},"source":["#### MertonUtilityNet"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655373268730,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"JRraqOG4aXKx","outputId":"ac092b8a-1b83-4254-a5f2-e5a690029ac4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 13.3 ms (started: 2022-06-16 09:54:28 +00:00)\n"]}],"source":["class MertonUtilityNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh):\n","        super(MertonUtilityNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5 + 1  # wealth, time, mu, r, sigma, pi\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act(self.fc_input(x))\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out "]},{"cell_type":"markdown","metadata":{"id":"fyFbPZr7I5RE"},"source":["#### MertonPiNet"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1655373270404,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"PR7PHL4KI1S9","outputId":"fdeb7d63-0ddf-4646-eea5-0718f5f99d17"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 15.5 ms (started: 2022-06-16 09:54:30 +00:00)\n"]}],"source":["import torch.nn.functional as F\n","\n","class MertonPiNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.relu  ):\n","        super(MertonPiNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5   # wealth, time, mu, r, sigma\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)            \n","        # self.fc_output_d = nn.Linear(self.NN, 2)\n","        # self.fc_output = torch.nn.Softmax(dim=1)\n","        self.fc_output = nn.Linear(self.NN, 1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        # out = self.fc_output_d(h)\n","        out = self.fc_output(h)\n","        return out \n","        "]},{"cell_type":"markdown","metadata":{"id":"RNhAbZ727RC_"},"source":["#### MertonAlternativePiNet\n","\n","[implement from github](https://github.com/Plemeur/DGM/blob/master/first_net.py)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1655373272481,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"_c-dZ5b37NwV","outputId":"4315c0e3-e7eb-4bab-ede6-5c838e932fbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 90.6 ms (started: 2022-06-16 09:54:32 +00:00)\n"]}],"source":["class LinearWithXavier(nn.Module):\n","    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n","        TODO : figure out what to do with the bias\"\"\"\n","    def __init__(self, in_features, out_features, bias=True, batch_normalize=True):\n","        super(LinearWithXavier, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.batch_normalize = batch_normalize\n","        \n","        if self.batch_normalize == True:\n","          self.batch_norm = torch.nn.BatchNorm1d(out_features)\n","        \n","        if bias:\n","            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","    \n","    def reset_parameters(self):\n","        torch.nn.init.xavier_uniform_(self.weight)\n","        if self.bias is not None:\n","            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n","    \n","    def forward(self, input):\n","        if self.batch_normalize == True:\n","          return self.batch_norm(torch.nn.functional.linear(input, self.weight, self.bias))\n","        return torch.nn.functional.linear(input, self.weight, self.bias)\n","    \n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n","\n","\n","class DGM_layer(nn.Module):\n","    \"\"\" See readme for paper source\"\"\"\n","    def __init__(self, in_features, out_feature, residual=False, batch_normalize=False):\n","        super(DGM_layer, self).__init__()\n","        self.residual = residual\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.G = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.R = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.H = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","\n","    def forward(self, x, s):\n","        z = torch.tanh(self.UZ(x) + self.Z(s))\n","        g = torch.tanh(self.UG(x) + self.G(s))\n","        r = torch.tanh(self.UR(x) + self.R(s))\n","        h = torch.tanh(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonAlternativePiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonAlternativePiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n","        # self.middle_layer_2 = nn.ModuleList([DGM_layer(in_size, neurons, batch_normalize=False) for i in range(2)])\n","        self.final_layer = LinearWithXavier(neurons, out_size, batch_normalize=False)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","        \n","        # for i, layer in enumerate(self.middle_layer_2):\n","        #     s = torch.tanh(layer(X, s))\n","        \n","        # s = torch.nn.functional.gelu(self.input_layer(X))\n","        # for i, layer in enumerate(self.middle_layer):\n","        #     s = torch.nn.functional.elu(layer(X, s))\n","        # for i, layer in enumerate(self.middle_layer):\n","        #     s = torch.nn.functional.gelu(layer(X, s))\n","        # for i, layer in enumerate(self.middle_layer_2):\n","        #     s = torch.nn.functional.gelu(layer(X, s))\n","\n","        return self.final_layer(s)\n","        # return torch.pow(self.final_layer(s), 1)\n"]},{"cell_type":"markdown","metadata":{"id":"CNsqOm1ithSG"},"source":["#### MertonMatchPiNet\n","\n","[Matching Paper by hand](https://arxiv.org/abs/1912.01455v3)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1655373274989,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"REWml4aYthSH","outputId":"2d6a0727-d763-44d3-88ae-82dcab3bfc35"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 26.9 ms (started: 2022-06-16 09:54:34 +00:00)\n"]}],"source":["class DGMLayerPaper(nn.Module):\n","\n","    def __init__(self, in_features, out_feature, activation=torch.relu, residual=False):\n","        \n","        super(DGMLayerPaper, self).__init__()\n","        self.residual = residual\n","        self.activation = activation\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature) # w.S\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=True) # u.x\n","        self.G = LinearWithXavier(out_feature, out_feature)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.R = LinearWithXavier(out_feature, out_feature)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.H = LinearWithXavier(out_feature, out_feature) # w.(S(o)R)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=True)\n","\n","    def forward(self, x, s):\n","        z = self.activation(self.UZ(x) + self.Z(s))\n","        g = self.activation(self.UG(x) + self.G(s))\n","        r = self.activation(self.UR(x) + self.R(s))\n","        h = self.activation(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonMatchPiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonMatchPiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGMLayerPaper(in_size, neurons) for i in range(depth)])\n","        self.final_layer = LinearWithXavier(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","\n","        return self.final_layer(s)\n"]},{"cell_type":"markdown","metadata":{"id":"wClW1g9rbm8o"},"source":["#### PiEquation"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655373278847,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"uQTTkUhWuiJi","outputId":"82050e80-ae16-4cb2-cb97-2bc273a38074"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 40.2 ms (started: 2022-06-16 09:54:38 +00:00)\n"]}],"source":["class PiEquation():\n","\n","    def __init__(self , pi_net, du_dx, d2u_dx2):\n","        self.pi_net = pi_net\n","        self.wgamma = 0.0001\n","        self.du_dx = Variable(du_dx, requires_grad=False)\n","        self.d2u_dx2 = Variable(d2u_dx2, requires_grad=False)\n","\n","    def criterion(self, x_internal, grads):\n","      #  time, wealth, mu, r, sigma\n","      du_dt = Variable(grads[0], requires_grad=False)\n","      du_dx = Variable(grads[1], requires_grad=False)\n","      d2u_dx2 = Variable(grads[2], requires_grad=False)\n","\n","      pi_net_preds = self.pi_net(x_internal)\n","      pi_net_preds = pi_net_preds.reshape(-1,1)\n","      \n","      x_internal_const = Variable(x_internal.clone(), requires_grad=False)\n","      \n","      intC = None\n","      # pdb.set_trace()\n","      if len(x_internal) == 0:\n","        intC_loss = torch.tensor(0).cuda().float()  \n","      else:\n","        # pdb.set_trace()\n","        \n","        # pi * (mu-r)\n","        loss_1 = pi_net_preds*(x_internal_const[:,2].reshape(-1,1) - x_internal_const[:,3].reshape(-1,1))\n","        \n","        # r * wealth\n","        loss_2 = x_internal_const[:,3].reshape(-1,1)*x_internal_const[:,1].reshape(-1,1)\n","        \n","        # sigma^2 * pi^2\n","        loss_3 = (x_internal_const[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n","        \n","        part_a = (loss_1 +loss_2)*du_dx\n","        part_b =  0.5*loss_3*d2u_dx2\n","        \n","        # err1 = torch.abs((part_a-part_b)/(part_a+1e-7) )\n","        # err2 = torch.abs((part_a-part_b)/(part_b+1e-7) )\n","        # terC = torch.where(torch.abs(terminal_target) < torch.abs(util_net_pred), err2, err1)\n","\n","        # intC_loss = torch.abs(intC_loss - du_dt)/0.1\n","\n","        intC_loss = part_a + part_b\n","        # # r*x\n","        # loss_1 = x_internal_const[:,3].reshape(-1,1) * x_internal_const[:,1].reshape(-1,1)\n","\n","        # # (mu - r)^2/sigma^2\n","        # loss_2 = ((x_internal_const[:,2].reshape(-1,1) - x_internal_const[:,3].reshape(-1,1))**2) / (x_internal_const[:,4].reshape(-1,1)**2)\n","\n","      return  -1.0*intC_loss\n","\n","    def calculatePiLoss(self, x_internal, grads, keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        x_internal = Variable( x_internal , requires_grad=True)\n","        Ls = self.criterion( x_internal, grads)\n","        \n","        return_losses = []\n","        \n","        if not keep_batch:\n","          loss_pi = torch.mean(Ls)           \n","          return loss_pi          \n","        else:\n","          return Ls\n"]},{"cell_type":"markdown","metadata":{"id":"8RRoBgFQINMv"},"source":["#### TrainInternalPiWithDGM\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1655373279960,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"VREn3fanpP1b","outputId":"157c99b5-bda7-4d69-adf3-08e0d8bc3bdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 148 ms (started: 2022-06-16 09:54:39 +00:00)\n"]}],"source":["class TrainInternalPiWithDGM():\n","    \n","    def __init__(self , u_equation, pi_equation, pi_equation_traget, BATCH_SIZE , epoch, lr, debug = False, loss_multiply = 1.0):\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.u_model = u_equation        \n","        \n","        self.pi_model = pi_equation\n","        self.pi_model_traget = pi_equation_traget\n","        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.pi_model.pi_net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","\n","        self.epoch = epoch\n","        self.lr = lr\n","        self.optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n","        self.loss_multiply = loss_multiply\n","\n","    def get_utility_function_derivatives(self, u_net_val, x_internal, normalize=False):\n","        du = torch.autograd.grad( u_net_val, \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(u_net_val),\n","                                  create_graph=True,\n","                                  retain_graph=True)\n","        \n","        du_dt = du[0][:,0].reshape(-1,1)\n","        du_dx = du[0][:,1].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(du_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(du_dx),\n","                                      retain_graph=True\n","                                      )[0][:,1].reshape(-1,1)\n","\n","        return du_dt, du_dx, d2u_dx2 + 1e-7\n","        \n","    def train(self , eqLossFn = 'calculatePiLoss', sample_method_X = \"U\"):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((self.epoch, 3 ), dtype='float32') * np.nan\n","        \n","        self.train_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer\n","        \n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        \n","        try:\n","            loss_calc_method = getattr(self.pi_model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(self.epoch):\n","            sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            loss_avg = 0.0\n","            # pdb.set_trace()\n","            x_internal = Variable(sample_batch[0], requires_grad=True)\n","            x_internal2 =  Variable(sample_batch[0], requires_grad=True)\n","            \n","            utility_net_val = self.u_model.u_net(x_internal)\n","            du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(utility_net_val, x_internal)\n","            \n","            loss  = loss_calc_method(x_internal2, [du_dt, du_dx, d2u_dx2], keep_batch = False )            \n","            # print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) ]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              self.validation_losses[e,:] = validation_loss_list\n","            \n","            if self.use_early_stop:\n","              loss_to_check = loss\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            optimizer.zero_grad()\n","            loss_avg = loss_avg + float(loss.item())\n","\n","            # print('self.epoch:', e)\n","            loss.backward()\n","            # for param in self.pi_model.pi_net.parameters():\n","            #   print('pi grad', param.grad)\n","            optimizer.step()\n","            \n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                # print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(e , self.lr , loss))\n","\n","        self.stop_epoch = e\n","        self.pi_model_traget.pi_net.load_state_dict(self.pi_model.pi_net.state_dict())\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n"]},{"cell_type":"markdown","metadata":{"id":"hp4BG1ewKF6o"},"source":["#### MertonEquation"]},{"cell_type":"code","execution_count":28,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1177,"status":"ok","timestamp":1655373281850,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"LBMZYQSPaXKy","outputId":"be0e3b92-aaa5-42b9-9225-77dde16a3835"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.07 s (started: 2022-06-16 09:54:40 +00:00)\n"]}],"source":["import math\n","\n","class MertonEquation():\n","    \n","    def __init__(self , u_net, pi_net, pi_net_target, pi_net_epoch, pi_net_lr, term_utility_function = partial(expTerminalUtilityOfWealth, gamma_discount=1)):\n","\n","        self.u_net = u_net\n","        self.pi_net = pi_net\n","        self.pi_net_target = pi_net_target\n","\n","        self.wgamma = 0.0001\n","        self.term_utility_func = term_utility_function\n","        self.xbreaks = None\n","        self.tbreaks = None\n","\n","        self.MAX_X = 1.0\n","        self.T = 1.0\n","        self.MAX_MU = 0.2\n","        self.MAX_SIGMA = 1.0\n","\n","        self.pi_net_epoch = pi_net_epoch\n","        self.pi_net_lr = pi_net_lr\n","        self.loss_multiply = 1.0\n","\n","        self.FORCE_MU = None\n","        self.FORCE_R = None\n","        self.FORCE_SIGMA = None\n","\n","        self.pi_net.train()\n","\n","    def g(self,x):\n","        # Time, Wealth, Mu, R, Sigma\n","        return self.term_utility_func(x[:,1].reshape(-1,1))\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def mu_r_sample(self, size, range_multiplier = 1.0):\n","      mu_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      r_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      \n","      # return (mu_candidate, r_candidate)\n","\n","      r_sample = torch.where(r_candidate < mu_candidate, r_candidate, mu_candidate)\n","      mu_sample = torch.where(r_candidate > mu_candidate, r_candidate, mu_candidate)\n","      return (mu_sample, r_sample)\n","\n","    def apply_forced_mu_r_sigma(self, mu_sample, r_sample, sigma_sample):\n","      if self.FORCE_MU is not None:\n","         mu_sample = self.FORCE_MU*torch.ones_like(mu_sample)            \n","      if self.FORCE_R is not None:\n","        r_sample = self.FORCE_R*torch.ones_like(r_sample)\n","      if self.FORCE_SIGMA is not None:\n","        sigma_sample = self.FORCE_SIGMA*torch.ones_like(sigma_sample)\n","      return mu_sample, r_sample, sigma_sample\n","\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","        '''\n","        Sampling function\n","        '''\n","        if sample_method_X in [\"U\"]:\n","            range_multiplier = 1.0\n","            \n","            ### internal samples of Time, Wealth, Mu, R, Sigma\n","            mu_sample_internal, r_sample_internal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_internal, r_sample_internal, sigma_sample_internal = self.apply_forced_mu_r_sigma(mu_sample_internal, r_sample_internal, sigma_sample_internal)\n","            x_internal = self.to_device(torch.cat(( torch.rand([size,1])*self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_internal, # mu\n","                                                    r_sample_internal, # R\n","                                                    sigma_sample_internal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            ### Terminal time samples\n","            mu_sample_terminal, r_sample_terminal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_terminal, r_sample_terminal, sigma_sample_terminal = self.apply_forced_mu_r_sigma(mu_sample_terminal, r_sample_terminal, sigma_sample_terminal)\n","            x_terminal = self.to_device(torch.cat(( torch.zeros(size, 1) + self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_terminal, # mu\n","                                                    r_sample_terminal, # R\n","                                                    sigma_sample_terminal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            \n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            return x_internal , x_terminal\n","\n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","        \n","    def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","\n","      if self.xbreaks is None and self.tbreaks is None:\n","        return self.sample(sample_method_X, size, to_cpu)\n","\n","      internal_strata_xts = []\n","      terminal_strata_xts = []\n","      \n","      if sample_method_X in [\"U\"]:\n","          range_multiplier = 1.0\n","          xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n","          tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n","          if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n","            xbreaks_used.append(range_multiplier*self.MAX_X)\n","          while xbreaks_used[0] < 0.0:\n","            xbreaks_used.pop(0)\n","          if not xbreaks_used:\n","            xbreaks_used = [0,range_multiplier*self.MAX_X]\n","          if xbreaks_used[0] > 0.0:            \n","            xbreaks_used.insert(0, 0.0)\n","\n","          if tbreaks_used[-1] < self.T:\n","            tbreaks_used.append(self.T)\n","          xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n","          tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n","\n","          total_strat_processed = 0\n","\n","          # internal samples\n","          for stratum_x_count in range(len(xbreaks_used)-1):\n","              \n","            num_samples_in_stratum = 0\n","            if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n","              range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n","              num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n","\n","            for stratum_t_count in range(len(self.tbreaks)-1):\n","\n","              if num_samples_in_stratum == 0: # there is only a T division, so use it\n","                range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n","                num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n","              else:\n","                # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n","                stratum_coverage_on_unit_square = \\\n","                  ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n","                  ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n","                num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n","\n","              range_multiplier = 1.0\n","\n","              ### internal samples of Time, Wealth, Mu, R, Sigma\n","              internal_stratum_t_sample = tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])\n","              internal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_internal, stratum_r_sample_internal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal)\n","              x_internal_stratum = self.to_device(torch.cat(( internal_stratum_t_sample , # Time\n","                                                              internal_stratum_x_sample, # Wealth\n","                                                              stratum_mu_sample_internal, # mu\n","                                                              stratum_r_sample_internal, # R\n","                                                               # Sigma\n","                                                            ) , dim = 1 ),to_cpu)\n","              if not internal_strata_xts: \n","                internal_strata_xts = [ x_internal_stratum ] \n","              else:\n","                internal_strata_xts.append(x_internal_stratum) \n","\n","              ### Terminal time samples\n","              terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal)\n","              x_terminal_stratum = self.to_device(torch.cat(( torch.zeros(num_samples_in_stratum, 1) + self.T , # Time\n","                                                      terminal_stratum_x_sample, # Wealth\n","                                                      stratum_mu_sample_terminal, # mu\n","                                                      stratum_r_sample_terminal, # R\n","                                                      stratum_sigma_sample_terminal # Sigma\n","                                                    ) , dim = 1 ),to_cpu)\n","              if not terminal_strata_xts:\n","                terminal_strata_xts = [ x_terminal_stratum ] # terminal_stratum_xt[None,:,:]\n","              else:\n","                terminal_strata_xts.append(x_terminal_stratum) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n","\n","              total_strat_processed += 1 \n","              # print((len(internal_strata_xts),xbreaks_used[stratum_x_count],tbreaks_used[stratum_t_count]))\n","\n","          # pdb.set_trace()\n","          # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","          return internal_strata_xts , terminal_strata_xts\n","    \n","      raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","\n","    def get_utility_function_derivatives(self, u_net_val, x_internal, normalize=False):\n","        du = torch.autograd.grad( u_net_val, \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(u_net_val),\n","                                  create_graph=True,\n","                                  retain_graph=True)\n","        \n","        du_dt = du[0][:,0].reshape(-1,1)\n","        du_dx = du[0][:,1].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(du_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(du_dx),\n","                                      retain_graph=True\n","                                      )[0][:,1].reshape(-1,1)\n","\n","        return du_dt, du_dx, d2u_dx2 + 1e-7\n","\n","    def criterion(self, x_internal, x_terminal, loss_transforms = [torch.square], util_network=None):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # Time / Wealth / Mu / r / Sigma (sample data order)\n","        # pdb.set_trace()\n","        x_internal_before = x_internal.clone()\n","\n","      \n","        # G(time, wealth, mu, r, sigma, pi)\n","        x_internal =  Variable(x_internal, requires_grad=True)\n","        u_net_val = util_network(x_internal)\n","        du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n","    \n","\n","        pi_model = PiEquation(self.pi_net, du_dx, d2u_dx2)\n","        pi_model_target = PiEquation(self.pi_net_target, None, None)                \n","        \n","        pi_trainer = TrainInternalPiWithDGM(self, pi_model, pi_model_target, \n","                                            x_internal.shape[0], self.pi_net_epoch, \n","                                            self.pi_net_lr, debug=True, loss_multiply=1.0)\n","        pi_trainer.use_early_stop = True\n","        pi_trainer.early_stop_patience = min(200,math.ceil(self.pi_net_epoch/10.0))\n","        pi_trainer.train()\n","        \n","\n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]\n","\n","        intC = None\n","        terC = None\n","\n","        if len(x_internal) == 0:\n","          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          # Time, Wealth, Mu, R, Sigma\n","          # pdb.set_trace()\n","          pi_net_preds = self.pi_net_target(x_internal_before)\n","          pi_net_preds = pi_net_preds.detach().reshape(-1,1)\n","          \n","          x_internal =  Variable(x_internal_before.clone(), requires_grad=True)\n","          \n","          u_net_val = util_network(x_internal)\n","          du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n","\n","          x_internal_const = x_internal.detach().clone()\n","          # x_internal_const = x_internal.clone()\n","          \n","          # pi*(mu-r)\n","          intC_loss_1 = pi_net_preds*(x_internal_const[:,2].reshape(-1,1)-x_internal_const[:,3].reshape(-1,1)) \n","          \n","          # r * wealth\n","          intC_loss_2 =  x_internal_const[:,3].reshape(-1,1)*x_internal_const[:,1].reshape(-1,1)  \n","          \n","          # sigma^2 * pi^2\n","          intC_loss_3 = (x_internal_const[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n","\n","          intC_loss = du_dt + (intC_loss_1 + intC_loss_2)*du_dx + 0.5*intC_loss_3*d2u_dx2\n","\n","          eps = 1e-2\n","          intC_loss = torch.abs((intC_loss))/eps\n","          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n","        \n","        \n","        # print('mu:', x_internal[:,2].reshape(-1,1))\n","        # print('r:', x_internal[:,3].reshape(-1,1))\n","        # print('wealth:', x_internal[:,1].reshape(-1,1))\n","        # print('sigma^2', x_internal[:,4].reshape(-1,1)**2)\n","        # print('pi:', pi_net_preds)\n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        x_terminal_before = x_terminal.clone()\n","        x_terminal =  Variable(x_terminal_before, requires_grad=True)\n","        # print('non terminal pred', u_net_val)\n","        # print('terminal utility pred', util_network(x_terminal))\n","        # print('utility target', self.g(x_terminal))\n","        # print('----------------------')\n","        # print('g terminal:', self.g(x_terminal))\n","        # print('unet:', util_network(x_terminal))\n","        # print('diff:', (self.g(x_terminal)-util_network(x_terminal)) )\n","        # print('perecentage error:', (self.g(x_terminal)-util_network(x_terminal))/(self.g(x_terminal) + 1e-7))\n","        # print('------------------------------')\n","        \n","        terminal_target = self.g(x_terminal)\n","        util_net_pred = util_network(x_terminal)\n","\n","        err1 = torch.abs(torch.abs(util_net_pred-terminal_target)/(util_net_pred+1e-7) )\n","        err2 = torch.abs(torch.abs(util_net_pred-terminal_target)/(terminal_target+1e-7) )\n","        terC = torch.where(torch.abs(terminal_target) < torch.abs(util_net_pred), err2, err1)\n","        terC = terC/0.01\n","        # terC = terminal_target - util_net_pred\n","\n","        terC = [ loss_transform(terC) for loss_transform in loss_transforms ]\n","        \n","        # print('mean d2u_dx2:', d2u_dx2.mean(),'max d2u_dx2:', d2u_dx2.max())\n","        # print('mean du_dx:', du_dx.mean(),'max du_dx:', d2u_dx2.max())\n","        # print('mean du_dt:', du_dt.mean(),'max d2u_dx2:', d2u_dx2.max())\n","        \n","        \n","        return  intC , terC\n","\n","    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False, util_network=None):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        # pdb.set_trace()\n","        x_internal , x_terminal = batch_x\n","        x_internal = Variable( x_internal , requires_grad=True)\n","        x_terminal = Variable( x_terminal , requires_grad=True)\n","        # print('MertonEquation calling self.criterion')\n","        Ls = self.criterion(x_internal , x_terminal, loss_transforms = loss_transforms, util_network=util_network)\n","        intC , terC  = Ls\n","\n","        return_losses = []\n","        # print('internal Loss', torch.mean(intC[0]))\n","        # print('external Loss', torch.mean(terC[0]))\n","\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            loss_equalWeightedByType = torch.mean(intC[lc]) + torch.mean(terC[lc])\n","            return_losses.append( [ loss_equalWeightedByType , \n","                                    torch.mean(intC[lc]) , torch.mean(terC[lc]), \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC.numpy(), terC.numpy()] )\n","        return return_losses\n","\n","    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , loss_transforms = loss_transforms)\n","        intC , terC = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n","            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n","            loss_equalWeightedByType = 100*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt)\n","            return_losses.append( [ transformed_loss , \n","                                    100*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","\n","    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal  = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal, loss_transforms = loss_transforms)\n","        intC , terC  = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n","            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt )\n","            return_losses.append( [ transformed_loss , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"65nooklCbsdy"},"source":["#### TrainHJBMertonWithDGM"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1655373283433,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"74jqGI3o2GlN","outputId":"a84c996f-20b6-4d68-9711-4f6f5e1c0245"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.8 ms (started: 2022-06-16 09:54:43 +00:00)\n"]}],"source":["def attach_pi_used(x, pi_net, requires_grad=True):\n","  pi_used = pi_net(x)  \n","  pi_used.detach_()\n","  # pi_used = pi_used[:,0].reshape(-1,1)\n","  pi_used = pi_used.reshape(-1,1)\n","  \n","  before_x = x.detach().clone()\n","  new_x =  Variable(torch.cat((x, pi_used), dim=1),requires_grad=requires_grad)\n","  return before_x, new_x"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":830,"status":"ok","timestamp":1655373284261,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"OtO8fV7oaXK2","outputId":"ad308f05-cc17-4a0c-c1c2-78b82667f427"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 966 ms (started: 2022-06-16 09:54:43 +00:00)\n"]}],"source":["class TrainHJBMertonWithDGM():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","        \n","        self.net.train()\n","    \n","    def Htx(self, x, gamma=1):\n","      #  wealth * gamma * exp(r*(1-t))\n","      part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1)*(1.0 - x[:,0].reshape(-1,1)))\n","      \n","      #  0.5*(1-t)*[(mu - r)/sigma]^2\n","      part_b = 0.5*(1.0 - x[:,0].reshape(-1,1))* ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n","\n","      return -1.0*torch.exp(-part_a - part_b)\n","\n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\", key_loss_func = torch.square, huber_delta = 0.5):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((epoch, 3*4 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        \n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            # pdb.set_trace()\n","            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            losses_L2, losses_ABS = loss_calc_method(sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False, util_network =self.net )\n","            \n","            # pdb.set_trace()\n","            loss , internal , terminal , losses_equalWeightedByType = losses_L2\n","            loss_abs , internal_abs , terminal_abs , losses_equalWeightedByType_abs = losses_ABS\n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) , to_cpu_detach(internal) , to_cpu_detach(terminal) , \n","                                       to_cpu_detach(loss_abs) , to_cpu_detach(internal_abs) , to_cpu_detach(terminal_abs), \n","                                       to_cpu_detach(losses_equalWeightedByType_abs)]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False,\n","                                                                                                     util_network =self.net)\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss_list = validation_loss_list.pop(5) # the L2 loss is duplicated at index 1\n","              self.validation_losses[e,:] = validation_loss_list\n","              # pdb.set_trace()\n","              # print(f\"Epoch {e} - Pi Pred (0.47) {self.model.pi_net(self.validation_sample[0]).item()}\")\n","\n","            \n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            before_params = list(self.net.parameters())[0].clone()\n","            # print('before_params', before_params)\n","            optimizer.zero_grad()\n","            \n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","            # for param in self.net.parameters():\n","            #   print('grad:', param.grad)\n","            optimizer.step()\n","            after_params = list(self.net.parameters())[0].clone()\n","            # print('after_params', after_params)\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                if (e+1) % 500 == 0:\n","                  print('Updating lr & num epochs')\n","                  self.model.pi_net_epoch += 1\n","                  self.model.pi_net_lr += 0.0001\n","                  for g in optimizer.param_groups:\n","                    g['lr'] *= 0.9\n","                  \n","\n","\n","                loss_avg = loss_avg/self.hook_interval\n","                \n","                sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","                x_internal , x_terminal = sample_batch\n","                \n","                # pdb.set_trace()\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                print(f\"Epoch {e} - lr {lr} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                print('internal loss:', internal , 'terminal loss:', terminal)\n","                \n","\n","                # plot the fitted value function vs the closed form (ideally straight line...)\n","                u_internal_sample = x_internal.clone()\n","                mask = (u_internal_sample[:,0] > 0.1) & (u_internal_sample[:,4] > 0.1)\n","                u_internal_sample = u_internal_sample[mask.reshape(-1),:]\n","                \n","                gamma = 1\n","                time = u_internal_sample[:,0].cpu().detach()\n","                wealth = u_internal_sample[:,1].cpu().detach()\n","                mu = u_internal_sample[:,2].cpu().detach()\n","                r = u_internal_sample[:,3].cpu().detach()\n","                sigma = u_internal_sample[:,4].cpu().detach()\n","                \n","                u_internal_sample = Variable(u_internal_sample, requires_grad=True)\n","                unet_preds = self.net(u_internal_sample)\n","                du_dt, du_dx, d2u_dx2 = self.model.get_utility_function_derivatives(unet_preds, u_internal_sample)\n","                \n","                pi_pred_example = self.model.pi_net(u_internal_sample.detach())\n","                \n","                print('mean pi', pi_pred_example.mean(), 'max pi', pi_pred_example.max(), 'min pi', pi_pred_example.min())\n","                print('mean d2u_dx2:', d2u_dx2.mean(),'max d2u_dx2:', d2u_dx2.max(), 'min d2u_dx2:', d2u_dx2.min())\n","                print('mean du_dx:', du_dx.mean(),'max du_dx:', du_dx.max(), 'min du_dx:', du_dx.min())\n","                print('mean du_dt:', du_dx.mean(),'max d2u_dx2:', du_dt.max(), 'min d2u_dx2:', du_dt.min())\n","\n","\n","                u_net_results = unet_preds.detach().cpu().numpy().reshape(-1).tolist()\n","                htx_results = self.Htx(u_internal_sample, gamma).cpu().detach().numpy().reshape(-1).tolist()\n","                dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n","                \n","                print(ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point())\n","                \n","                dataf = pd.DataFrame( { 'pi_net': self.model.pi_net(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n","                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n","\n","                print(ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point())\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                \n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal = self.validation_sample\n","                    xinternal_before, xinternal_expanded = attach_pi_used(xinternal, self.model.pi_net, requires_grad=False)\n","                    xterminal_before, xterminal_expanded = attach_pi_used(xterminal, self.model.pi_net, requires_grad=False)\n","\n","                    xinternal_res = self.model.u_net(xinternal_expanded).detach()\n","                    xterminal_res = self.model.u_net(xterminal_expanded).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\", \"Mu\", \"R\", \"Sigma\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n","    def train_stratified(self , epoch , lr, \n","                         eqLossFn = 'calculateLoss', \n","                         sample_method_X = \"U\", \n","                         key_loss_func = torch.square, \n","                         huber_delta = 0.5\n","                         ):\n","        \n","        self.validation_losses = np.ones((epoch, 3*3 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1), dtype='float32') * np.nan\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            # pdb.set_trace()\n","            internal_xts_bts, terminal_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n","            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n","            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n","            \n","            # pdb.set_trace()\n","            for stratum_count in range(len(internal_xts_bts)):              \n","              sample_batch = (internal_xts_bts[stratum_count], \n","                              terminal_xts_bts[stratum_count])  \n","\n","              # pdb.set_trace()\n","              stratum_losses_L2, stratum_losses_ABS = loss_calc_method(sample_batch, \n","                                                                       loss_transforms = [ key_loss_func, torch.abs ], \n","                                                                       keep_batch = False )\n","              # if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n","              #   pdb.set_trace()\n","              #   pass\n","            \n","              if training_stratum_losses is not None:\n","                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n","              else:\n","                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n","\n","              # pdb.set_trace()  \n","              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n","\n","            # pdb.set_trace()              \n","            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n","            loss = training_value_to_optimize\n","\n","            loss_optimized , internal , terminal, losses_equalWeightedByType, \\\n","            loss_abs , internal_abs , terminal_abs ,losses_equalWeightedByType_abs = training_loss_for_epoch            \n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n","                loss_calc_method( self.validation_sample, \n","                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                  keep_batch = False )\n","              validation_loss = [*to_cpu_detach(losses_L2_validation),\n","                                              *to_cpu_detach(losses_ABS_validation),\n","                                              *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n","              self.validation_losses[e,:] = validation_loss\n","\n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                print('internal loss:', internal , 'terminal loss:', terminal)\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n"]},{"cell_type":"markdown","metadata":{"id":"oy05I1QFh7EM"},"source":["### Test Case"]},{"cell_type":"markdown","metadata":{"id":"U7zqglm1ewTL"},"source":["#### Test Case NO Stratification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Sl0SptVrlNpTX2-6w2x8mXClBXspDk6X"},"id":"Tf-VoPhf2AJu","outputId":"a504acb2-1d7b-4f9c-9ff5-945dd1af9323"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["seed = 420\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","# u_net, pi_net, pi_net_epoch, pi_net_lr\n","eqLossFn= 'calculateLoss'\n","sample_method= \"U\"\n","lr = 0.02 \n","lr_for_pi = 0.0001\n","max_pi_epochs = 12 # has to be low!!!\n","\n","u_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 20, depth=20 )\n","u_net.to(torch.device(\"cuda:0\")) \n","\n","pi_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 20, depth=10 )\n","pi_net.to(torch.device(\"cuda:0\")) \n","\n","pi_net_target = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 20, depth=10  )\n","pi_net_target.to(torch.device(\"cuda:0\")) \n","mequation = MertonEquation(u_net, pi_net, pi_net_target, max_pi_epochs, lr_for_pi)\n","\n","trainMertonAlloc = TrainHJBMertonWithDGM(u_net, mequation, BATCH_SIZE = 2**9 , debug = False )\n","trainMertonAlloc.hook_interval = 100\n","trainMertonAlloc.use_early_stop = False\n","trainMertonAlloc.early_stop_patience = 1000\n","\n","trainMertonAlloc.train(epoch = 30000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method, key_loss_func=torch.abs)"]},{"cell_type":"markdown","metadata":{"id":"kowxNupDAwOI"},"source":["List of params for successfull run\n","\n","\n","\n","*   Loss = L1\n","*   Util Net + piNet: depth = 3, NN= 50\n","*   lr = 0.005\n","*   lr_for_pi = 0.002\n","*   max_pi_epochs = 5\n","*   trainMertonAlloc.use_early_stop = False\n","*   epoch = 6100\n","*   loss weights = 1\\*intC + 1\\*terminal\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWqq1hGijWvw"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u35g5sAPbsG_"},"outputs":[],"source":["# check control for closed form:\n","# PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))\n","# ((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))\n","# gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   # PI\n","\n","gamma = 1\n","internal_sample, terminal_sample = mequation.sample(size=2000, to_cpu=False)\n","mask = (internal_sample[:,0] > 0.1) & (internal_sample[:,4] > 0.1)\n","internal_sample = internal_sample[mask.reshape(-1),:]\n","# time, wealth, mu, r, sigma\n","time = internal_sample[:,0].cpu().detach()\n","wealth = internal_sample[:,1].cpu().detach()\n","mu = internal_sample[:,2].cpu().detach()\n","r = internal_sample[:,3].cpu().detach()\n","sigma = internal_sample[:,4].cpu().detach()\n","\n","# mequation.pi_net(internal_sample)[:,0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-NmMG33xGBu"},"outputs":[],"source":["# closed form value function\n","def Htx(x, gamma=1):\n","  #  wealth * gamma * exp(r*(1-t))\n","  part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1)*(1.0 - x[:,0].reshape(-1,1)))\n","  \n","  #  0.5*(1-t)*[(mu - r)/sigma]^2\n","  part_b = 0.5*(1.0 - x[:,0].reshape(-1,1))* ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n","\n","  return -1.0*torch.exp(-part_a - part_b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Woj8BoAK5OLC"},"outputs":[],"source":["def Htx(x, gamma=1):\n","  #  wealth * gamma * exp(r*(1-t))\n","  part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1)*(1.0 - x[:,0].reshape(-1,1)))\n","  \n","  #  0.5*(1-t)*[(mu - r)/sigma]^2\n","  part_b = 0.5*(1.0 - x[:,0].reshape(-1,1))* ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n","\n","  return -1.0*torch.exp(-part_a - part_b)\n","  \n","# plot the fitted value function vs the closed form (ideally straight line...)\n","u_internal_sample = torch.cat((internal_sample, mequation.pi_net(internal_sample).reshape(-1,1)), dim=1)\n","u_internal_sample = internal_sample\n","\n","u_net_results = u_net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n","htx_results = Htx(u_internal_sample, gamma).cpu().detach().numpy().reshape(-1).tolist()\n","dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n","ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHfTC6GljYnd"},"outputs":[],"source":["# plot the control function vs the closed form (ideally straight line...)\n","dataf = pd.DataFrame( { 'pi_net': mequation.pi_net(internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n","                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n","\n","temp = dataf[dataf['closed_form'] < 0.1]\n","# plt.scatter(temp['pi_net'], temp['closed_form'])\n","g = ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point()\n","g.draw()\n","# ggplot(temp, aes(x='pi_net', y='closed_form')) + geom_point()\n","\n","\n","# plt.yscale('log')\n","# plt.xscale('log')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f85Kg2zvhMdM"},"outputs":[],"source":["mequation = MertonEquation(MertonUtilityNet( NL = 1 , NN = 3 ), MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 ), 1, 10000.0)\n","# val_sample_to_use = tuple([ x.cpu().detach() for x in mequation.sample(sample_method_X=\"U\", size=1) ] )\n","val_sample_to_use = mequation.sample(sample_method_X=\"U\", size=1) \n","# # gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   \n","val_sample_to_use[0][0,0] = 0.0\n","val_sample_to_use[0][0,2] = 0.05\n","val_sample_to_use[0][0,3] = 0.02\n","val_sample_to_use[0][0,4] = 0.25"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7ce6_TPrsG4"},"outputs":[],"source":["import torch\n","\n","def u(q):\n","  x = q[:,1]\n","  y = q[:, 0]\n","  return x**2 + y**2\n","x = torch.randn(3, requires_grad=True)\n","t = torch.randn(3, requires_grad=True)\n","u_val = u(torch.cat((x, t), axis=1))\n","\n","print(x,t,u_val)\n","print(torch.cat((t,x)))\n","# 1st derivatives\n","dt = torch.autograd.grad(u_val, x, grad_outputs=torch.ones_like(u_val), create_graph=True, allow_unused=True)\n","print(dt[0])\n","dt = torch.autograd.grad(u_val.sum(), torch.cat((t,x)), create_graph=True)\n","print(dt[0])\n","dx = torch.autograd.grad(u_val.sum(), x, create_graph=True)[0]\n","\n","# 2nd derivatives (higher orders require `create_graph=True`)\n","ddx = torch.autograd.grad(dx.sum(), x)[0]\n","ddx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1653673137043,"user":{"displayName":"Parth Pahwa","userId":"09647461209367776624"},"user_tz":-60},"id":"mUOACTjkXBM0","outputId":"985b30f8-3c80-495d-a3b7-c4ed838e3757"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri May 27 17:38:56 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","time: 127 ms (started: 2022-05-27 17:38:56 +00:00)\n"]}],"source":["!nvidia-smi"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["BErSeefeQwQi","bvy0WvxDGCxk","N-GO35FcJPP6","wClW1g9rbm8o","8RRoBgFQINMv","hp4BG1ewKF6o","65nooklCbsdy"],"machine_shape":"hm","name":"DGM_HJB_Sol.ipynb","provenance":[{"file_id":"1CtKlQ93tsZfek5aAcwFvK7L5o5fVISB_","timestamp":1655125250316},{"file_id":"17w9RIhDp3px4HtiqvSe75tPff8Dv2WJP","timestamp":1653656775305},{"file_id":"1pveJg02r9TLc1JY3cSludAQEuDHdEi0m","timestamp":1653413257376},{"file_id":"12X2KXaaeispO6TznFB-M85IhV1NDIolN","timestamp":1653056653625}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}