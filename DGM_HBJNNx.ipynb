{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DGM_HBJNNx.ipynb","provenance":[],"collapsed_sections":["BErSeefeQwQi","bvy0WvxDGCxk","N-GO35FcJPP6","HrivvbmubiiY","fyFbPZr7I5RE","RNhAbZ727RC_","leiCOWL89Mr5","CNsqOm1ithSG","wClW1g9rbm8o","8RRoBgFQINMv","hp4BG1ewKF6o"],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyOKnhdpep1so/VRMgRljMjg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BErSeefeQwQi"},"source":["### Setup packages "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21019,"status":"ok","timestamp":1653240806372,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"L8DGCgVxR2AB","outputId":"214186ff-0ab8-4ea1-8d54-0ce78e8b4f6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11740,"status":"ok","timestamp":1653240818106,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"3xIx5C6UQn4u","outputId":"52bb1c1d-9ba2-454b-91a4-a43e35567218"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting progressbar\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","Building wheels for collected packages: progressbar\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=46346e7a50be556164c38f341c9b64387ab70933faeb1757ee189e84b133965f\n","  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n","Successfully built progressbar\n","Installing collected packages: progressbar\n","Successfully installed progressbar-2.5\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.2.0)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.3.1\n","time: 126 Âµs (started: 2022-05-22 17:33:38 +00:00)\n"]}],"source":["%pip install progressbar\n","%pip install plotnine\n","%pip install torch\n","%pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"EfIU_eNp3Zio","executionInfo":{"status":"ok","timestamp":1653240819854,"user_tz":-60,"elapsed":1752,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd0157ef-fced-4dcb-eee6-a52ff6e728c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.69 s (started: 2022-05-22 17:33:38 +00:00)\n"]}],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ZmUjYbArAuQT","executionInfo":{"status":"ok","timestamp":1653240822505,"user_tz":-60,"elapsed":2653,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b607eb30-cd38-4992-9fec-c104762e399e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.63 s (started: 2022-05-22 17:33:40 +00:00)\n"]}],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BI4p7ZKb0Qz2","executionInfo":{"status":"ok","timestamp":1653240822505,"user_tz":-60,"elapsed":4,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c484c60-96ae-45bf-e028-da5a99194d78"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.34 ms (started: 2022-05-22 17:33:43 +00:00)\n"]}],"source":["paper_name = \"dgm_hjb\""]},{"cell_type":"code","execution_count":7,"metadata":{"id":"433z6V3T2rB2","executionInfo":{"status":"ok","timestamp":1653240822890,"user_tz":-60,"elapsed":387,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d9ed1fe8-c3de-498b-bc6c-822219d95b67"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 562 ms (started: 2022-05-22 17:33:43 +00:00)\n"]}],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uat0pG8aR3Rh","executionInfo":{"status":"ok","timestamp":1653240822890,"user_tz":-60,"elapsed":3,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b592e52-f290-49a1-f921-f4fe06518d0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 7.39 ms (started: 2022-05-22 17:33:43 +00:00)\n"]}],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"KpFjo3MkLus9","executionInfo":{"status":"ok","timestamp":1653240825723,"user_tz":-60,"elapsed":2835,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5b3d2af-a188-463c-b636-3a62d8b3a504"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.72 s (started: 2022-05-22 17:33:43 +00:00)\n"]}],"source":["import torch \n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from scipy.stats import norm\n","from matplotlib import cm\n","import pdb\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"CbfN42gpGZhC","executionInfo":{"status":"ok","timestamp":1653240827773,"user_tz":-60,"elapsed":2052,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad6aa3f2-54c3-4b09-e6bf-3cc04515d2c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.91 s (started: 2022-05-22 17:33:46 +00:00)\n"]}],"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","from pprint import pprint as pp"]},{"cell_type":"markdown","metadata":{"id":"bvy0WvxDGCxk"},"source":["### Shared functions across models"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cpVaz5dwXZNq","executionInfo":{"status":"ok","timestamp":1653240827774,"user_tz":-60,"elapsed":12,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68e6db7b-54fd-4e1c-a8b0-618953851557"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 27.2 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["import pandas as pd\n","\n","def plot_report(train_instance):\n","        \n","    history_tl_cpu = [ x for x in train_instance.history_tl ]\n","    history_internal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_internal_cpu ]\n","    history_terminal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_terminal ]\n","    history_initial_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_initial ]\n","\n","    obs_data = pd.DataFrame({\"Epochs\" : [ (x+1)*train_instance.hook_interval for x in range(len(history_initial_cpu))], \n","                             \"AvgLogLoss\": np.log(history_tl_cpu), \n","                             \"TerminalLogLoss\" :  np.log(history_terminal_cpu),\n","                             \"InternalLogLoss\" :  np.log(history_internal_cpu),\n","                             \"InitialLogLoss\" : np.log(history_initial_cpu)\n","                             })\n","\n","    return (ggplot(obs_data, aes(\"Epochs\",\"AvgLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"TerminalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InternalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InitialLogLoss\")) + geom_line() + geom_point()\n","            )\n","\n","def plot_activation_mean(train_instance):\n","    \n","    # pdb.set_trace()\n","\n","    if train_instance.debug == False:\n","        print( 'error: debug is off , turn it on and train again ' )\n","    else:\n","        history = np.array(train_instance.history_mean_hooks)\n","        jet= plt.get_cmap('jet')\n","        colors = iter(jet(np.linspace(0,1,10)))\n","        fig, ax = plt.subplots()\n","        for i in range(history.shape[1]):\n","            ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n","        fig.suptitle('Layers activation mean value', fontsize=10)\n","        leg = ax.legend();\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QMAuMqdgU9kL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653240827774,"user_tz":-60,"elapsed":11,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"fbb050a4-1dd6-4d1e-abe5-1ffb107f2791"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 671 Âµs (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["# plot_report(train)\n","# plot_activation_mean(train)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"sCV-yFDXUV4J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653240827774,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"48b64c4f-aa9c-433d-8c53-d3c98b456f98"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.23 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["# print( 'Value at 0' , net( torch.tensor( [ 0. , 1. , 1. , 1. ] ).cuda() ) )\n","# #%% save\n","# torch.save(net.state_dict(), './model3Assets')\n","# #%%\n","# net = TheModelClass(*args, **kwargs)\n","# net.load_state_dict(torch.load('./modelmodel3Assets'))\n","# net.eval()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ONB5NopRa3fD","executionInfo":{"status":"ok","timestamp":1653240827775,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d75a5396-ea69-45da-dd26-b67abde3f21f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 30 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n","from scipy.optimize import LinearConstraint, NonlinearConstraint\n","from scipy.optimize import Bounds\n","from functools import partial\n","from scipy.optimize import minimize\n","from functools import wraps\n","\n","def negative(f):\n","    @wraps(f)\n","    def g(*args,**kwargs):\n","        return - f(*args,**kwargs)\n","    # g.__name__ = f'negative({f.__name__})'\n","    return g\n","# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n","\n","# we can add more minimization functions here later (e.g. SS diff)\n","def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n","  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n","  input = torch.tensor(varX*loss_terms)\n","  loss_pointwise = target * (torch.log(target) - torch.log(input))\n","  if reduction == \"mean\":  # default\n","      loss = loss_pointwise.mean()\n","  elif reduction == \"batchmean\":  # mathematically correct\n","      loss = loss_pointwise.sum() / input.size(0)\n","  elif reduction == \"sum\":\n","      loss = loss_pointwise.sum()\n","  else:  # reduction == \"none\"\n","      loss = loss_pointwise  \n","  return loss\n","\n","  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n","\n","def minimize_weights_st(loss_terms, loss_func):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  x0 = [0.25]*len(loss_terms)\n","  res = minimize( partial(loss_func, loss_terms=loss_terms), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n","\n","def maximizeloss_weights_st(loss_terms, loss_func, eps):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-12,eps)\n","  # even though zero is the KL minimum it helps to put a negative number here to explore\n","\n","  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n","  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint, nonlinear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1653240827775,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"RM0IVdZ_TXW3","outputId":"bcf46a4f-3e0c-4ae8-bd94-3347ed2189f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.33334757 0.33333761 0.33331482]\n","time: 72.2 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n","print(r1.x)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ewko67bDIcz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653240827775,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"63a5414e-f947-4f2b-97da-55d4933713d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.6 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["### There is an issue getting this to work because of nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","\n","    # def calculateLossAdaptWeights(self , size = 2**8 , train = True, min_max = True):\n","    #     '''\n","    #     Helper function that Sample and Calculate loss,\n","    #     This is adapted in that it changes the weights on the losses to maximize the loss provided\n","    #     the KL distance of the new weighting is within self.eps of the previous distribution (starting at equally weighted)\n","    #     '''        \n","    #     x , x_terminal , x_boundary = self.sample(size)\n","    #     x = Variable( x , requires_grad=True)\n","    #     Ls = self.criterion( x , x_terminal , x_boundary )\n","    #     DO , TC , BC = Ls\n","    #     DOm = torch.mean(DO).detach().cpu().float().item()\n","    #     TCm = torch.mean(TC).detach().cpu().float().item()\n","    #     BCm = torch.mean(BC).detach().cpu().float().item()\n","\n","    #     losses_for_reweighting = [ torch.mean(lv).detach().cpu().float().item() for lv in Ls if list(lv.size())] \n","    #     mask_for_available_losses = [ True if list(lv.size()) else False for lv in Ls ]\n","\n","    #     # print([ DOm, TCm, BCm])\n","    #     # if is.nan(DOm):\n","    #     #   print(DO)\n","\n","    #     if self.weights is None:\n","    #       self.weights = torch.ones(1,len(Ls))/len(Ls)\n","\n","    #     # pdb.set_trace()\n","\n","    #     if min_max:\n","    #         r1 = maximizeloss_weights_st( losses_for_reweighting , KLDiffHere, self.eps)\n","    #         candidate_weigths = torch.zeros_like(self.weights).to(torch.device(\"cuda:0\"))\n","    #         candidate_weigths[0][mask_for_available_losses] = torch.tensor(r1.x).to(torch.device(\"cuda:0\")).float()\n","    #         self.weights = candidate_weigths.to(torch.device(\"cuda:0\"))\n","    #         self.weights_tbl.append(self.weights.detach().cpu().numpy())\n","\n","    #     numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","    #     if train == True:\n","    #         return  (self.weights[0,0]*torch.mean(DO) + \n","    #                  self.weights[0,1]*torch.mean(TC) + \n","    #                  self.weights[0,2]*torch.mean(BC)) , \\\n","    #                  self.weights[0,0]*torch.mean(DO) , \\\n","    #                  self.weights[0,1]*torch.mean(TC) , \\\n","    #                  self.weights[0,2]*torch.mean(BC) , \\\n","    #                  (1./numActive*torch.mean(DO) + \n","    #                  1./numActive*torch.mean(TC) + \n","    #                  1./numActive*torch.mean(BC))             \n","    #     else:\n","    #         return  DO , TC , BC\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"iyacROFeXgNp","executionInfo":{"status":"ok","timestamp":1653240827775,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a60a826b-c847-46f5-e6fc-17a3bdc12599"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 8.03 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["import torch\n","from torch.distributions import Normal\n","\n","std_norm_cdf = Normal(0, 1).cdf\n","std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n","\n","def bs_price(right, K, S, T, sigma, r):\n","    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n","    d_2 = d_1 - sigma * torch.sqrt(T)\n","    \n","    if right == \"C\":\n","        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n","        return C\n","        \n","    elif right == \"P\":\n","        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n","        return P"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"sLsA5AvqpMM7","executionInfo":{"status":"ok","timestamp":1653240827776,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"28b566db-c1a6-4884-c6e3-4911a65c55a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.93 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["import torch\n","\n","def to_cpu_detach(x):\n","  if isinstance(x, list):\n","    return [ y.detach().cpu().item() for y in x ]\n","  else:\n","    return x.detach().cpu().item()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"PC-E2SeX46A9","executionInfo":{"status":"ok","timestamp":1653240827776,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"321f906a-457b-40bb-b989-68d76f2314c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.12 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["def huber_loss_zero_target(x, delta = 1.0):\n","  loss_function = torch.nn.HuberLoss(delta=delta)\n","  return loss_function(x, torch.zeros_like(x))\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"MNYJyHWpeL66","executionInfo":{"status":"ok","timestamp":1653240827777,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6b3e9d2-23f9-4b86-ce1f-f69dfc34b61d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 21.7 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_wgamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"code","source":["def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        xbreaks = getattr(eqObject,\"xbreaks\")\n","        xbreaks_str = str(len(xbreaks))\n","        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        tbreaks = getattr(eqObject,\"tbreaks\")\n","        tbreaks_str = str(len(tbreaks))\n","        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"],"metadata":{"id":"ipogSsVTbv0k","executionInfo":{"status":"ok","timestamp":1653240828216,"user_tz":-60,"elapsed":446,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"16403995-63c3-4571-8d85-9cd2a70e68ef"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 42.4 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Tz5tUJuYaXKu"},"source":["### Merton Invest-Consumption Problem - Equation HJB optimization\n","\n","[Extensions of the Deep Galerkin Method](https://arxiv.org/pdf/1912.01455v3.pdf)"]},{"cell_type":"markdown","source":["##### Closed form terminal utility functions"],"metadata":{"id":"N-GO35FcJPP6"}},{"cell_type":"code","source":["def expTerminalUtilityOfWealth(x, gamma_discount = 1.0):\n","  return(-torch.exp(-gamma_discount*x))\n","\n","def expTerminalUtilityOfWealth_np(x, gamma_discount = 1.0):\n","  return(-np.exp(-gamma_discount*x))\n","\n","from functools import partial\n","\n","# should give a closed form solution for the control => PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cWoRXs02PoF","executionInfo":{"status":"ok","timestamp":1653240828217,"user_tz":-60,"elapsed":11,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"ea09887c-4a36-49c9-dd80-8b3c580f3597"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.57 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"HrivvbmubiiY"},"source":["#### MertonUtilityNet"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"JRraqOG4aXKx","executionInfo":{"status":"ok","timestamp":1653240828217,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5678816-4e71-4b76-e975-6e8e745b6b19"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 10.3 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["class MertonUtilityNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh  ):\n","        super(MertonUtilityNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5 + 1  # wealth, time, mu, r, sigma, pi\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out "]},{"cell_type":"markdown","source":["#### MertonPiNet"],"metadata":{"id":"fyFbPZr7I5RE"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","class MertonPiNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.relu  ):\n","        super(MertonPiNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5   # wealth, time, mu, r, sigma\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)            \n","        # self.fc_output_d = nn.Linear(self.NN, 2)\n","        # self.fc_output = torch.nn.Softmax(dim=1)\n","        self.fc_output = nn.Linear(self.NN, 1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        # out = self.fc_output_d(h)\n","        out = self.fc_output(h)\n","        return out \n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PR7PHL4KI1S9","executionInfo":{"status":"ok","timestamp":1653240828217,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"e572cccd-fa1b-4be6-d0f1-5357c7f0ee8e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 23.1 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### MertonAlternativePiNet\n","\n","[implement from github](https://github.com/Plemeur/DGM/blob/master/first_net.py)"],"metadata":{"id":"RNhAbZ727RC_"}},{"cell_type":"code","source":["class LinearWithXavier(nn.Module):\n","    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n","        TODO : figure out what to do with the bias\"\"\"\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(LinearWithXavier, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        if bias:\n","            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","    def reset_parameters(self):\n","        torch.nn.init.xavier_uniform_(self.weight)\n","        if self.bias is not None:\n","            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n","    def forward(self, input):\n","        return torch.nn.functional.linear(input, self.weight, self.bias)\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n","\n","\n","class DGM_layer(nn.Module):\n","    \"\"\" See readme for paper source\"\"\"\n","    def __init__(self, in_features, out_feature, residual=False):\n","        super(DGM_layer, self).__init__()\n","        self.residual = residual\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature)\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=False)\n","        self.G = LinearWithXavier(out_feature, out_feature)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=False)\n","        self.R = LinearWithXavier(out_feature, out_feature)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=False)\n","        self.H = LinearWithXavier(out_feature, out_feature)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=False)\n","\n","    def forward(self, x, s):\n","        z = torch.tanh(self.UZ(x) + self.Z(s))\n","        g = torch.tanh(self.UG(x) + self.G(s))\n","        r = torch.tanh(self.UR(x) + self.R(s))\n","        h = torch.tanh(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonAlternativePiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonAlternativePiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n","        self.final_layer = LinearWithXavier(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","\n","        return self.final_layer(s)\n"],"metadata":{"id":"_c-dZ5b37NwV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653240828218,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"6cff9cb9-ea21-4258-ad1c-08c854da7997"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 49.3 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### MertonAlternativeUtilityNet"],"metadata":{"id":"leiCOWL89Mr5"}},{"cell_type":"code","source":["class MertonAlternativeUtilityNet(MertonAlternativePiNet):\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonAlternativeUtilityNet, self).__init__(in_size, out_size, neurons, depth)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sD3ZL1NY9TMF","executionInfo":{"status":"ok","timestamp":1653240828218,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"1697a1f9-d3d3-4261-83f2-edd99c16fab2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.22 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### MertonMatchPiNet\n","\n","[Matching Paper by hand](https://arxiv.org/abs/1912.01455v3)"],"metadata":{"id":"CNsqOm1ithSG"}},{"cell_type":"code","source":["class DGMLayerPaper(nn.Module):\n","\n","    def __init__(self, in_features, out_feature, activation=torch.relu, residual=False):\n","        \n","        super(DGMLayerPaper, self).__init__()\n","        self.residual = residual\n","        self.activation = activation\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature) # w.S\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=True) # u.x\n","        self.G = LinearWithXavier(out_feature, out_feature)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.R = LinearWithXavier(out_feature, out_feature)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.H = LinearWithXavier(out_feature, out_feature) # w.(S(o)R)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=True)\n","\n","    def forward(self, x, s):\n","        z = self.activation(self.UZ(x) + self.Z(s))\n","        g = self.activation(self.UG(x) + self.G(s))\n","        r = self.activation(self.UR(x) + self.R(s))\n","        h = self.activation(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonMatchPiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonMatchPiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGMLayerPaper(in_size, neurons) for i in range(depth)])\n","        self.final_layer = LinearWithXavier(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","\n","        return self.final_layer(s)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653240828218,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"91806510-cdab-4261-ce56-0615e1175997","id":"REWml4aYthSH"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 23.3 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"wClW1g9rbm8o"},"source":["#### PiEquation"]},{"cell_type":"code","source":["# torch.max(torch.tensor([1.0,1.3,1.5]), torch.tensor(1.32).expand_as(torch.tensor([1.0,1.3,1.5])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ivFXAdDYERq","executionInfo":{"status":"ok","timestamp":1653240828218,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"2e52d5d5-7407-4ea4-c2c7-47e44a9372ca"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 707 Âµs (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"code","source":["class PiEquation():\n","\n","    def __init__(self , pi_net, du_dx, d2u_dx2):\n","        self.pi_net = pi_net\n","        self.wgamma = 0.0001\n","        self.du_dx = (torch.sign(du_dx).to(du_dx.device)*torch.max(torch.abs(du_dx).to(du_dx.device), torch.tensor(1e-12).expand_as(du_dx).to(du_dx.device))).to(du_dx.device)\n","        self.d2u_dx2 = (torch.sign(d2u_dx2).to(du_dx.device)*torch.max(torch.abs(d2u_dx2).to(du_dx.device), torch.tensor(1e-12).expand_as(d2u_dx2).to(d2u_dx2.device))).to(d2u_dx2.device)\n","\n","    def criterion(self, x_internal):\n","      #  time, wealth, mu, r, sigma\n","      pi_net_preds = self.pi_net(x_internal)\n","      # pi_net_preds = pi_net_preds[:,0].reshape(-1,1)\n","      pi_net_preds = pi_net_preds.reshape(-1,1)\n","\n","      dpi = torch.autograd.grad( pi_net_preds, \n","                                x_internal, \n","                                grad_outputs=torch.ones_like(pi_net_preds) ,\n","                                create_graph=True,\n","                                retain_graph=True)\n","      dpi_dt = dpi[0][:,0].reshape(-1,1)\n","      dpi_dx = dpi[0][:,1].reshape(-1,1)\n","\n","      d2pi_dx2 = torch.autograd.grad( dpi_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(dpi_dx) ,\n","                                      create_graph = True,\n","                                      retain_graph=True)[0][:,1].reshape(-1,1)\n","      intC = None\n","      # pdb.set_trace()\n","      if len(x_internal) == 0:\n","        intC_loss = torch.tensor(0).cuda().float()  \n","      else:\n","        # pdb.set_trace()\n","        intC_loss = -(pi_net_preds*(x_internal[:,2].reshape(-1,1)-x_internal[:,3].reshape(-1,1)) + x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1))*self.du_dx - \\\n","                                    0.5*(x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)*self.d2u_dx2\n","\n","        # print(f\"Pi Loss {torch.mean(intC_loss).item()} {x_internal.shape[0]} {torch.mean(self.du_dx)}\")          \n","\n","        # intC_loss = (pi_net_preds*(x_internal[:,2].reshape(-1,1)-x_internal[:,3].reshape(-1,1)) + x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1))*self.du_dx + \\\n","        #                             0.5*(x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)*self.d2u_dx2\n","\n","      return  1.0*intC_loss\n","\n","    def calculatePiLoss(self, x_internal, keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        x_internal = Variable( x_internal , requires_grad=True)\n","        Ls = self.criterion( x_internal )\n","        \n","        return_losses = []\n","        if not keep_batch:\n","          loss_pi = torch.mean(Ls)           \n","          return loss_pi          \n","        else:\n","          return Ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQTTkUhWuiJi","executionInfo":{"status":"ok","timestamp":1653240828219,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"1e2e18cb-d104-41d7-df4c-4330d70669d5"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 33.9 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### TrainInternalPiWithDGM\n"],"metadata":{"id":"8RRoBgFQINMv"}},{"cell_type":"code","source":["class TrainInternalPiWithDGM():\n","    \n","    def __init__(self , u_equation, pi_equation, BATCH_SIZE , epoch, lr, debug = False, loss_multiply = 1.0):\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.u_model = u_equation        \n","        self.pi_model = pi_equation\n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.pi_model.pi_net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","\n","        self.epoch = epoch\n","        self.lr = lr\n","\n","        self.loss_multiply = loss_multiply\n","        \n","    def train(self , eqLossFn = 'calculatePiLoss', sample_method_X = \"U\"):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((self.epoch, 3 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n","        # optimizer = self.optimizer_used(self.u_model.pi_net.parameters(), self.lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        \n","        loss_avg = 0.0\n","        # pdb.set_trace()\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.pi_model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(self.epoch):\n","\n","            optimizer.zero_grad()            \n","\n","            sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            loss_avg = 0.0\n","            # pdb.set_trace()\n","            loss  = loss_calc_method( sample_batch[0], keep_batch = False )            \n","            # print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) ]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              self.validation_losses[e,:] = validation_loss_list\n","            \n","            if self.use_early_stop:\n","              loss_to_check = loss\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward(retain_graph=True)\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                # print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(e , self.lr , loss))\n","\n","        self.stop_epoch = e\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VREn3fanpP1b","executionInfo":{"status":"ok","timestamp":1653240828219,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"3a969e28-1ee1-4b94-ff8d-9f755260aad9"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 78.9 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### MertonEquation"],"metadata":{"id":"hp4BG1ewKF6o"}},{"cell_type":"code","execution_count":31,"metadata":{"cellView":"code","id":"LBMZYQSPaXKy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653240829209,"user_tz":-60,"elapsed":997,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"825b94ac-d5a9-4537-ea6f-9e0be4250c2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 968 ms (started: 2022-05-22 17:33:48 +00:00)\n"]}],"source":["import math\n","\n","class MertonEquation():\n","    \n","    def __init__(self , u_net, pi_net, pi_net_epoch, pi_net_lr, term_utility_function = partial(expTerminalUtilityOfWealth,gamma_discount=0.1) ):\n","\n","        self.u_net = u_net\n","        self.pi_net = pi_net\n","        self.wgamma = 0.0001\n","        self.term_utility_func = term_utility_function\n","        self.xbreaks = None\n","        self.tbreaks = None\n","\n","        self.MAX_X = 1.0\n","        self.T = 1.0\n","        self.MAX_MU = 0.2\n","        self.MAX_SIGMA = 1.0\n","\n","        self.pi_net_epoch = pi_net_epoch\n","        self.pi_net_lr = pi_net_lr\n","        self.loss_multiply = 1.0\n","\n","        self.FORCE_MU = None\n","        self.FORCE_R = None\n","        self.FORCE_SIGMA = None\n","\n","        self.weights=None\n","        self.gamma = 1.0\n","\n","    def g(self,x):\n","        # Time, Wealth, Mu, R, Sigma\n","        return self.term_utility_func(x[:,1].reshape(-1,1))\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def mu_r_sample(self, size, range_multiplier = 1.0):\n","      mu_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      r_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      r_sample = torch.where(r_candidate < mu_candidate, r_candidate, mu_candidate)\n","      mu_sample = torch.where(r_candidate > mu_candidate, r_candidate, mu_candidate)\n","      return (mu_sample, r_sample)\n","\n","    def apply_forced_mu_r_sigma(self, mu_sample, r_sample, sigma_sample):\n","      if self.FORCE_MU is not None:\n","         mu_sample = self.FORCE_MU*torch.ones_like(mu_sample)            \n","      if self.FORCE_R is not None:\n","        r_sample = self.FORCE_R*torch.ones_like(r_sample)\n","      if self.FORCE_SIGMA is not None:\n","        sigma_sample = self.FORCE_SIGMA*torch.ones_like(sigma_sample)\n","      return mu_sample, r_sample, sigma_sample\n","\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","        '''\n","        Sampling function\n","        '''\n","        if sample_method_X in [\"U\"]:\n","            range_multiplier = 1.0\n","            \n","            ### internal samples of Time, Wealth, Mu, R, Sigma\n","            mu_sample_internal, r_sample_internal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_internal, r_sample_internal, sigma_sample_internal = self.apply_forced_mu_r_sigma(mu_sample_internal, r_sample_internal, sigma_sample_internal)\n","            x_internal = self.to_device(torch.cat(( torch.rand([size,1])*self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_internal, # mu\n","                                                    r_sample_internal, # R\n","                                                    sigma_sample_internal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            ### Terminal time samples\n","            mu_sample_terminal, r_sample_terminal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_terminal, r_sample_terminal, sigma_sample_terminal = self.apply_forced_mu_r_sigma(mu_sample_terminal, r_sample_terminal, sigma_sample_terminal)\n","            x_terminal = self.to_device(torch.cat(( torch.zeros(size, 1) + self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_terminal, # mu\n","                                                    r_sample_terminal, # R\n","                                                    sigma_sample_terminal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            \n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            return x_internal , x_terminal\n","\n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","        \n","    def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","\n","      if self.xbreaks is None and self.tbreaks is None:\n","        return self.sample(sample_method_X, size, to_cpu)\n","\n","      internal_strata_xts = []\n","      terminal_strata_xts = []\n","      \n","      if sample_method_X in [\"U\"]:\n","          range_multiplier = 1.0\n","          xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n","          tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n","          if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n","            xbreaks_used.append(range_multiplier*self.MAX_X)\n","          while xbreaks_used[0] < 0.0:\n","            xbreaks_used.pop(0)\n","          if not xbreaks_used:\n","            xbreaks_used = [0,range_multiplier*self.MAX_X]\n","          if xbreaks_used[0] > 0.0:            \n","            xbreaks_used.insert(0, 0.0)\n","\n","          if tbreaks_used[-1] < self.T:\n","            tbreaks_used.append(self.T)\n","          xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n","          tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n","\n","          total_strat_processed = 0\n","\n","          # internal samples\n","          for stratum_x_count in range(len(xbreaks_used)-1):\n","              \n","            num_samples_in_stratum = 0\n","            if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n","              range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n","              num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n","\n","            for stratum_t_count in range(len(self.tbreaks)-1):\n","\n","              if num_samples_in_stratum == 0: # there is only a T division, so use it\n","                range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n","                num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n","              else:\n","                # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n","                stratum_coverage_on_unit_square = \\\n","                  ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n","                  ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n","                num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n","\n","              range_multiplier = 1.0\n","\n","              ### internal samples of Time, Wealth, Mu, R, Sigma\n","              internal_stratum_t_sample = tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])\n","              internal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_internal, stratum_r_sample_internal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal)\n","              x_internal_stratum = self.to_device(torch.cat(( internal_stratum_t_sample , # Time\n","                                                              internal_stratum_x_sample, # Wealth\n","                                                              stratum_mu_sample_internal, # mu\n","                                                              stratum_r_sample_internal, # R\n","                                                               # Sigma\n","                                                            ) , dim = 1 ),to_cpu)\n","              if not internal_strata_xts: \n","                internal_strata_xts = [ x_internal_stratum ] \n","              else:\n","                internal_strata_xts.append(x_internal_stratum) \n","\n","              ### Terminal time samples\n","              terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal)\n","              x_terminal_stratum = self.to_device(torch.cat(( torch.zeros(num_samples_in_stratum, 1) + self.T , # Time\n","                                                      terminal_stratum_x_sample, # Wealth\n","                                                      stratum_mu_sample_terminal, # mu\n","                                                      stratum_r_sample_terminal, # R\n","                                                      stratum_sigma_sample_terminal # Sigma\n","                                                    ) , dim = 1 ),to_cpu)\n","              if not terminal_strata_xts:\n","                terminal_strata_xts = [ x_terminal_stratum ] # terminal_stratum_xt[None,:,:]\n","              else:\n","                terminal_strata_xts.append(x_terminal_stratum) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n","\n","              total_strat_processed += 1 \n","              # print((len(internal_strata_xts),xbreaks_used[stratum_x_count],tbreaks_used[stratum_t_count]))\n","\n","          # pdb.set_trace()\n","          # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","          return internal_strata_xts , terminal_strata_xts\n","    \n","      raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","\n","    def criterion(self, x_internal , x_terminal, loss_transforms = [torch.square]):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # Time / Wealth / Mu / r / Sigma (sample data order)\n","        # pdb.set_trace()\n","\n","        # replace with closed form just to check\n","        # self.pi_net = lambda x: (((x[:,2]-x[:,3])/(1.0*(x[:,4]**2)))*torch.exp(-x[:,3]*(1.0-x[:,0])))\n","\n","        # pdb.set_trace()\n","        pi_used = self.pi_net(x_internal)  \n","        pi_used.detach_()\n","        # let's assign the first column as the allocation\n","        # pdb.set_trace()     \n","        # pi_used = pi_used[:,0].reshape(-1,1)\n","        pi_used = pi_used.reshape(-1,1)\n","        x_internal_before = x_internal.detach().clone()\n","        x_internal =  Variable(torch.cat((x_internal, pi_used), dim=1),requires_grad=True)\n","\n","        du = torch.autograd.grad( self.u_net(x_internal), \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(self.u_net(x_internal)) ,\n","                                  create_graph=True,\n","                                  retain_graph=True )\n","        \n","        du_dt = du[0][:,0].reshape(-1,1)\n","        du_dx = du[0][:,1].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(  du_dx, \n","                                        x_internal , \n","                                        grad_outputs=torch.ones_like(du_dx) ,\n","                                        create_graph = True,\n","                                        retain_graph = True)[0][:,1].reshape(-1,1)\n","    \n","        # def pi_net_fn(x,du_dx = du_dx,d2u_dx2 = d2u_dx2): \n","        #   return (-(((x[:,2]-x[:,3])/(1.0*(x[:,4]**2)))*torch.div(du_dx,d2u_dx2).reshape(-1)))\n","\n","        # pi_net_fn2 = lambda x: (((x[:,2]-x[:,3])/(1.0*(x[:,4]**2)))*torch.exp(-x[:,3]*(1.0-x[:,0])))\n","\n","        pi_model = PiEquation(self.pi_net, du_dx, d2u_dx2)                \n","        pi_trainer = TrainInternalPiWithDGM(self, pi_model, x_internal.shape[0], \n","                                            self.pi_net_epoch, self.pi_net_lr, \n","                                            debug=True, loss_multiply=1.0)\n","        pi_trainer.use_early_stop = True\n","        pi_trainer.early_stop_patience = min(200,math.ceil(self.pi_net_epoch/10.0))\n","        pi_trainer.train()\n","        \n","        # self.pi_net =  pi_net_fn\n","        # self.pi_net =  pi_net_fn2\n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]\n","\n","        intC = None\n","        terC = None\n","\n","        if len(x_internal) == 0:\n","          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          # Time, Wealth, Mu, R, Sigma\n","          # pdb.set_trace()\n","          pi_net_preds = self.pi_net(x_internal_before)\n","          pi_net_preds.detach_()\n","          # pi_net_preds = pi_net_preds[:,0].reshape(-1,1)\n","          pi_net_preds = pi_net_preds.reshape(-1,1)\n","          intC_loss = du_dt + (pi_net_preds*(x_internal[:,2].reshape(-1,1)-x_internal[:,3].reshape(-1,1))+x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1))*du_dx + 0.5*(x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)*d2u_dx2\n","          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        x_terminal_before = x_terminal.detach().clone()\n","        pi_net_preds_terminal = self.pi_net(x_terminal_before)\n","        pi_net_preds_terminal.detach_()\n","        # pi_net_preds_terminal = pi_net_preds_terminal[:,0].reshape(-1,1)\n","        pi_net_preds_terminal = pi_net_preds_terminal.reshape(-1,1)\n","        x_terminal =  Variable(torch.cat((x_terminal, pi_net_preds_terminal), dim=1),requires_grad=True)\n","\n","        terC = [ loss_transform( self.u_net(x_terminal) - self.g(x_terminal)   ) for loss_transform in loss_transforms ]\n","\n","        return  intC , terC\n","\n","    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        # pdb.set_trace()\n","        x_internal , x_terminal = batch_x\n","        x_internal = Variable( x_internal , requires_grad=True)\n","        Ls = self.criterion( x_internal , x_terminal, loss_transforms = loss_transforms )\n","        intC , terC  = Ls\n","\n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc])\n","            return_losses.append( [ loss_equalWeightedByType , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]), \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC.numpy(), terC.numpy()] )\n","        return return_losses\n","\n","    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , loss_transforms = loss_transforms)\n","        intC , terC = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n","            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt)\n","            return_losses.append( [ transformed_loss , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","\n","    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal  = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal, loss_transforms = loss_transforms)\n","        intC , terC  = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n","            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt )\n","            return_losses.append( [ transformed_loss , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"65nooklCbsdy"},"source":["#### TrainHJBMertonWithDGM"]},{"cell_type":"code","source":["def attach_pi_used(x, pi_net, requires_grad=True):\n","  pi_used = pi_net(x)  \n","  pi_used.detach_()\n","  # pi_used = pi_used[:,0].reshape(-1,1)\n","  pi_used = pi_used.reshape(-1,1)\n","  \n","  before_x = x.detach().clone()\n","  new_x =  Variable(torch.cat((x, pi_used), dim=1),requires_grad=requires_grad)\n","  return before_x, new_x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74jqGI3o2GlN","executionInfo":{"status":"ok","timestamp":1653165555859,"user_tz":-60,"elapsed":4,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"eac13c6a-8ee9-4103-d9ea-cd9b2586d341"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.71 ms (started: 2022-05-21 20:39:17 +00:00)\n"]}]},{"cell_type":"code","execution_count":32,"metadata":{"id":"OtO8fV7oaXK2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653165556598,"user_tz":-60,"elapsed":740,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"fc252f8a-bd01-4bc8-acbb-7e2268f7bd8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 733 ms (started: 2022-05-21 20:39:17 +00:00)\n"]}],"source":["class TrainHJBMertonWithDGM():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None   \n","\n","        self.min_epoch_for_min_loss = 100     \n","        \n","\n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\", key_loss_func = torch.square, huber_delta = 0.5, save_name_best = None):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((epoch, 3*4 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        \n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","\n","            optimizer.zero_grad()\n","            \n","            # pdb.set_trace()\n","            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            losses_L2, losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n","            # pdb.set_trace()\n","            loss , internal , terminal , losses_equalWeightedByType = losses_L2\n","            loss_abs , internal_abs , terminal_abs , losses_equalWeightedByType_abs = losses_ABS\n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) , to_cpu_detach(internal) , to_cpu_detach(terminal) , \n","                                       to_cpu_detach(loss_abs) , to_cpu_detach(internal_abs) , to_cpu_detach(terminal_abs), \n","                                       to_cpu_detach(losses_equalWeightedByType_abs)]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss_list = validation_loss_list.pop(5) # the L2 loss is duplicated at index 1\n","              self.validation_losses[e,:] = validation_loss_list\n","              # pdb.set_trace()\n","              # print(f\"Epoch {e} - Pi Pred (0.47) {self.model.pi_net(self.validation_sample[0]).item()}\")\n","\n","            \n","            if self.use_early_stop or (save_name_best is not None):\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                if (e > self.min_epoch_for_min_loss) and (save_name_best is not None):\n","                  torch.save(self.net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/Unet_{save_name_best}_{self.net.__class__.__name__}_{self.model.__class__.__name__}\")\n","                  torch.save(self.model.pi_net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/Pinet_{save_name_best}_{self.model.pi_net.__class__.__name__}_{self.model.__class__.__name__}\")\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.use_early_stop and (self.early_stop_counter>=self.early_stop_patience):\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            # self.min_epoch_for_min_loss\n","\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","\n","                loss_avg = loss_avg/self.hook_interval\n","                # pdb.set_trace()\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                print(f\"Epoch {e} - lr {lr} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                \n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal = self.validation_sample\n","                    xinternal_before, xinternal_expanded = attach_pi_used(xinternal, self.model.pi_net, requires_grad=False)\n","                    xterminal_before, xterminal_expanded = attach_pi_used(xterminal, self.model.pi_net, requires_grad=False)\n","\n","                    xinternal_res = self.model.u_net(xinternal_expanded).detach()\n","                    xterminal_res = self.model.u_net(xterminal_expanded).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\", \"Mu\", \"R\", \"Sigma\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n","    def train_stratified(self , epoch , lr, \n","                         eqLossFn = 'calculateLoss', \n","                         sample_method_X = \"U\", \n","                         key_loss_func = torch.square, \n","                         huber_delta = 0.5,\n","                         save_name_best = None\n","                         ):\n","        \n","        self.validation_losses = np.ones((epoch, 3*3 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1), dtype='float32') * np.nan\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            # pdb.set_trace()\n","            internal_xts_bts, terminal_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n","            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n","            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n","            \n","            # pdb.set_trace()\n","            for stratum_count in range(len(internal_xts_bts)):              \n","              sample_batch = (internal_xts_bts[stratum_count], \n","                              terminal_xts_bts[stratum_count])  \n","\n","              # pdb.set_trace()\n","              stratum_losses_L2, stratum_losses_ABS = loss_calc_method(sample_batch, \n","                                                                       loss_transforms = [ key_loss_func, torch.abs ], \n","                                                                       keep_batch = False )\n","              # if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n","              #   pdb.set_trace()\n","              #   pass\n","            \n","              if training_stratum_losses is not None:\n","                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n","              else:\n","                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n","\n","              # pdb.set_trace()  \n","              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n","\n","            # pdb.set_trace()              \n","            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n","            loss = training_value_to_optimize\n","\n","            loss_optimized , internal , terminal, losses_equalWeightedByType, \\\n","            loss_abs , internal_abs , terminal_abs ,losses_equalWeightedByType_abs = training_loss_for_epoch            \n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n","                loss_calc_method( self.validation_sample, \n","                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                  keep_batch = False )\n","              validation_loss = [*to_cpu_detach(losses_L2_validation),\n","                                              *to_cpu_detach(losses_ABS_validation),\n","                                              *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n","              self.validation_losses[e,:] = validation_loss\n","\n","            if self.use_early_stop or (save_name_best is not None):\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                if (e > self.min_epoch_for_min_loss) and (save_name_best is not None):\n","                  torch.save(self.net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/Unet_{save_name_best}_{self.net.__class__.__name__}_{self.model.__class__.__name__}\")\n","                  torch.save(self.model.pi_net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/Pinet_{save_name_best}_{self.model.pi_net.__class__.__name__}_{self.model.__class__.__name__}\")\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.use_early_stop and (self.early_stop_counter>=self.early_stop_patience):\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n"," "]},{"cell_type":"markdown","source":["### Test Case"],"metadata":{"id":"oy05I1QFh7EM"}},{"cell_type":"markdown","source":["#### Test Case NO Stratification"],"metadata":{"id":"U7zqglm1ewTL"}},{"cell_type":"code","source":["mequation = MertonEquation(MertonUtilityNet( NL = 1 , NN = 3 ), MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 ), 1, 10000.0)\n","# val_sample_to_use = tuple([ x.cpu().detach() for x in mequation.sample(sample_method_X=\"U\", size=1) ] )\n","val_sample_to_use = mequation.sample(sample_method_X=\"U\", size=1) \n","# # gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   \n","val_sample_to_use[0][0,0] = 0.0\n","val_sample_to_use[0][0,2] = 0.05\n","val_sample_to_use[0][0,3] = 0.02\n","val_sample_to_use[0][0,4] = 0.25"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yLM2WbUoymO","executionInfo":{"status":"ok","timestamp":1653165561053,"user_tz":-60,"elapsed":3511,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"32e18f8b-5562-4a62-d526-468a9d3d47c0"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.33 s (started: 2022-05-21 20:39:19 +00:00)\n"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","# u_net, pi_net, pi_net_epoch, pi_net_lr\n","eqLossFn= 'calculateLoss' #'calculateLoss'\n","sample_method= \"U\"\n","lr = 0.000001\n","lr_for_pi = 0.000001\n","max_pi_epochs = 1 # has to be low!!!\n","save_name_best = f\"{datetime.datetime.now():%Y%m%d%H%M%S}\"\n","\n","u_net = MertonUtilityNet(NL = 3, NN =100) # \n","# u_net = MertonAlternativeUtilityNet( in_size = 6 , out_size = 1, neurons = 100, depth=2 )\n","u_net.to(torch.device(\"cuda:0\")) \n","# pi_net = MertonPiNet(NL =3, NN=100)  # \n","pi_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 64, depth= 3 )\n","pi_net.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","mequation = MertonEquation(u_net, pi_net, max_pi_epochs, lr_for_pi)\n","# mequation.FORCE_MU = 0.05\n","# mequation.FORCE_R = 0.02\n","# mequation.FORCE_SIGMA = 0.25\n","trainMertonAlloc = TrainHJBMertonWithDGM(u_net, mequation, BATCH_SIZE = 2**5 , debug = True )\n","trainMertonAlloc.hook_interval = 400\n","# trainMertonAlloc.use_early_stop = True\n","# trainMertonAlloc.early_stop_patience = 1000\n","# trainMertonAlloc.validation_sample = val_sample_to_use\n","trainMertonAlloc.train(epoch = 100000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method, save_name_best = save_name_best)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"466509ad-9fab-4751-a911-8b5b6f954dbc","id":"IUNDoPJRe6dL","executionInfo":{"status":"error","timestamp":1653240778371,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}}},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3832c44438b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["# check control for closed form:\n","# PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))\n","# ((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))\n","# gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   # PI\n","\n","gamma = 1.0\n","internal_sample, terminal_sample = mequation.sample(size=100, to_cpu=False)\n","mask = (internal_sample[:,0] > 0.1) & (internal_sample[:,4] > 0.1)\n","internal_sample = internal_sample[mask.reshape(-1),:]\n","# time, wealth, mu, r, sigma\n","time = internal_sample[:,0].cpu().detach()\n","wealth = internal_sample[:,1].cpu().detach()\n","mu = internal_sample[:,2].cpu().detach()\n","r = internal_sample[:,3].cpu().detach()\n","sigma = internal_sample[:,4].cpu().detach()\n","\n","# mequation.pi_net(internal_sample)[:,0]"],"metadata":{"id":"u35g5sAPbsG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# closed form value function\n","def Htx(x):\n","  return -torch.exp(-x[:,1].reshape(-1,1)*1.0*torch.exp(x[:,3].reshape(-1,1)*(1.0-x[:,0].reshape(-1,1))) - \n","                    0.5*(1.0-x[:,0].reshape(-1,1))*((x[:,2].reshape(-1,1)-x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2  )"],"metadata":{"id":"O-NmMG33xGBu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the fitted value function vs the closed form (ideally straight line...)\n","u_internal_sample = torch.cat((internal_sample, mequation.pi_net(internal_sample).reshape(-1,1)), dim=1)\n","u_net_results = u_net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n","htx_results = Htx(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist()\n","dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n","ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point()"],"metadata":{"id":"Woj8BoAK5OLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot the control function vs the closed form (ideally straight line...)\n","dataf = pd.DataFrame( { 'pi_net': mequation.pi_net(internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n","                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n","ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point()\n"],"metadata":{"id":"GHfTC6GljYnd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"f85Kg2zvhMdM"},"execution_count":null,"outputs":[]}]}