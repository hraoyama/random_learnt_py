{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DGM_HBJNNx_Par2.ipynb","provenance":[{"file_id":"12X2KXaaeispO6TznFB-M85IhV1NDIolN","timestamp":1653056653625}],"collapsed_sections":["bvy0WvxDGCxk","N-GO35FcJPP6","HrivvbmubiiY","fyFbPZr7I5RE","RNhAbZ727RC_","CNsqOm1ithSG","wClW1g9rbm8o","8RRoBgFQINMv","hp4BG1ewKF6o","65nooklCbsdy"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BErSeefeQwQi"},"source":["### Setup packages "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24994,"status":"ok","timestamp":1653392500703,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"L8DGCgVxR2AB","outputId":"c1323917-58d0-4e51-ae34-148f0c09d339"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11956,"status":"ok","timestamp":1653392512651,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"3xIx5C6UQn4u","outputId":"f7f30943-b2db-4569-d299-7ddbcdbc801a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting progressbar\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","Building wheels for collected packages: progressbar\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=525db3c5293dd6b26062cc98052abf379dd9eafe31dbb95c386b5d54dcf6a7a9\n","  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n","Successfully built progressbar\n","Installing collected packages: progressbar\n","Successfully installed progressbar-2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.2.0)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.3.1\n","time: 132 µs (started: 2022-05-24 11:41:52 +00:00)\n"]}],"source":["%pip install progressbar\n","%pip install plotnine\n","%pip install torch\n","%pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EfIU_eNp3Zio","executionInfo":{"status":"ok","timestamp":1653392514915,"user_tz":-60,"elapsed":2270,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cdd540fb-3e0a-4f1c-ee21-1df44b455353"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.6 s (started: 2022-05-24 11:41:52 +00:00)\n"]}],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmUjYbArAuQT","executionInfo":{"status":"ok","timestamp":1653392517011,"user_tz":-60,"elapsed":2101,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9eb5f2f4-07d0-49df-f271-8c3a9983c3e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.35 s (started: 2022-05-24 11:41:53 +00:00)\n"]}],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BI4p7ZKb0Qz2","executionInfo":{"status":"ok","timestamp":1653392517011,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"db2fdeb5-1697-44d8-bbfd-315d1a9bfa22"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 842 µs (started: 2022-05-24 11:41:56 +00:00)\n"]}],"source":["paper_name = \"dgm_hjb\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"433z6V3T2rB2","executionInfo":{"status":"ok","timestamp":1653392517444,"user_tz":-60,"elapsed":440,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5fa0b31-d09f-4fff-a9c9-ddafb6e2bce4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 942 ms (started: 2022-05-24 11:41:56 +00:00)\n"]}],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uat0pG8aR3Rh","executionInfo":{"status":"ok","timestamp":1653392517444,"user_tz":-60,"elapsed":5,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6940deff-63d7-4670-9632-bf9e975daa32"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 7.79 ms (started: 2022-05-24 11:41:57 +00:00)\n"]}],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpFjo3MkLus9","executionInfo":{"status":"ok","timestamp":1653392520874,"user_tz":-60,"elapsed":3433,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3323c45d-0769-4d6c-8597-d27d41fb404f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.58 s (started: 2022-05-24 11:41:57 +00:00)\n"]}],"source":["import torch \n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from scipy.stats import norm\n","from matplotlib import cm\n","import pdb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbfN42gpGZhC","executionInfo":{"status":"ok","timestamp":1653392522540,"user_tz":-60,"elapsed":1670,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"77d4cdf2-715f-4ab2-a1c8-0b7c3f578cc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.81 s (started: 2022-05-24 11:41:59 +00:00)\n"]}],"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","from pprint import pprint as pp"]},{"cell_type":"markdown","metadata":{"id":"bvy0WvxDGCxk"},"source":["### Shared functions across models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpVaz5dwXZNq","executionInfo":{"status":"ok","timestamp":1653392522540,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b73969bb-b1ff-4b1d-98b7-598d4b1410c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 21.2 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["import pandas as pd\n","\n","def plot_report(train_instance):\n","        \n","    history_tl_cpu = [ x for x in train_instance.history_tl ]\n","    history_internal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_internal_cpu ]\n","    history_terminal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_terminal ]\n","    history_initial_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_initial ]\n","    history_nonzero_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_nonzero ]\n","\n","    obs_data = pd.DataFrame({\"Epochs\" : [ (x+1)*train_instance.hook_interval for x in range(len(history_initial_cpu))], \n","                             \"AvgLogLoss\": np.log(history_tl_cpu), \n","                             \"TerminalLogLoss\" :  np.log(history_terminal_cpu),\n","                             \"InternalLogLoss\" :  np.log(history_internal_cpu),\n","                             \"InitialLogLoss\" : np.log(history_initial_cpu),\n","                             \"NonZeroLogLoss\" : np.log(history_nonzero_cpu),\n","                             })\n","\n","    return (ggplot(obs_data, aes(\"Epochs\",\"AvgLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"TerminalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InternalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InitialLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"NonZeroLogLoss\")) + geom_line() + geom_point(),\n","            )\n","\n","def plot_activation_mean(train_instance):\n","    \n","    # pdb.set_trace()\n","\n","    if train_instance.debug == False:\n","        print( 'error: debug is off , turn it on and train again ' )\n","    else:\n","        history = np.array(train_instance.history_mean_hooks)\n","        jet= plt.get_cmap('jet')\n","        colors = iter(jet(np.linspace(0,1,10)))\n","        fig, ax = plt.subplots()\n","        for i in range(history.shape[1]):\n","            ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n","        fig.suptitle('Layers activation mean value', fontsize=10)\n","        leg = ax.legend();\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMAuMqdgU9kL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653392522542,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"29947aad-d96a-44e1-8c2c-284d97fbd966"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 524 µs (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["# plot_report(train)\n","# plot_activation_mean(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCV-yFDXUV4J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653392522542,"user_tz":-60,"elapsed":14,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"a466127f-5f3f-4f58-d022-237500ab40fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 829 µs (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["# print( 'Value at 0' , net( torch.tensor( [ 0. , 1. , 1. , 1. ] ).cuda() ) )\n","# #%% save\n","# torch.save(net.state_dict(), './model3Assets')\n","# #%%\n","# net = TheModelClass(*args, **kwargs)\n","# net.load_state_dict(torch.load('./modelmodel3Assets'))\n","# net.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONB5NopRa3fD","executionInfo":{"status":"ok","timestamp":1653392522542,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8b87a3d-381e-4e28-a4af-8e17b110599d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 29.5 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n","from scipy.optimize import LinearConstraint, NonlinearConstraint\n","from scipy.optimize import Bounds\n","from functools import partial\n","from scipy.optimize import minimize\n","from functools import wraps\n","\n","def negative(f):\n","    @wraps(f)\n","    def g(*args,**kwargs):\n","        return - f(*args,**kwargs)\n","    # g.__name__ = f'negative({f.__name__})'\n","    return g\n","# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n","\n","# we can add more minimization functions here later (e.g. SS diff)\n","def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n","  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n","  input = torch.tensor(varX*loss_terms)\n","  loss_pointwise = target * (torch.log(target) - torch.log(input))\n","  if reduction == \"mean\":  # default\n","      loss = loss_pointwise.mean()\n","  elif reduction == \"batchmean\":  # mathematically correct\n","      loss = loss_pointwise.sum() / input.size(0)\n","  elif reduction == \"sum\":\n","      loss = loss_pointwise.sum()\n","  else:  # reduction == \"none\"\n","      loss = loss_pointwise  \n","  return loss\n","\n","  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n","\n","def minimize_weights_st(loss_terms, loss_func):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  x0 = [0.25]*len(loss_terms)\n","  res = minimize( partial(loss_func, loss_terms=loss_terms), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n","\n","def maximizeloss_weights_st(loss_terms, loss_func, eps):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","  # even though zero is the KL minimum it helps to put a negative number here to explore\n","\n","  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n","  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint, nonlinear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1653392522542,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"RM0IVdZ_TXW3","outputId":"d377abfa-bd97-4b61-8ee2-43c15a61a5d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.33334757 0.33333761 0.33331482]\n","time: 75.7 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n","print(r1.x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewko67bDIcz9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653392522543,"user_tz":-60,"elapsed":12,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"c5246608-21ed-46b5-ea36-75d047640301"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.96 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["### There is an issue getting this to work because of nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","\n","    # def calculateLossAdaptWeights(self , size = 2**8 , train = True, min_max = True):\n","    #     '''\n","    #     Helper function that Sample and Calculate loss,\n","    #     This is adapted in that it changes the weights on the losses to maximize the loss provided\n","    #     the KL distance of the new weighting is within self.eps of the previous distribution (starting at equally weighted)\n","    #     '''        \n","    #     x , x_terminal , x_boundary = self.sample(size)\n","    #     x = Variable( x , requires_grad=True)\n","    #     Ls = self.criterion( x , x_terminal , x_boundary )\n","    #     DO , TC , BC = Ls\n","    #     DOm = torch.mean(DO).detach().cpu().float().item()\n","    #     TCm = torch.mean(TC).detach().cpu().float().item()\n","    #     BCm = torch.mean(BC).detach().cpu().float().item()\n","\n","    #     losses_for_reweighting = [ torch.mean(lv).detach().cpu().float().item() for lv in Ls if list(lv.size())] \n","    #     mask_for_available_losses = [ True if list(lv.size()) else False for lv in Ls ]\n","\n","    #     # print([ DOm, TCm, BCm])\n","    #     # if is.nan(DOm):\n","    #     #   print(DO)\n","\n","    #     if self.weights is None:\n","    #       self.weights = torch.ones(1,len(Ls))/len(Ls)\n","\n","    #     # pdb.set_trace()\n","\n","    #     if min_max:\n","    #         r1 = maximizeloss_weights_st( losses_for_reweighting , KLDiffHere, self.eps)\n","    #         candidate_weigths = torch.zeros_like(self.weights).to(torch.device(\"cuda:0\"))\n","    #         candidate_weigths[0][mask_for_available_losses] = torch.tensor(r1.x).to(torch.device(\"cuda:0\")).float()\n","    #         self.weights = candidate_weigths.to(torch.device(\"cuda:0\"))\n","    #         self.weights_tbl.append(self.weights.detach().cpu().numpy())\n","\n","    #     numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","    #     if train == True:\n","    #         return  (self.weights[0,0]*torch.mean(DO) + \n","    #                  self.weights[0,1]*torch.mean(TC) + \n","    #                  self.weights[0,2]*torch.mean(BC)) , \\\n","    #                  self.weights[0,0]*torch.mean(DO) , \\\n","    #                  self.weights[0,1]*torch.mean(TC) , \\\n","    #                  self.weights[0,2]*torch.mean(BC) , \\\n","    #                  (1./numActive*torch.mean(DO) + \n","    #                  1./numActive*torch.mean(TC) + \n","    #                  1./numActive*torch.mean(BC))             \n","    #     else:\n","    #         return  DO , TC , BC\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyacROFeXgNp","executionInfo":{"status":"ok","timestamp":1653392522543,"user_tz":-60,"elapsed":11,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"783be535-ddb9-4e76-da7d-35385019526b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 7.12 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["import torch\n","from torch.distributions import Normal\n","\n","std_norm_cdf = Normal(0, 1).cdf\n","std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n","\n","def bs_price(right, K, S, T, sigma, r):\n","    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n","    d_2 = d_1 - sigma * torch.sqrt(T)\n","    \n","    if right == \"C\":\n","        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n","        return C\n","        \n","    elif right == \"P\":\n","        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n","        return P"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLsA5AvqpMM7","executionInfo":{"status":"ok","timestamp":1653392522543,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"82a11743-2693-495d-f978-442dcd4782cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.73 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["import torch\n","\n","def to_cpu_detach(x):\n","  if isinstance(x, list):\n","    return [ y.detach().cpu().item() for y in x ]\n","  else:\n","    return x.detach().cpu().item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PC-E2SeX46A9","executionInfo":{"status":"ok","timestamp":1653392522544,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"510f9657-8d5c-4a86-870c-d29a0ed170f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.21 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["def huber_loss_zero_target(x, delta = 1.0):\n","  loss_function = torch.nn.HuberLoss(delta=delta)\n","  return loss_function(x, torch.zeros_like(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNYJyHWpeL66","executionInfo":{"status":"ok","timestamp":1653392522544,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2ca769f-b8e4-4b18-bd59-b4b2bff615e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 18.5 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}],"source":["def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_wgamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"code","source":["def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        xbreaks = getattr(eqObject,\"xbreaks\")\n","        xbreaks_str = str(len(xbreaks))\n","        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        tbreaks = getattr(eqObject,\"tbreaks\")\n","        tbreaks_str = str(len(tbreaks))\n","        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"],"metadata":{"id":"ipogSsVTbv0k","executionInfo":{"status":"ok","timestamp":1653392523579,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"15e2bd39-c631-4982-b7a5-b69bdd082d9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 28 ms (started: 2022-05-24 11:42:01 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Tz5tUJuYaXKu"},"source":["### Merton Invest-Consumption Problem - Equation HJB optimization\n","\n","[Extensions of the Deep Galerkin Method](https://arxiv.org/pdf/1912.01455v3.pdf)"]},{"cell_type":"markdown","source":["##### Closed form terminal utility functions"],"metadata":{"id":"N-GO35FcJPP6"}},{"cell_type":"code","source":["def expTerminalUtilityOfWealth(x, gamma_discount = 1.0):\n","  return(-torch.exp(-gamma_discount*x))\n","\n","def expTerminalUtilityOfWealth_np(x, gamma_discount = 1.0):\n","  return(-np.exp(-gamma_discount*x))\n","\n","from functools import partial\n","\n","# should give a closed form solution for the control => PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0cWoRXs02PoF","executionInfo":{"status":"ok","timestamp":1653057376682,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"3738f121-df40-48d1-fbed-aab02d2385f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.6 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"HrivvbmubiiY"},"source":["#### MertonUtilityNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRraqOG4aXKx","executionInfo":{"status":"ok","timestamp":1653057376682,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b153fab4-db82-4ae6-d10b-9ad8b1618fdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 12.4 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}],"source":["class MertonUtilityNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh  ):\n","        super(MertonUtilityNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5 + 1  # wealth, time, mu, r, sigma, pi\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out "]},{"cell_type":"markdown","source":["#### MertonPiNet"],"metadata":{"id":"fyFbPZr7I5RE"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","class MertonPiNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.relu  ):\n","        super(MertonPiNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5   # wealth, time, mu, r, sigma\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)            \n","        # self.fc_output_d = nn.Linear(self.NN, 2)\n","        # self.fc_output = torch.nn.Softmax(dim=1)\n","        self.fc_output = nn.Linear(self.NN, 1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        # out = self.fc_output_d(h)\n","        out = self.fc_output(h)\n","        return out \n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PR7PHL4KI1S9","executionInfo":{"status":"ok","timestamp":1653057376682,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"9b7614ce-d065-4c18-ab68-9083837d5486"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 25.1 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### MertonAlternativePiNet\n","\n","[implement from github](https://github.com/Plemeur/DGM/blob/master/first_net.py)"],"metadata":{"id":"RNhAbZ727RC_"}},{"cell_type":"code","source":["class LinearWithXavier(nn.Module):\n","    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n","        TODO : figure out what to do with the bias\"\"\"\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(LinearWithXavier, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        if bias:\n","            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","    def reset_parameters(self):\n","        torch.nn.init.xavier_uniform_(self.weight)\n","        if self.bias is not None:\n","            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n","    def forward(self, input):\n","        return torch.nn.functional.linear(input, self.weight, self.bias)\n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n","\n","\n","class DGM_layer(nn.Module):\n","    \"\"\" See readme for paper source\"\"\"\n","    def __init__(self, in_features, out_feature, residual=False):\n","        super(DGM_layer, self).__init__()\n","        self.residual = residual\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature)\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=False)\n","        self.G = LinearWithXavier(out_feature, out_feature)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=False)\n","        self.R = LinearWithXavier(out_feature, out_feature)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=False)\n","        self.H = LinearWithXavier(out_feature, out_feature)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=False)\n","\n","    def forward(self, x, s):\n","        z = torch.tanh(self.UZ(x) + self.Z(s))\n","        g = torch.tanh(self.UG(x) + self.G(s))\n","        r = torch.tanh(self.UR(x) + self.R(s))\n","        h = torch.tanh(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonAlternativePiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonAlternativePiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n","        self.final_layer = LinearWithXavier(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","\n","        return self.final_layer(s)\n"],"metadata":{"id":"_c-dZ5b37NwV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653057376682,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"df6d5ffe-d617-4685-d9fe-fb144b8b2d07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 70.6 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"2IlPEu_XthB2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### MertonMatchPiNet\n","\n","[Matching Paper by hand](https://arxiv.org/abs/1912.01455v3)"],"metadata":{"id":"CNsqOm1ithSG"}},{"cell_type":"code","source":["class DGMLayerPaper(nn.Module):\n","\n","    def __init__(self, in_features, out_feature, activation=torch.relu, residual=False):\n","        \n","        super(DGMLayerPaper, self).__init__()\n","        self.residual = residual\n","        self.activation = activation\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature) # w.S\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=True) # u.x\n","        self.G = LinearWithXavier(out_feature, out_feature)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.R = LinearWithXavier(out_feature, out_feature)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.H = LinearWithXavier(out_feature, out_feature) # w.(S(o)R)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=True)\n","\n","    def forward(self, x, s):\n","        z = self.activation(self.UZ(x) + self.Z(s))\n","        g = self.activation(self.UG(x) + self.G(s))\n","        r = self.activation(self.UR(x) + self.R(s))\n","        h = self.activation(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonMatchPiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonMatchPiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGMLayerPaper(in_size, neurons) for i in range(depth)])\n","        self.final_layer = LinearWithXavier(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","\n","        return self.final_layer(s)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653057376682,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"68ec2753-1804-4d98-eda4-0f49c7fa28ea","id":"REWml4aYthSH"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 35.7 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"wClW1g9rbm8o"},"source":["#### PiEquation"]},{"cell_type":"code","source":["class PiEquation():\n","\n","    def __init__(self , pi_net, du_dx, d2u_dx2):\n","        self.pi_net = pi_net\n","        self.wgamma = 0.0001\n","        self.du_dx = du_dx\n","        self.d2u_dx2 = d2u_dx2\n","\n","    def criterion(self, x_internal):\n","      #  time, wealth, mu, r, sigma\n","      pi_net_preds = self.pi_net(x_internal)\n","      # pi_net_preds = pi_net_preds[:,0].reshape(-1,1)\n","      pi_net_preds = pi_net_preds.reshape(-1,1)\n","\n","      dpi = torch.autograd.grad( pi_net_preds, \n","                                x_internal, \n","                                grad_outputs=torch.ones_like(pi_net_preds) ,\n","                                create_graph=True,\n","                                retain_graph=True)\n","      dpi_dt = dpi[0][:,0].reshape(-1,1)\n","      dpi_dx = dpi[0][:,1].reshape(-1,1)\n","\n","      d2pi_dx2 = torch.autograd.grad( dpi_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(dpi_dx) ,\n","                                      create_graph = True,\n","                                      retain_graph=True)[0][:,1].reshape(-1,1)\n","      intC = None\n","      # pdb.set_trace()\n","      if len(x_internal) == 0:\n","        intC_loss = torch.tensor(0).cuda().float()  \n","      else:\n","        # pdb.set_trace()\n","        intC_loss = -(pi_net_preds*(x_internal[:,2].reshape(-1,1)-x_internal[:,3].reshape(-1,1)) + x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1))*self.du_dx - \\\n","                                    0.5*(x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)*self.d2u_dx2\n","\n","        # print(f\"Pi Loss {torch.mean(intC_loss).item()} {x_internal.shape[0]} {torch.mean(self.du_dx)}\")          \n","\n","        # intC_loss = (pi_net_preds*(x_internal[:,2].reshape(-1,1)-x_internal[:,3].reshape(-1,1)) + x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1))*self.du_dx + \\\n","        #                             0.5*(x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)*self.d2u_dx2\n","\n","      return  1.0*intC_loss\n","\n","    def calculatePiLoss(self, x_internal, keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        x_internal = Variable( x_internal , requires_grad=True)\n","        Ls = self.criterion( x_internal )\n","        \n","        return_losses = []\n","        if not keep_batch:\n","          loss_pi = torch.mean(Ls)           \n","          return loss_pi          \n","        else:\n","          return Ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQTTkUhWuiJi","executionInfo":{"status":"ok","timestamp":1653057377428,"user_tz":-60,"elapsed":5,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"36a802b0-9c42-484a-b3e8-c74af0350f99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 40.4 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### TrainInternalPiWithDGM\n"],"metadata":{"id":"8RRoBgFQINMv"}},{"cell_type":"code","source":["class TrainInternalPiWithDGM():\n","    \n","    def __init__(self , u_equation, pi_equation, BATCH_SIZE , epoch, lr, debug = False, loss_multiply = 1.0):\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.u_model = u_equation        \n","        self.pi_model = pi_equation\n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.pi_model.pi_net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","\n","        self.epoch = epoch\n","        self.lr = lr\n","\n","        self.loss_multiply = loss_multiply\n","        \n","    def train(self , eqLossFn = 'calculatePiLoss', sample_method_X = \"U\"):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((self.epoch, 3 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n","        # optimizer = self.optimizer_used(self.u_model.pi_net.parameters(), self.lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        \n","        loss_avg = 0.0\n","        # pdb.set_trace()\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.pi_model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(self.epoch):\n","\n","            optimizer.zero_grad()            \n","\n","            sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            loss_avg = 0.0\n","            # pdb.set_trace()\n","            loss  = loss_calc_method( sample_batch[0], keep_batch = False )            \n","            # print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) ]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              self.validation_losses[e,:] = validation_loss_list\n","            \n","            if self.use_early_stop:\n","              loss_to_check = loss\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward(retain_graph=True)\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                # print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(e , self.lr , loss))\n","\n","        self.stop_epoch = e\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VREn3fanpP1b","executionInfo":{"status":"ok","timestamp":1653057377428,"user_tz":-60,"elapsed":4,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"ef97c116-ad6f-4a9f-ea1b-48ebd78931f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 106 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}]},{"cell_type":"markdown","source":["#### MertonEquation"],"metadata":{"id":"hp4BG1ewKF6o"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"LBMZYQSPaXKy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653057377872,"user_tz":-60,"elapsed":448,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"797f6c6a-35c3-4de1-a0ac-a5fd11396bd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 959 ms (started: 2022-05-20 14:36:16 +00:00)\n"]}],"source":["import math\n","\n","class MertonEquation():\n","    \n","    def __init__(self , u_net, pi_net, pi_net_epoch, pi_net_lr, term_utility_function = partial(expTerminalUtilityOfWealth,gamma_discount=0.1) ):\n","\n","        self.u_net = u_net\n","        self.pi_net = pi_net\n","        self.wgamma = 0.0001\n","        self.term_utility_func = term_utility_function\n","        self.xbreaks = None\n","        self.tbreaks = None\n","\n","        self.MAX_X = 1.0\n","        self.T = 1.0\n","        self.MAX_MU = 0.2\n","        self.MAX_SIGMA = 1.0\n","\n","        self.pi_net_epoch = pi_net_epoch\n","        self.pi_net_lr = pi_net_lr\n","        self.loss_multiply = 1.0\n","\n","        self.FORCE_MU = None\n","        self.FORCE_R = None\n","        self.FORCE_SIGMA = None\n","\n","    def g(self,x):\n","        # Time, Wealth, Mu, R, Sigma\n","        return self.term_utility_func(x[:,1].reshape(-1,1))\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def mu_r_sample(self, size, range_multiplier = 1.0):\n","      mu_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      r_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      r_sample = torch.where(r_candidate < mu_candidate, r_candidate, mu_candidate)\n","      mu_sample = torch.where(r_candidate > mu_candidate, r_candidate, mu_candidate)\n","      return (mu_sample, r_sample)\n","\n","    def apply_forced_mu_r_sigma(self, mu_sample, r_sample, sigma_sample):\n","      if self.FORCE_MU is not None:\n","         mu_sample = self.FORCE_MU*torch.ones_like(mu_sample)            \n","      if self.FORCE_R is not None:\n","        r_sample = self.FORCE_R*torch.ones_like(r_sample)\n","      if self.FORCE_SIGMA is not None:\n","        sigma_sample = self.FORCE_SIGMA*torch.ones_like(sigma_sample)\n","      return mu_sample, r_sample, sigma_sample\n","\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","        '''\n","        Sampling function\n","        '''\n","        if sample_method_X in [\"U\"]:\n","            range_multiplier = 1.0\n","            \n","            ### internal samples of Time, Wealth, Mu, R, Sigma\n","            mu_sample_internal, r_sample_internal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_internal, r_sample_internal, sigma_sample_internal = self.apply_forced_mu_r_sigma(mu_sample_internal, r_sample_internal, sigma_sample_internal)\n","            x_internal = self.to_device(torch.cat(( torch.rand([size,1])*self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_internal, # mu\n","                                                    r_sample_internal, # R\n","                                                    sigma_sample_internal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            ### Terminal time samples\n","            mu_sample_terminal, r_sample_terminal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_terminal, r_sample_terminal, sigma_sample_terminal = self.apply_forced_mu_r_sigma(mu_sample_terminal, r_sample_terminal, sigma_sample_terminal)\n","            x_terminal = self.to_device(torch.cat(( torch.zeros(size, 1) + self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_terminal, # mu\n","                                                    r_sample_terminal, # R\n","                                                    sigma_sample_terminal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            \n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            return x_internal , x_terminal\n","\n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","        \n","    def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","\n","      if self.xbreaks is None and self.tbreaks is None:\n","        return self.sample(sample_method_X, size, to_cpu)\n","\n","      internal_strata_xts = []\n","      terminal_strata_xts = []\n","      \n","      if sample_method_X in [\"U\"]:\n","          range_multiplier = 1.0\n","          xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n","          tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n","          if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n","            xbreaks_used.append(range_multiplier*self.MAX_X)\n","          while xbreaks_used[0] < 0.0:\n","            xbreaks_used.pop(0)\n","          if not xbreaks_used:\n","            xbreaks_used = [0,range_multiplier*self.MAX_X]\n","          if xbreaks_used[0] > 0.0:            \n","            xbreaks_used.insert(0, 0.0)\n","\n","          if tbreaks_used[-1] < self.T:\n","            tbreaks_used.append(self.T)\n","          xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n","          tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n","\n","          total_strat_processed = 0\n","\n","          # internal samples\n","          for stratum_x_count in range(len(xbreaks_used)-1):\n","              \n","            num_samples_in_stratum = 0\n","            if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n","              range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n","              num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n","\n","            for stratum_t_count in range(len(self.tbreaks)-1):\n","\n","              if num_samples_in_stratum == 0: # there is only a T division, so use it\n","                range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n","                num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n","              else:\n","                # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n","                stratum_coverage_on_unit_square = \\\n","                  ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n","                  ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n","                num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n","\n","              range_multiplier = 1.0\n","\n","              ### internal samples of Time, Wealth, Mu, R, Sigma\n","              internal_stratum_t_sample = tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])\n","              internal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_internal, stratum_r_sample_internal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal)\n","              x_internal_stratum = self.to_device(torch.cat(( internal_stratum_t_sample , # Time\n","                                                              internal_stratum_x_sample, # Wealth\n","                                                              stratum_mu_sample_internal, # mu\n","                                                              stratum_r_sample_internal, # R\n","                                                               # Sigma\n","                                                            ) , dim = 1 ),to_cpu)\n","              if not internal_strata_xts: \n","                internal_strata_xts = [ x_internal_stratum ] \n","              else:\n","                internal_strata_xts.append(x_internal_stratum) \n","\n","              ### Terminal time samples\n","              terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal)\n","              x_terminal_stratum = self.to_device(torch.cat(( torch.zeros(num_samples_in_stratum, 1) + self.T , # Time\n","                                                      terminal_stratum_x_sample, # Wealth\n","                                                      stratum_mu_sample_terminal, # mu\n","                                                      stratum_r_sample_terminal, # R\n","                                                      stratum_sigma_sample_terminal # Sigma\n","                                                    ) , dim = 1 ),to_cpu)\n","              if not terminal_strata_xts:\n","                terminal_strata_xts = [ x_terminal_stratum ] # terminal_stratum_xt[None,:,:]\n","              else:\n","                terminal_strata_xts.append(x_terminal_stratum) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n","\n","              total_strat_processed += 1 \n","              # print((len(internal_strata_xts),xbreaks_used[stratum_x_count],tbreaks_used[stratum_t_count]))\n","\n","          # pdb.set_trace()\n","          # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","          return internal_strata_xts , terminal_strata_xts\n","    \n","      raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","\n","    def criterion(self, x_internal , x_terminal, loss_transforms = [torch.square]):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # Time / Wealth / Mu / r / Sigma (sample data order)\n","        # pdb.set_trace()\n","\n","        # replace with closed form just to check\n","        # self.pi_net = lambda x: (((x[:,2]-x[:,3])/(1.0*(x[:,4]**2)))*torch.exp(-x[:,3]*(1.0-x[:,0])))\n","\n","        # pdb.set_trace()\n","        pi_used = self.pi_net(x_internal)  \n","        pi_used.detach_()\n","        # let's assign the first column as the allocation\n","        # pdb.set_trace()     \n","        # pi_used = pi_used[:,0].reshape(-1,1)\n","        pi_used = pi_used.reshape(-1,1)\n","        x_internal_before = x_internal.detach().clone()\n","        x_internal =  Variable(torch.cat((x_internal, pi_used), dim=1),requires_grad=True)\n","\n","        du = torch.autograd.grad( self.u_net(x_internal), \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(self.u_net(x_internal)) ,\n","                                  create_graph=True,\n","                                  retain_graph=True )\n","        \n","        du_dt = du[0][:,0].reshape(-1,1)\n","        du_dx = du[0][:,1].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(  du_dx, \n","                                        x_internal , \n","                                        grad_outputs=torch.ones_like(du_dx) ,\n","                                        create_graph = True,\n","                                        retain_graph = True)[0][:,1].reshape(-1,1)\n","    \n","        # def pi_net_fn(x,du_dx = du_dx,d2u_dx2 = d2u_dx2): \n","        #   return (-(((x[:,2]-x[:,3])/(1.0*(x[:,4]**2)))*torch.div(du_dx,d2u_dx2).reshape(-1)))\n","\n","        # pi_net_fn2 = lambda x: (((x[:,2]-x[:,3])/(1.0*(x[:,4]**2)))*torch.exp(-x[:,3]*(1.0-x[:,0])))\n","\n","        pi_model = PiEquation(self.pi_net, du_dx, d2u_dx2)                \n","        pi_trainer = TrainInternalPiWithDGM(self, pi_model, x_internal.shape[0], \n","                                            self.pi_net_epoch, self.pi_net_lr, \n","                                            debug=True, loss_multiply=1.0)\n","        pi_trainer.use_early_stop = True\n","        pi_trainer.early_stop_patience = min(200,math.ceil(self.pi_net_epoch/10.0))\n","        pi_trainer.train()\n","        \n","        # self.pi_net =  pi_net_fn\n","        # self.pi_net =  pi_net_fn2\n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]\n","\n","        intC = None\n","        terC = None\n","\n","        if len(x_internal) == 0:\n","          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          # Time, Wealth, Mu, R, Sigma\n","          # pdb.set_trace()\n","          pi_net_preds = self.pi_net(x_internal_before)\n","          pi_net_preds.detach_()\n","          # pi_net_preds = pi_net_preds[:,0].reshape(-1,1)\n","          pi_net_preds = pi_net_preds.reshape(-1,1)\n","          intC_loss = du_dt + (pi_net_preds*(x_internal[:,2].reshape(-1,1)-x_internal[:,3].reshape(-1,1))+x_internal[:,3].reshape(-1,1)*x_internal[:,1].reshape(-1,1))*du_dx + 0.5*(x_internal[:,4].reshape(-1,1)**2)*(pi_net_preds**2)*d2u_dx2\n","          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        x_terminal_before = x_terminal.detach().clone()\n","        pi_net_preds_terminal = self.pi_net(x_terminal_before)\n","        pi_net_preds_terminal.detach_()\n","        # pi_net_preds_terminal = pi_net_preds_terminal[:,0].reshape(-1,1)\n","        pi_net_preds_terminal = pi_net_preds_terminal.reshape(-1,1)\n","        x_terminal =  Variable(torch.cat((x_terminal, pi_net_preds_terminal), dim=1),requires_grad=True)\n","\n","        terC = [ loss_transform( self.u_net(x_terminal) - self.g(x_terminal)   ) for loss_transform in loss_transforms ]\n","\n","        return  intC , terC\n","\n","    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        # pdb.set_trace()\n","        x_internal , x_terminal = batch_x\n","        x_internal = Variable( x_internal , requires_grad=True)\n","        Ls = self.criterion( x_internal , x_terminal, loss_transforms = loss_transforms )\n","        intC , terC  = Ls\n","\n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc])\n","            return_losses.append( [ loss_equalWeightedByType , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]), \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC.numpy(), terC.numpy()] )\n","        return return_losses\n","\n","    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , loss_transforms = loss_transforms)\n","        intC , terC = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n","            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt)\n","            return_losses.append( [ transformed_loss , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","\n","    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal  = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal, loss_transforms = loss_transforms)\n","        intC , terC  = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n","            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt )\n","            return_losses.append( [ transformed_loss , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","    "]},{"cell_type":"markdown","metadata":{"id":"65nooklCbsdy"},"source":["#### TrainHJBMertonWithDGM"]},{"cell_type":"code","source":["def attach_pi_used(x, pi_net, requires_grad=True):\n","  pi_used = pi_net(x)  \n","  pi_used.detach_()\n","  # pi_used = pi_used[:,0].reshape(-1,1)\n","  pi_used = pi_used.reshape(-1,1)\n","  \n","  before_x = x.detach().clone()\n","  new_x =  Variable(torch.cat((x, pi_used), dim=1),requires_grad=requires_grad)\n","  return before_x, new_x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74jqGI3o2GlN","executionInfo":{"status":"ok","timestamp":1653057377873,"user_tz":-60,"elapsed":5,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"533bd1c2-b802-4bc6-d26d-67ba0b46c81f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.32 ms (started: 2022-05-20 14:36:17 +00:00)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtO8fV7oaXK2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653057378941,"user_tz":-60,"elapsed":1071,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"4c7b7525-b07a-43e5-e475-3f27ffa2162f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 697 ms (started: 2022-05-20 14:36:17 +00:00)\n"]}],"source":["class TrainHJBMertonWithDGM():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","        \n","\n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\", key_loss_func = torch.square, huber_delta = 0.5):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((epoch, 3*4 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        \n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","\n","            optimizer.zero_grad()\n","            \n","            # pdb.set_trace()\n","            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            losses_L2, losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n","            # pdb.set_trace()\n","            loss , internal , terminal , losses_equalWeightedByType = losses_L2\n","            loss_abs , internal_abs , terminal_abs , losses_equalWeightedByType_abs = losses_ABS\n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) , to_cpu_detach(internal) , to_cpu_detach(terminal) , \n","                                       to_cpu_detach(loss_abs) , to_cpu_detach(internal_abs) , to_cpu_detach(terminal_abs), \n","                                       to_cpu_detach(losses_equalWeightedByType_abs)]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss_list = validation_loss_list.pop(5) # the L2 loss is duplicated at index 1\n","              self.validation_losses[e,:] = validation_loss_list\n","              # pdb.set_trace()\n","              # print(f\"Epoch {e} - Pi Pred (0.47) {self.model.pi_net(self.validation_sample[0]).item()}\")\n","            \n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","\n","                loss_avg = loss_avg/self.hook_interval\n","                # pdb.set_trace()\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                print(f\"Epoch {e} - lr {lr} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                \n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal = self.validation_sample\n","                    xinternal_before, xinternal_expanded = attach_pi_used(xinternal, self.model.pi_net, requires_grad=False)\n","                    xterminal_before, xterminal_expanded = attach_pi_used(xterminal, self.model.pi_net, requires_grad=False)\n","\n","                    xinternal_res = self.model.u_net(xinternal_expanded).detach()\n","                    xterminal_res = self.model.u_net(xterminal_expanded).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\", \"Mu\", \"R\", \"Sigma\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n","    def train_stratified(self , epoch , lr, \n","                         eqLossFn = 'calculateLoss', \n","                         sample_method_X = \"U\", \n","                         key_loss_func = torch.square, \n","                         huber_delta = 0.5\n","                         ):\n","        \n","        self.validation_losses = np.ones((epoch, 3*3 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1), dtype='float32') * np.nan\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            # pdb.set_trace()\n","            internal_xts_bts, terminal_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n","            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n","            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n","            \n","            # pdb.set_trace()\n","            for stratum_count in range(len(internal_xts_bts)):              \n","              sample_batch = (internal_xts_bts[stratum_count], \n","                              terminal_xts_bts[stratum_count])  \n","\n","              # pdb.set_trace()\n","              stratum_losses_L2, stratum_losses_ABS = loss_calc_method(sample_batch, \n","                                                                       loss_transforms = [ key_loss_func, torch.abs ], \n","                                                                       keep_batch = False )\n","              # if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n","              #   pdb.set_trace()\n","              #   pass\n","            \n","              if training_stratum_losses is not None:\n","                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n","              else:\n","                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n","\n","              # pdb.set_trace()  \n","              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n","\n","            # pdb.set_trace()              \n","            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n","            loss = training_value_to_optimize\n","\n","            loss_optimized , internal , terminal, losses_equalWeightedByType, \\\n","            loss_abs , internal_abs , terminal_abs ,losses_equalWeightedByType_abs = training_loss_for_epoch            \n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n","                loss_calc_method( self.validation_sample, \n","                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                  keep_batch = False )\n","              validation_loss = [*to_cpu_detach(losses_L2_validation),\n","                                              *to_cpu_detach(losses_ABS_validation),\n","                                              *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n","              self.validation_losses[e,:] = validation_loss\n","\n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n"]},{"cell_type":"markdown","source":["### Test Case"],"metadata":{"id":"oy05I1QFh7EM"}},{"cell_type":"markdown","source":["#### Test Case NO Stratification"],"metadata":{"id":"U7zqglm1ewTL"}},{"cell_type":"code","source":["mequation = MertonEquation(MertonUtilityNet( NL = 1 , NN = 3 ), MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 ), 1, 10000.0)\n","# val_sample_to_use = tuple([ x.cpu().detach() for x in mequation.sample(sample_method_X=\"U\", size=1) ] )\n","val_sample_to_use = mequation.sample(sample_method_X=\"U\", size=1) \n","# # gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   \n","val_sample_to_use[0][0,0] = 0.0\n","val_sample_to_use[0][0,2] = 0.05\n","val_sample_to_use[0][0,3] = 0.02\n","val_sample_to_use[0][0,4] = 0.25"],"metadata":{"id":"-yLM2WbUoymO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mequation.sample(sample_method_X=\"U\", size=1)"],"metadata":{"id":"ul_C1i9oos6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pi_net2 = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 )"],"metadata":{"id":"g099sPpvXwo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","# u_net, pi_net, pi_net_epoch, pi_net_lr\n","eqLossFn= 'calculateLoss'\n","sample_method= \"U\"\n","lr = 0.1\n","lr_for_pi = 0.1\n","max_pi_epochs = 1 # has to be low!!!\n","\n","u_net = MertonUtilityNet( NL = 6 , NN = 100 )\n","u_net.to(torch.device(\"cuda:0\")) \n","pi_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 )\n","pi_net.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","mequation = MertonEquation(u_net, pi_net, max_pi_epochs, lr_for_pi)\n","# mequation.FORCE_MU = 0.05\n","# mequation.FORCE_R = 0.02\n","# mequation.FORCE_SIGMA = 0.25\n","trainMertonAlloc = TrainHJBMertonWithDGM(u_net, mequation, BATCH_SIZE = 2**10 , debug = True )\n","trainMertonAlloc.hook_interval = 100\n","trainMertonAlloc.use_early_stop = True\n","trainMertonAlloc.early_stop_patience = 1000\n","# trainMertonAlloc.validation_sample = val_sample_to_use\n","trainMertonAlloc.train(epoch = 40000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"outputId":"e1a7e558-6a34-408c-d461-8645bdac7be3","id":"IUNDoPJRe6dL","executionInfo":{"status":"error","timestamp":1653057321260,"user_tz":-60,"elapsed":464,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-c4542759537c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtrainMertonAlloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# trainMertonAlloc.validation_sample = val_sample_to_use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtrainMertonAlloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqLossFn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqLossFn\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msample_method_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-66-fe167f929d0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch, lr, eqLossFn, sample_method_X, key_loss_func, huber_delta)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0msample_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_method_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_method_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mlosses_L2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_ABS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc_method\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkey_loss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;31m# pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minternal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlosses_equalWeightedByType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_L2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-9f788ea0f2c0>\u001b[0m in \u001b[0;36mcalculateLoss\u001b[0;34m(self, batch_x, train, loss_transforms, keep_batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mx_internal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mx_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_internal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mLs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_internal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_terminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_transforms\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mintC\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mterC\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-64-9f788ea0f2c0>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(self, x_internal, x_terminal, loss_transforms)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mpi_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_early_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mpi_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_net_epoch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mpi_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# self.pi_net =  pi_net_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-2a27fcabc7f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, eqLossFn, sample_method_X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mloss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mloss_calc_method\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;31m# print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-62-d268600e75af>\u001b[0m in \u001b[0;36mcalculatePiLoss\u001b[0;34m(self, x_internal, keep_batch)\u001b[0m\n\u001b[1;32m     46\u001b[0m         '''        \n\u001b[1;32m     47\u001b[0m         \u001b[0mx_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_internal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mLs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_internal\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mreturn_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-62-d268600e75af>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(self, x_internal)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi_net_preds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                 retain_graph=True)\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mdpi_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mdpi_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    275\u001b[0m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    276\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 23.75 MiB free; 14.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]},{"output_type":"stream","name":"stdout","text":["time: 190 ms (started: 2022-05-20 14:35:20 +00:00)\n"]}]},{"cell_type":"code","source":["!nvidia-smi\n","import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF']='max_split_size_mb:1'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsO1pos34g98","executionInfo":{"status":"ok","timestamp":1653057344268,"user_tz":-60,"elapsed":1368,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"c3336a7e-0de1-46e3-ef6a-07f50251e5a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri May 20 14:35:42 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# check control for closed form:\n","# PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))\n","# ((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))\n","# gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   # PI\n","\n","gamma = 1.0\n","internal_sample, terminal_sample = mequation.sample(size=100, to_cpu=False)\n","mask = (internal_sample[:,0] > 0.1) & (internal_sample[:,4] > 0.1)\n","internal_sample = internal_sample[mask.reshape(-1),:]\n","# time, wealth, mu, r, sigma\n","time = internal_sample[:,0].cpu().detach()\n","wealth = internal_sample[:,1].cpu().detach()\n","mu = internal_sample[:,2].cpu().detach()\n","r = internal_sample[:,3].cpu().detach()\n","sigma = internal_sample[:,4].cpu().detach()\n","\n","# mequation.pi_net(internal_sample)[:,0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u35g5sAPbsG_","executionInfo":{"status":"ok","timestamp":1653051009822,"user_tz":-60,"elapsed":282,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"e11af765-669a-4735-991d-5b675addd2eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 33.5 ms (started: 2022-05-20 12:50:09 +00:00)\n"]}]},{"cell_type":"code","source":["# closed form value function\n","def Htx(x):\n","  return -torch.exp(-x[:,1].reshape(-1,1)*1.0*torch.exp(x[:,3].reshape(-1,1)*(1.0-x[:,0].reshape(-1,1))) - \n","                    0.5*(1.0-x[:,0].reshape(-1,1))*((x[:,2].reshape(-1,1)-x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-NmMG33xGBu","executionInfo":{"status":"ok","timestamp":1653051010948,"user_tz":-60,"elapsed":4,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"a097e2dc-b72e-405a-c102-0ac06be4a1de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.27 ms (started: 2022-05-20 12:50:10 +00:00)\n"]}]},{"cell_type":"code","source":["# plot the fitted value function vs the closed form (ideally straight line...)\n","u_internal_sample = torch.cat((internal_sample, mequation.pi_net(internal_sample).reshape(-1,1)), dim=1)\n","u_net_results = u_net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n","htx_results = Htx(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist()\n","dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n","ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"Woj8BoAK5OLC","executionInfo":{"status":"ok","timestamp":1653051012025,"user_tz":-60,"elapsed":558,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"cb6bd028-9682-4446-ccb1-59a46345da6b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<ggplot: (8754503739677)>"]},"metadata":{},"execution_count":36},{"output_type":"stream","name":"stdout","text":["time: 319 ms (started: 2022-05-20 12:50:11 +00:00)\n"]}]},{"cell_type":"code","source":["# plot the control function vs the closed form (ideally straight line...)\n","dataf = pd.DataFrame( { 'pi_net': mequation.pi_net(internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n","                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n","ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"GHfTC6GljYnd","executionInfo":{"status":"ok","timestamp":1653051017877,"user_tz":-60,"elapsed":339,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"272b93e5-dd24-43dd-f240-677b0a12b869"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<ggplot: (8754503354661)>"]},"metadata":{},"execution_count":37},{"output_type":"stream","name":"stdout","text":["time: 267 ms (started: 2022-05-20 12:50:17 +00:00)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"f85Kg2zvhMdM"},"execution_count":null,"outputs":[]}]}