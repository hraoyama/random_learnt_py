{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deepAR_benchmark.ipynb","provenance":[],"authorship_tag":"ABX9TyOna5OG5//kg4TJlG3fneEO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# https://github.com/alphaj-jaeminyx/DeepAR-keras/blob/master/DeepAR.ipynb\n","\n"],"metadata":{"id":"LUfriy77Wwpq"}},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow_probability as tfp\n","from keras.layers import Input, Dense, LSTM, Dropout, TimeDistributed, Lambda\n","\n","# LSTM To estimate the Gaussian distribution based on Simple DeepAR\n","# If you know the principle, you can make anything..\n","\n","# challenge\n","# 1. convert to Seq2Seq \n","# 2. LSTM introduction of a model other than (CNN-LSTM, GRU)\n","# 3. Try changing the distribution function => change the distribution estimation coefficient mu, sigma It is not and needs to be changed to a different value.\n","# 2주 정도 시간 들여서 해보기. (6주차에 솔루션 업로드)\n","# 4. 분포 함수 자체를 딥러닝으로 모델링 하기.  (5,6주차 때 다룰 예정, GAN/AutoEncoder)\n","\n","class DeepAR(tf.keras.models.Model):\n","    def __init__(self, lstm_units, n_steps_in, n_steps_out, n_features):\n","        super().__init__()\n","\n","        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state=True, input_shape=(n_steps_in, n_features))\n","        self.dense_mu = tf.keras.layers.Dense(1) # 정규분포 (가우시안 분포)의 평균\n","        self.dense_sigma = tf.keras.layers.Dense(1, activation='softplus') # 정규분포 (가우시안 분포)의 시그마\n","\n","    def call(self, inputs, initial_state=None):\n","        outputs, state_h, state_c = self.lstm(inputs, initial_state=initial_state)\n","\n","        mu = self.dense_mu(outputs) # 정규분포의 평균을 추정하는 레이어\n","        sigma = self.dense_sigma(outputs) # 정규분포의 표준편차를 추정하는 레이어\n","        state = [state_h, state_c]\n","\n","        return [mu, sigma, state]\n","\n","# 모델의 로스를 계산하는 부분,\n","def log_gaussian_loss(mu, sigma, y_true):\n","    \"\"\"\n","    Gaussian loss function\n","    \"\"\"\n","                              # 모델링된 mu와 sigma를 가지고 분포 생성\n","                                                                    # y_true=실제값과 분포에서 샘플링된 값의 차이를 구함.\n","    return -tf.reduce_sum(tfp.distributions.Normal(loc=mu, scale=sigma).log_prob(y_true))\n"],"metadata":{"id":"aco_pj3oVMpl","executionInfo":{"status":"ok","timestamp":1644325343400,"user_tz":0,"elapsed":431,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime, timedelta\n","\n","df = pd.read_csv('/content/drive/MyDrive/시계열_교안/3주차/Gemini_ETHUSD_d.csv',  skiprows=1, parse_dates=True, index_col='Date')\n","df = df.sort_index().drop(['Symbol','Unix Timestamp'] , axis=1)\n","\n","test_cutoff_date = df.index.max() - timedelta(days=90)\n","\n","df_test = df[df.index > test_cutoff_date]['Close']\n","df_train = df[df.index <= test_cutoff_date]['Close']\n","\n","\n","############# 데이터를 정규화\n","training_mean = df_train.mean() # train의 평균\n","training_std = df_train.std() # train의 편차\n","df_training_value = (df_train - training_mean) / training_std # 데이터 = (원본 - 평균) / 편차로 정규화\n","\n","test_mean = df_test.mean() # test의 평균\n","test_std = df_test.std() # test의 편차\n","df_test_value = (df_test - test_mean) / test_std # 데이터 = (원본 - 평균) / 편차로 정규화\n","\n","\n","##############\n","\n","from numpy import array\n","\n","def split_sequence(sequence, n_steps_in, n_steps_out):\n","\tX, y = list(), list()\n","\tfor i in range(len(sequence)):\n","\t\tend_ix = i + n_steps_in\n","\t\tout_end_ix = end_ix + n_steps_out\n","\t\tif out_end_ix > len(sequence):\n","\t\t\tbreak\n","\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n","\t\tX.append(seq_x)\n","\t\ty.append(seq_y)\n","\treturn array(X), array(y)\n","\n","raw_seq = df_training_value\n","n_steps_in = 30\n","n_steps_out = 1\n","\n","X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n","\n","n_features = 1\n","X = X.reshape((X.shape[0], X.shape[1], n_features))\n","y = y.reshape((y.shape[0], y.shape[1], n_features))\n","\n","test_raw_seq = df_test_value\n","\n","test_X, test_y = split_sequence(test_raw_seq, n_steps_in, n_steps_out)\n","test_X = test_X.reshape((test_X.shape[0], test_X.shape[1], n_features))\n","\n","\n"],"metadata":{"id":"lVloGOeaVRro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LSTM_UNITS = 128\n","EPOCHS = 100\n","\n","model = DeepAR(LSTM_UNITS, n_steps_in, n_steps_out, n_features)\n","\n","# optmizer\n","optimizer = tf.keras.optimizers.Adam()\n","\n","\n","# metric\n","rmse = tf.keras.metrics.RootMeanSquaredError()\n","\n","def train_step(x, y):\n","    with tf.GradientTape() as tape:\n","        mu, sigma, _ = model(x)\n","        loss = log_gaussian_loss(mu, sigma, y) # Forward Learning\n","    # backword\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","    rmse(y, mu)\n","\n","for epoch in range(EPOCHS):\n","    train_step(X, y)\n","    print('Epoch %d, RMSE %.4f' % (epoch + 1, rmse.result().numpy()))\n","    rmse.reset_states()\n"],"metadata":{"id":"xi6O6QruWeiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# 샘플링된 값들을 예측값으로 반환.\n","pred = model.predict(test_X)\n","\n"],"metadata":{"id":"WgfWaRIfWfRz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","lower_bound = []\n","upper_bound = []\n","median = []\n","for step_pred in pred[0]:\n","                           # 정규화된 값을 원래대로.\n","  lb = (np.quantile(step_pred, 0.05) + test_mean) * test_std\n","  ub = (np.quantile(step_pred, 0.95) + test_mean) * test_std\n","  med = (np.quantile(step_pred, 0.5) + test_mean) * test_std\n","  lower_bound.append(lb)\n","  upper_bound.append(ub)\n","  median.append(med)\n"],"metadata":{"id":"t4K90LrgWirI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","fig = plt.figure(figsize = (12,6))\n","ax = fig.add_subplot(111)\n","\n","\n","df['Close'].plot(ax=ax)\n","ax.vlines('2020-08-22', 0, 1000, linestyle='--', color='r', label='forecast boundary')\n","ax.fill_between(df_test[30:].index, lower_bound, upper_bound, color='b', alpha=0.1, label='95% Confidence Interval')\n","ax.plot(df_test[30:].index, median, label='Prediction')\n","ax.legend(loc='upper left')\n","# plt.suptitle(f\"ARIMA {optimal[0][0]} Prediction Result (r2 score: {r2}\")\n","# plt.show()"],"metadata":{"id":"3uZKQ8evWmvQ"},"execution_count":null,"outputs":[]}]}