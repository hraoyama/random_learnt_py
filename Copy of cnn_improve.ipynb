{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of cnn_improve.ipynb","provenance":[{"file_id":"1wmrmlcZwMgk0tLmb-3rxssy5_xpQ7siK","timestamp":1629731815721}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1wmrmlcZwMgk0tLmb-3rxssy5_xpQ7siK","authorship_tag":"ABX9TyNHApxf5/S27lBAy7INE9dJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6EWYZ7i3I1AW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629730976847,"user_tz":-60,"elapsed":7068,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"1ea0d89f-8f04-4ece-ac6c-31dda32c2b64"},"source":["%pip install keras-tuner\n","%pip install keras_self_attention"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.0.3-py3-none-any.whl (96 kB)\n","\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 4.8 MB/s \n","\u001b[?25hCollecting kt-legacy\n","  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.0.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.0.3 kt-legacy-1.0.4\n","Collecting keras_self_attention\n","  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.6.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=31096ddca1d3e8311750748d7a08fbf7f0ca666b6f69d0456ee26943027d753b\n","  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.50.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56KS77UD34pw","executionInfo":{"status":"ok","timestamp":1629730979101,"user_tz":-60,"elapsed":202,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"0086e9d3-b062-4a29-8633-b90300ea6a27"},"source":["!nvidia-smi -L"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-58d78ef8-7906-567a-fd04-b03938d357e4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iUTGIn5nImcI"},"source":["import tensorflow as tf\n","import site\n","import pandas as pd\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n","from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n","import os\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mC-h14gTJJeE","executionInfo":{"status":"ok","timestamp":1629730986527,"user_tz":-60,"elapsed":278,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"4e0e43dd-8368-46f1-8584-fcc1a02ba5e2"},"source":["tf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.6.0'"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"j64-lcWdJO6G"},"source":["import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","import errno\n","import os\n","from collections import defaultdict\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.summary import create_file_writer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import itertools\n","import multiprocessing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNqzrfwimwIG"},"source":["def coShuffled_vectors(X, Y):\n","    if tf.shape(X)[0] == tf.shape(Y)[0]:\n","        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n","        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n","        return (tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs))\n","    else:\n","        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n","\n","\n","def getNpArrayFromH5(hf_Data):\n","    X_train = hf_Data['Train_Data']  # Get train set\n","    X_train = np.array(X_train)\n","    Y_train = hf_Data['Label']  # Get train label\n","    Y_train = np.array(Y_train)\n","    return X_train, Y_train\n","\n","# data extraction\n","def getData(is500=True, shuffle=False, ise2e=False, include_secondary=False, validation_split=None, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Train_Data\" if ise2e else \"Fold_10_Train_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Test_Data\" if ise2e else \"Fold_10_Test_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    X_validation = Y_validation = None\n","    if validation_split is not None:\n","        # sklearn split shuffles anyway\n","        X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_split)\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eData(is500=True, shuffle=False, include_secondary=False, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Secondary_Data_1136.h5', 'r') if include_secondary else h5.File(\n","        f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eDataJustSecondary(shuffle=False,isColab=False):\n","    hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_just_Secondary_Data_1000.h5', 'r')\n","    hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_just_Secondary_Data_1000.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_just_Secondary_Data_1000.h5', 'r')\n","    \n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDKkbD1fJVpN"},"source":["def plot_history(history):\n","    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n","    for k, v in history.history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        else:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()\n","\n","\n","def get_layer_by_name(layers, name, return_first=True):\n","    matching_named_layers = [l for l in layers if l.name == name]\n","    if not matching_named_layers:\n","        return None\n","    return matching_named_layers[0] if return_first else matching_named_layers\n","\n","\n","def get_combined_features_from_models(\n","        to_combine,\n","        X_train, Y_train,\n","        X_test, Y_test,\n","        reverse_one_hot=False,\n","        normalize_X_func=None):\n","    models = []\n","    models_dict = {}\n","    X_trains_out = []\n","    X_test_out = []\n","    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n","\n","    if reverse_one_hot:\n","        Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train) + 1\n","        Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test) + 1\n","    else:\n","        Y_train_new = Y_train.copy()\n","        Y_test_new = Y_test.copy()\n","\n","    for model_file_name, layer_name, kwargs in to_combine:\n","        model_here = None\n","        if isinstance(model_file_name, tf.keras.models.Model):\n","            model_here = model_file_name\n","            model_file_name = model_here.name\n","        else:\n","            if model_file_name in models_dict.keys():\n","                model_here = models_dict[model_file_name]\n","            else:\n","                model_here = tf.keras.models.load_model(model_file_name,\n","                                                        **kwargs) if kwargs is not None else tf.keras.models.load_model \\\n","                    (model_file_name)\n","\n","        features_model = Model(model_here.input,\n","                               get_layer_by_name(model_here.layers, layer_name).output)\n","        if normalize_X_func is None:\n","            X_trains_out.append(np.array(features_model.predict(X_train), dtype='float64'))\n","            X_test_out.append(np.array(features_model.predict(X_test), dtype='float64'))\n","        else:\n","            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train)), dtype='float64'))\n","            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test)), dtype='float64'))\n","        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n","        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n","        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n","        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n","        models.append(((model_file_name, layer_name), (model_here, features_model)))\n","        models_dict[model_file_name] = model_here\n","\n","    X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n","    X_test_new = np.concatenate(tuple(X_test_out), axis=1)\n","\n","    data_train = (X_train_new, Y_train_new)\n","    data_test = (X_test_new, Y_test_new)\n","\n","    return models, data_train, data_test, XY_dict\n","\n","\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","\n","def source_model(model_func, model_name, input_shape):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    return m\n","\n","\n","def compile_and_fit_model_with_tb(model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_every_epoch=True,\n","                                  save_final=False,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    tb_callback = TensorBoard(log_dir=f'{m.name}_logs', histogram_freq=kwargs.pop(\"histogram_freq\", 1))\n","    if save_every_epoch:\n","        tb_callback.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=[tb_callback], verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","    # m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n","\n","\n","def compile_and_fit_model(model_func,\n","                          model_name,\n","                          input_shape,\n","                          X_train,\n","                          Y_train,\n","                          save_every_epoch=True,\n","                          save_final=False,\n","                          **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_every_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","\n","\n","def compile_model_and_fit_with_custom_loop(model_func,\n","                                           model_name,\n","                                           input_shape,\n","                                           X_train,\n","                                           Y_train,\n","                                           **kwargs):\n","    make_dir_if_not_exist(model_name)\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    train_writer = create_file_writer(f'{m.name}_logs/train/')\n","    test_writer = create_file_writer(f'{m.name}_logs/test/')\n","    train_step = test_step = 0\n","\n","    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","    optimizer = tf.keras.optimizers.Adam()\n","    num_epochs = kwargs.get(\"epochs\", 10)\n","\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    BATCH_SIZE = kwargs.get(\"batch_size\", 32)\n","    X_test, Y_test = kwargs.get(\"validation_data\", (None, None))\n","    if X_test is None:\n","        raise ValueError(\"Missing X validation data\")\n","    if Y_test is None:\n","        raise ValueError(\"Missing Y validation data\")\n","\n","    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n","    train_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    train_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n","    test_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    test_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","\n","    for epoch in range(num_epochs):\n","        # Iterate through training set\n","        for batch_idx, (x, y) in enumerate(train_dataset_tf):\n","            with tf.GradientTape() as tape:\n","                y_pred = m(x, training=True)\n","                loss = loss_fn(y, y_pred)\n","\n","            gradients = tape.gradient(loss, m.trainable_weights)\n","            optimizer.apply_gradients(zip(gradients, m.trainable_weights))\n","            acc_metric.update_state(y, y_pred)\n","\n","            with train_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=train_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=train_step,\n","                )\n","                train_step += 1\n","        # Reset accuracy in between epochs (and for testing and test)\n","        acc_metric.reset_states()\n","        # Iterate through test set\n","        for batch_idx, (x, y) in enumerate(test_dataset_tf):\n","            y_pred = m(x, training=False)\n","            loss = loss_fn(y, y_pred)\n","            acc_metric.update_state(y, y_pred)\n","            with test_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=test_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=test_step,\n","                )\n","                test_step += 1\n","\n","        acc_metric.reset_states()  # Reset accuracy in between epochs (and for testing and test)\n","\n","    return m\n","\n","\n","def reinitialize_weights(model):\n","    for ix, layer in enumerate(model.layers):\n","        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n","            weight_initializer = model.layers[ix].kernel_initializer\n","            bias_initializer = model.layers[ix].bias_initializer\n","\n","            old_weights, old_biases = model.layers[ix].get_weights()\n","\n","            model.layers[ix].set_weights([\n","                weight_initializer(shape=old_weights.shape),\n","                bias_initializer(shape=len(old_biases))])\n","    return model\n","\n","\n","def reverse_tensor(X):\n","    return tf.gather(X, tf.reverse(tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32), (0,)))\n","\n","\n","def run_mirrored_strategy(model_func, base_batch_size, nepochs, x_train, y_train, x_test, y_test, **kwargs):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        model = model_func()\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adam(),\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","            metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n","        )\n","    batch_size_mirr_strat = base_batch_size * strategy.num_replicas_in_sync\n","    history = model.fit(x_train, y_train, epochs=nepochs, batch_size=batch_size_mirr_strat,\n","                        validation_data=(x_test, y_test),\n","                        **kwargs)\n","    return model, history\n","\n","\n","def sparse_setdiff(a1, a2):\n","    a1a = a1.reshape(a1.shape[0], -1)\n","    a2a = a2.reshape(a2.shape[0], -1)\n","    spa2a = [np.where(x)[0].tolist() for x in a2a]\n","    spa1a = [np.where(x)[0].tolist() for x in a1a]\n","    idxs_to_keep = []\n","    for idx, sample in enumerate(spa1a):\n","        try:\n","            spa2a.index(sample)\n","        except ValueError:\n","            # not in list\n","            idxs_to_keep.append(idx)\n","    return a1[idxs_to_keep], idxs_to_keep\n","\n","\n","def unpacking_apply_along_axis(all_args):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but with arguments in a tuple\n","    instead.\n","\n","    This function is useful with multiprocessing.Pool().map(): (1)\n","    map() only handles functions that take a single argument, and (2)\n","    this function can generally be imported from a module, as required\n","    by map().\n","    \"\"\"\n","    (func1d, axis, arr, args, kwargs) = all_args\n","    # return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n","\n","\n","def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but takes advantage of multiple\n","    cores.\n","    \"\"\"\n","    # Effective axis where apply_along_axis() will be applied by each\n","    # worker (any non-zero axis number would work, so as to allow the use\n","    # of `np.array_split()`, which is only done on axis 0):\n","    effective_axis = 1 if axis == 0 else axis\n","    if effective_axis != axis:\n","        arr = arr.swapaxes(axis, effective_axis)\n","\n","    # Chunks for the mapping (only a few chunks):\n","    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n","              for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n","\n","    pool = multiprocessing.Pool()\n","    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n","    # Freeing the workers:\n","    pool.close()\n","    pool.join()\n","\n","    return np.concatenate(individual_results)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sx3l8P--JkEI"},"source":["def baseline_CNN_finalist(model_name, inshape, num_classes = 13):\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(tf.keras.layers.Conv1D(128 ,10 ,padding='same' ,input_shape=inshape))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(128 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Flatten())\n","\n","    model.add(tf.keras.layers.Dense(128))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(64))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n","    model._name = model_name\n","\n","    return model\n","\n","\n","def compile_and_fit_model_basic(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor='val_accuracy',\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(patience=patience_count))\n","\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","\n","def compile_and_fit_model_basic_noVal(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor='accuracy',\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(patience=patience_count))\n","\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"njaKwGybk9P7"},"source":["def model_A_CNN_256(model_name, inshape, num_classes = 13):\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same' ,input_shape=inshape))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Flatten())\n","\n","    model.add(tf.keras.layers.Dense(128))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(64))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n","    model._name = model_name\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WZhP7P9LrOD"},"source":["# 'new' data \n","X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData(is500=False,\n","                                                                                                    include_secondary=False,\n","                                                                                                    isColab=True)\n","X_train_1000e_w2nd, Y_train_1000e_w2nd, X_test_1000e_w2nd, Y_test_1000e_w2nd, X_val_1000e_w2nd, Y_val_1000e_w2nd = getE2eData(is500=False, include_secondary=True, isColab=True)\n","X_train_1000e_j2nd, Y_train_1000e_j2nd, X_test_1000e_j2nd, Y_test_1000e_j2nd, X_val_1000e_j2nd, Y_val_1000e_j2nd = getE2eDataJustSecondary(isColab=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-uBWRfUCkBL"},"source":["# merge into a new train:\n","X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n","Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )    \n","\n","X_new_train_j2nd = np.concatenate( (X_train_1000e_j2nd, X_val_1000e_j2nd), axis=0 )\n","Y_new_train_j2nd = np.concatenate( (Y_train_1000e_j2nd, Y_val_1000e_j2nd), axis=0 )    \n","\n","X_new_train_w2nd = np.concatenate( (X_train_1000e_w2nd, X_val_1000e_w2nd), axis=0 )\n","Y_new_train_w2nd = np.concatenate( (Y_train_1000e_w2nd, Y_val_1000e_w2nd), axis=0 )  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oqF7iuACoVR","executionInfo":{"status":"ok","timestamp":1629731522264,"user_tz":-60,"elapsed":198,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"5d4d3f88-9a87-44cf-92be-fde0e9abe1db"},"source":["X_new_train[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 8)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ee9OuBB7O-r-"},"source":["    # merge into a new train:\n","    X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n","    Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )\n","    cnn_2, history_cnn_2 = compile_and_fit_model_basic_noVal(baseline_CNN_finalist,\n","                                                  \"cnn_noTest_20210516\",\n","                                                  X_new_train[0].shape,\n","                                                  X_new_train,\n","                                                  Y_new_train,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=10,\n","                                                  batch_size=2048,\n","                                                  epochs=150,\n","                                                  class_weight=None)\n","    cnn_2.evaluate(X_test_1000e, Y_test_1000e)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"GbaeRavGTBGO","executionInfo":{"status":"error","timestamp":1621235100649,"user_tz":-60,"elapsed":1225,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"00ba7d06-8862-48a7-a9c3-dfcf65a008ec"},"source":["plot_history(history_cnn_2)\n","cnn_2b, history_cnn_2 = compile_and_fit_model_basic_noVal(cnn_2,\n","                                                  cnn_2.name,\n","                                                  X_new_train[0].shape,\n","                                                  X_new_train,\n","                                                  Y_new_train,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=None,\n","                                                  batch_size=2048,\n","                                                  epochs=500,\n","                                                  class_weight=None)\n","cnn_2b.evaluate(X_test_1000e, Y_test_1000e)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-ddcefe7ae6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_cnn_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m cnn_2b, history_cnn_2 = compile_and_fit_model_basic_noVal(cnn_2,\n\u001b[1;32m      3\u001b[0m                                                   \u001b[0mcnn_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                   \u001b[0mX_new_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                   \u001b[0mX_new_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plot_history' is not defined"]}]},{"cell_type":"code","metadata":{"id":"lFSSn2R6QZab"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"Uyr5kKfUc6v8","executionInfo":{"status":"ok","timestamp":1621189982858,"user_tz":-60,"elapsed":735,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"525ba9f0-bfc1-4ac5-db44-cde1118562c7"},"source":["from google.colab import files\n","files.download(\"/content/cnn_128.zip\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_d156323a-7bab-43c8-9d22-fe45c3f82757\", \"cnn_128.zip\", 21125700)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"hIGS_7kcQae3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oveuq70EIxh2"},"source":[""],"execution_count":null,"outputs":[]}]}