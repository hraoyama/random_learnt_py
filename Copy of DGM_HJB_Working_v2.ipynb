{"cells":[{"cell_type":"markdown","metadata":{"id":"BErSeefeQwQi"},"source":["### Setup packages "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3008,"status":"ok","timestamp":1655478972513,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"L8DGCgVxR2AB","outputId":"0b7fd2c0-ecb2-4100-ff7c-30b8d05cc727"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9190,"status":"ok","timestamp":1655478981700,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"3xIx5C6UQn4u","outputId":"b533913b-2f80-478e-9be9-da06c5a3322f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: progressbar in /usr/local/lib/python3.7/dist-packages (2.5)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.2.0)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","time: 156 µs (started: 2022-06-17 15:16:21 +00:00)\n"]}],"source":["%pip install progressbar\n","%pip install plotnine\n","%pip install torch\n","%pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":751,"status":"ok","timestamp":1655478982446,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"EfIU_eNp3Zio","outputId":"faa44d17-d63d-4d23-8e45-77db222c70d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 906 ms (started: 2022-06-17 15:16:21 +00:00)\n"]}],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3242,"status":"ok","timestamp":1655478985685,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ZmUjYbArAuQT","outputId":"64331ccf-0620-4669-ef01-09505c00aae2"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.41 s (started: 2022-06-17 15:16:22 +00:00)\n"]}],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655478985685,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"BI4p7ZKb0Qz2","outputId":"4e63e607-7b17-422a-fcb2-652745f38e9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 870 µs (started: 2022-06-17 15:16:24 +00:00)\n"]}],"source":["paper_name = \"dgm_hjb\""]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655478985686,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"433z6V3T2rB2","outputId":"3b9c906f-9965-48a5-c4bc-f4dfc3a8630b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 11.5 ms (started: 2022-06-17 15:16:24 +00:00)\n"]}],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655478985686,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"uat0pG8aR3Rh","outputId":"1a533e10-19e9-447c-94b7-e21b840d6f6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 8.51 ms (started: 2022-06-17 15:16:24 +00:00)\n"]}],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655478985687,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"KpFjo3MkLus9","outputId":"c9e5f5da-b2e9-4617-931a-eaf6c6d9afb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 558 ms (started: 2022-06-17 15:16:24 +00:00)\n"]}],"source":["import torch \n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from scipy.stats import norm\n","from matplotlib import cm\n","import pdb\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1655478986177,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"CbfN42gpGZhC","outputId":"5eac841a-1b72-485b-8832-1b72fe3c8430"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 712 ms (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","from pprint import pprint as pp"]},{"cell_type":"markdown","metadata":{"id":"bvy0WvxDGCxk"},"source":["### Shared functions across models"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655478986177,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"cpVaz5dwXZNq","outputId":"a54aa085-a69f-46d5-ac8a-b270005eb8b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 22.9 ms (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["import pandas as pd\n","\n","def plot_report(train_instance):\n","        \n","    history_tl_cpu = [ x for x in train_instance.history_tl ]\n","    history_internal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_internal_cpu ]\n","    history_terminal_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_terminal ]\n","    history_initial_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_initial ]\n","    history_nonzero_cpu = [ x.cpu().detach().numpy() for x in train_instance.history_nonzero ]\n","\n","    obs_data = pd.DataFrame({\"Epochs\" : [ (x+1)*train_instance.hook_interval for x in range(len(history_initial_cpu))], \n","                             \"AvgLogLoss\": np.log(history_tl_cpu), \n","                             \"TerminalLogLoss\" :  np.log(history_terminal_cpu),\n","                             \"InternalLogLoss\" :  np.log(history_internal_cpu),\n","                             \"InitialLogLoss\" : np.log(history_initial_cpu),\n","                             \"NonZeroLogLoss\" : np.log(history_nonzero_cpu),\n","                             })\n","\n","    return (ggplot(obs_data, aes(\"Epochs\",\"AvgLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"TerminalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InternalLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"InitialLogLoss\")) + geom_line() + geom_point(),\n","            ggplot(obs_data, aes(\"Epochs\",\"NonZeroLogLoss\")) + geom_line() + geom_point(),\n","            )\n","\n","def plot_activation_mean(train_instance):\n","    \n","    # pdb.set_trace()\n","\n","    if train_instance.debug == False:\n","        print( 'error: debug is off , turn it on and train again ' )\n","    else:\n","        history = np.array(train_instance.history_mean_hooks)\n","        jet= plt.get_cmap('jet')\n","        colors = iter(jet(np.linspace(0,1,10)))\n","        fig, ax = plt.subplots()\n","        for i in range(history.shape[1]):\n","            ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n","        fig.suptitle('Layers activation mean value', fontsize=10)\n","        leg = ax.legend();\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655478986178,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"QMAuMqdgU9kL","outputId":"446e9bfb-565a-4de8-bf7f-11f19cb58103"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 525 µs (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["# plot_report(train)\n","# plot_activation_mean(train)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1655478986178,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"sCV-yFDXUV4J","outputId":"b755dc13-1848-4ae8-bbfa-f33869d1ea07"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 995 µs (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["# print( 'Value at 0' , net( torch.tensor( [ 0. , 1. , 1. , 1. ] ).cuda() ) )\n","# #%% save\n","# torch.save(net.state_dict(), './model3Assets')\n","# #%%\n","# net = TheModelClass(*args, **kwargs)\n","# net.load_state_dict(torch.load('./modelmodel3Assets'))\n","# net.eval()"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1655478987082,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ONB5NopRa3fD","outputId":"b9cf1dce-3bf8-49b1-a08b-ef39d10778cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 32.3 ms (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n","from scipy.optimize import LinearConstraint, NonlinearConstraint\n","from scipy.optimize import Bounds\n","from functools import partial\n","from scipy.optimize import minimize\n","from functools import wraps\n","\n","def negative(f):\n","    @wraps(f)\n","    def g(*args,**kwargs):\n","        return - f(*args,**kwargs)\n","    # g.__name__ = f'negative({f.__name__})'\n","    return g\n","# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n","\n","# we can add more minimization functions here later (e.g. SS diff)\n","def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n","  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n","  input = torch.tensor(varX*loss_terms)\n","  loss_pointwise = target * (torch.log(target) - torch.log(input))\n","  if reduction == \"mean\":  # default\n","      loss = loss_pointwise.mean()\n","  elif reduction == \"batchmean\":  # mathematically correct\n","      loss = loss_pointwise.sum() / input.size(0)\n","  elif reduction == \"sum\":\n","      loss = loss_pointwise.sum()\n","  else:  # reduction == \"none\"\n","      loss = loss_pointwise  \n","  return loss\n","\n","  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n","\n","def minimize_weights_st(loss_terms, loss_func):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  x0 = [0.25]*len(loss_terms)\n","  res = minimize( partial(loss_func, loss_terms=loss_terms), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n","\n","def maximizeloss_weights_st(loss_terms, loss_func, eps):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","  # even though zero is the KL minimum it helps to put a negative number here to explore\n","\n","  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n","  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint, nonlinear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655478987082,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"RM0IVdZ_TXW3","outputId":"78869489-d105-4d65-a758-28dc7ed6d94b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.33334757 0.33333761 0.33331482]\n","time: 31.9 ms (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n","print(r1.x)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655478987083,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ewko67bDIcz9","outputId":"fcd65d0a-4287-48ff-d65c-349da07dd26f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.7 ms (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["### There is an issue getting this to work because of nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","\n","    # def calculateLossAdaptWeights(self , size = 2**8 , train = True, min_max = True):\n","    #     '''\n","    #     Helper function that Sample and Calculate loss,\n","    #     This is adapted in that it changes the weights on the losses to maximize the loss provided\n","    #     the KL distance of the new weighting is within self.eps of the previous distribution (starting at equally weighted)\n","    #     '''        \n","    #     x , x_terminal , x_boundary = self.sample(size)\n","    #     x = Variable( x , requires_grad=True)\n","    #     Ls = self.criterion( x , x_terminal , x_boundary )\n","    #     DO , TC , BC = Ls\n","    #     DOm = torch.mean(DO).detach().cpu().float().item()\n","    #     TCm = torch.mean(TC).detach().cpu().float().item()\n","    #     BCm = torch.mean(BC).detach().cpu().float().item()\n","\n","    #     losses_for_reweighting = [ torch.mean(lv).detach().cpu().float().item() for lv in Ls if list(lv.size())] \n","    #     mask_for_available_losses = [ True if list(lv.size()) else False for lv in Ls ]\n","\n","    #     # print([ DOm, TCm, BCm])\n","    #     # if is.nan(DOm):\n","    #     #   print(DO)\n","\n","    #     if self.weights is None:\n","    #       self.weights = torch.ones(1,len(Ls))/len(Ls)\n","\n","    #     # pdb.set_trace()\n","\n","    #     if min_max:\n","    #         r1 = maximizeloss_weights_st( losses_for_reweighting , KLDiffHere, self.eps)\n","    #         candidate_weigths = torch.zeros_like(self.weights).to(torch.device(\"cuda:0\"))\n","    #         candidate_weigths[0][mask_for_available_losses] = torch.tensor(r1.x).to(torch.device(\"cuda:0\")).float()\n","    #         self.weights = candidate_weigths.to(torch.device(\"cuda:0\"))\n","    #         self.weights_tbl.append(self.weights.detach().cpu().numpy())\n","\n","    #     numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","    #     if train == True:\n","    #         return  (self.weights[0,0]*torch.mean(DO) + \n","    #                  self.weights[0,1]*torch.mean(TC) + \n","    #                  self.weights[0,2]*torch.mean(BC)) , \\\n","    #                  self.weights[0,0]*torch.mean(DO) , \\\n","    #                  self.weights[0,1]*torch.mean(TC) , \\\n","    #                  self.weights[0,2]*torch.mean(BC) , \\\n","    #                  (1./numActive*torch.mean(DO) + \n","    #                  1./numActive*torch.mean(TC) + \n","    #                  1./numActive*torch.mean(BC))             \n","    #     else:\n","    #         return  DO , TC , BC\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1655478987083,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"iyacROFeXgNp","outputId":"49a35a19-170c-448b-c917-9df7def858df"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 8.38 ms (started: 2022-06-17 15:16:25 +00:00)\n"]}],"source":["import torch\n","from torch.distributions import Normal\n","\n","std_norm_cdf = Normal(0, 1).cdf\n","std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n","\n","def bs_price(right, K, S, T, sigma, r):\n","    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n","    d_2 = d_1 - sigma * torch.sqrt(T)\n","    \n","    if right == \"C\":\n","        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n","        return C\n","        \n","    elif right == \"P\":\n","        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n","        return P"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1655478987084,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"sLsA5AvqpMM7","outputId":"8959fb77-e57f-47d6-fd8e-7f1efa586049"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.52 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["import torch\n","\n","def to_cpu_detach(x):\n","  if isinstance(x, list):\n","    return [ y.detach().cpu().item() for y in x ]\n","  else:\n","    return x.detach().cpu().item()"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1655478987084,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"PC-E2SeX46A9","outputId":"977e7b95-769f-4926-d948-f39293cffd5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.25 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["def huber_loss_zero_target(x, delta = 1.0):\n","  loss_function = torch.nn.HuberLoss(delta=delta)\n","  return loss_function(x, torch.zeros_like(x))\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1655478987084,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"MNYJyHWpeL66","outputId":"4092f608-e248-47e0-f5f2-85079ebd898e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 20.3 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["# def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","#   model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","#   if eqObject is not None:\n","#     try:\n","#         beta = getattr(eqObject,\"beta\")\n","#         beta_str = str(beta).replace('.','p')\n","#         model_id_str = model_id_str + f\"_beta{beta_str}\"\n","#     except AttributeError:\n","#         pass\n","#     try:\n","#         wgamma = getattr(eqObject,\"wgamma\")\n","#         wgamma_str = str(wgamma).replace('.','p')\n","#         model_id_str = model_id_str + f\"_wgamma{wgamma_str}\"\n","#     except AttributeError:\n","#         pass\n","  \n","#   torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","#   df_at_hookintervals = None\n","#   train_losses = None\n","#   validation_losses = None\n","#   try:\n","#       df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","#       if df_at_hookintervals is not None:\n","#         df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","#   except AttributeError:\n","#       print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","#   try:\n","#       train_losses = getattr(trainObj,\"train_losses\")\n","#       if train_losses is not None:\n","#         train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","#   except AttributeError:\n","#       print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","#       # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","#   try:\n","#       validation_losses = getattr(trainObj,\"validation_losses\")\n","#       if validation_losses is not None:\n","#         validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","#   except AttributeError:\n","#       print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655478987084,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ipogSsVTbv0k","outputId":"5744ca10-21ce-4b80-b08f-40464b535435"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 29.2 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        wgamma = getattr(eqObject,\"wgamma\")\n","        wgamma_str = str(wgamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{wgamma_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        xbreaks = getattr(eqObject,\"xbreaks\")\n","        xbreaks_str = str(len(xbreaks))\n","        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        tbreaks = getattr(eqObject,\"tbreaks\")\n","        tbreaks_str = str(len(tbreaks))\n","        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"markdown","metadata":{"id":"Tz5tUJuYaXKu"},"source":["### Merton Invest-Consumption Problem - Equation HJB optimization\n","\n","[Extensions of the Deep Galerkin Method](https://arxiv.org/pdf/1912.01455v3.pdf)"]},{"cell_type":"markdown","metadata":{"id":"N-GO35FcJPP6"},"source":["##### Closed form terminal utility functions"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655478987084,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"0cWoRXs02PoF","outputId":"12526b12-8132-49f2-84ce-783bdd623cc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.4 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["def expTerminalUtilityOfWealth(x, gamma_discount = 1):\n","  return(-1*torch.exp(-gamma_discount*x))\n","\n","def expTerminalUtilityOfWealth_np(x, gamma_discount = 1):\n","  return(-np.exp(-gamma_discount*x))\n","\n","from functools import partial\n","\n","# should give a closed form solution for the control => PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))"]},{"cell_type":"markdown","metadata":{"id":"HrivvbmubiiY"},"source":["#### MertonUtilityNet"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655478987085,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"JRraqOG4aXKx","outputId":"56fec441-3190-4115-f25e-4e8306121fa9"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 10.1 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["class MertonUtilityNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh):\n","        super(MertonUtilityNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5 + 1  # wealth, time, mu, r, sigma, pi\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act(self.fc_input(x))\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out "]},{"cell_type":"markdown","metadata":{"id":"fyFbPZr7I5RE"},"source":["#### MertonPiNet"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655478987085,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"PR7PHL4KI1S9","outputId":"250d728e-de55-45aa-c9e3-6a8bf61395b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 14.6 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["import torch.nn.functional as F\n","\n","class MertonPiNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.relu  ):\n","        super(MertonPiNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5   # wealth, time, mu, r, sigma\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)            \n","        # self.fc_output_d = nn.Linear(self.NN, 2)\n","        # self.fc_output = torch.nn.Softmax(dim=1)\n","        self.fc_output = nn.Linear(self.NN, 1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        # out = self.fc_output_d(h)\n","        out = self.fc_output(h)\n","        return out \n","        "]},{"cell_type":"markdown","metadata":{"id":"RNhAbZ727RC_"},"source":["#### MertonAlternativePiNet\n","\n","[implement from github](https://github.com/Plemeur/DGM/blob/master/first_net.py)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655478987085,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"_c-dZ5b37NwV","outputId":"9e6bd660-3775-4494-8274-a3161844d397"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 97.5 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["class LinearWithXavier(nn.Module):\n","    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n","        TODO : figure out what to do with the bias\"\"\"\n","    def __init__(self, in_features, out_features, bias=True, batch_normalize=True):\n","        super(LinearWithXavier, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.batch_normalize = batch_normalize\n","        \n","        if self.batch_normalize == True:\n","          self.batch_norm = torch.nn.BatchNorm1d(out_features)\n","        \n","        if bias:\n","            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","    \n","    def reset_parameters(self):\n","        torch.nn.init.xavier_uniform_(self.weight)\n","        if self.bias is not None:\n","            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n","    \n","    def forward(self, input):\n","        if self.batch_normalize == True:\n","          return self.batch_norm(torch.nn.functional.linear(input, self.weight, self.bias))\n","        return torch.nn.functional.linear(input, self.weight, self.bias)\n","    \n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n","\n","\n","class DGM_layer(nn.Module):\n","    \"\"\" See readme for paper source\"\"\"\n","    def __init__(self, in_features, out_feature, residual=False, batch_normalize=False):\n","        super(DGM_layer, self).__init__()\n","        self.residual = residual\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.G = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.R = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.H = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","\n","    def forward(self, x, s):\n","        z = torch.tanh(self.UZ(x) + self.Z(s))\n","        g = torch.tanh(self.UG(x) + self.G(s))\n","        r = torch.tanh(self.UR(x) + self.R(s))\n","        h = torch.tanh(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonAlternativePiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonAlternativePiNet, self).__init__()\n","        self.neurons=neurons\n","        self.depth=depth\n","\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n","        # self.middle_layer_2 = nn.ModuleList([DGM_layer(in_size, neurons, batch_normalize=False) for i in range(2)])\n","        self.final_layer = LinearWithXavier(neurons, out_size, batch_normalize=False)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","        \n","        # for i, layer in enumerate(self.middle_layer_2):\n","        #     s = torch.tanh(layer(X, s))\n","        \n","        # s = torch.nn.functional.gelu(self.input_layer(X))\n","        # for i, layer in enumerate(self.middle_layer):\n","        #     s = torch.nn.functional.elu(layer(X, s))\n","        # for i, layer in enumerate(self.middle_layer):\n","        #     s = torch.nn.functional.gelu(layer(X, s))\n","        # for i, layer in enumerate(self.middle_layer_2):\n","        #     s = torch.nn.functional.gelu(layer(X, s))\n","\n","        return self.final_layer(s)\n","        # return torch.pow(self.final_layer(s), 1)\n"]},{"cell_type":"markdown","metadata":{"id":"CNsqOm1ithSG"},"source":["#### MertonMatchPiNet\n","\n","[Matching Paper by hand](https://arxiv.org/abs/1912.01455v3)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655478987086,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"REWml4aYthSH","outputId":"602f3f44-7911-4508-dc8c-93521b735f5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 25.7 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["class DGMLayerPaper(nn.Module):\n","\n","    def __init__(self, in_features, out_feature, activation=torch.relu, residual=False):\n","        \n","        super(DGMLayerPaper, self).__init__()\n","        self.residual = residual\n","        self.activation = activation\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature) # w.S\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=True) # u.x\n","        self.G = LinearWithXavier(out_feature, out_feature)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.R = LinearWithXavier(out_feature, out_feature)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=True)\n","        self.H = LinearWithXavier(out_feature, out_feature) # w.(S(o)R)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=True)\n","\n","    def forward(self, x, s):\n","        z = self.activation(self.UZ(x) + self.Z(s))\n","        g = self.activation(self.UG(x) + self.G(s))\n","        r = self.activation(self.UR(x) + self.R(s))\n","        h = self.activation(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class MertonMatchPiNet(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth):\n","        super(MertonMatchPiNet, self).__init__()\n","        self.dim = in_size\n","        self.input_layer = LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGMLayerPaper(in_size, neurons) for i in range(depth)])\n","        self.final_layer = LinearWithXavier(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = torch.tanh(self.input_layer(X))\n","        for i, layer in enumerate(self.middle_layer):\n","            s = torch.tanh(layer(X, s))\n","\n","        return self.final_layer(s)\n"]},{"cell_type":"markdown","metadata":{"id":"wClW1g9rbm8o"},"source":["#### PiEquation"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655478987086,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"uQTTkUhWuiJi","outputId":"fa5d98b8-7425-4e38-d2ba-48dc0ba44b02"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 32.1 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["class PiEquation():\n","\n","    def __init__(self , pi_net, du_dx, d2u_dx2):\n","        self.pi_net = pi_net\n","        self.wgamma = 0.0001\n","        self.du_dx = Variable(du_dx, requires_grad=False)\n","        self.d2u_dx2 = Variable(d2u_dx2, requires_grad=False)\n","\n","    def criterion(self, x_internal, grads):\n","      #  time, wealth, mu, r, sigma\n","      du_dx = Variable(grads[1], requires_grad=False)   # the derivatives with respect to u are held constant when finding pi \n","      d2u_dx2 = Variable(grads[2], requires_grad=False) # du_dt is grads[0], not relevant here\n","\n","      pi_net_preds = self.pi_net(x_internal)\n","      pi_net_preds = pi_net_preds.reshape(-1,1)\n","      \n","      x_internal_const = Variable(x_internal.clone(), requires_grad=False)\n","      \n","      intC = None\n","      # pdb.set_trace()\n","      if len(x_internal) == 0:\n","        intC_loss = torch.tensor(0).cuda().float()  \n","      else:\n","        # pdb.set_trace()\n","        \n","        # pi * (mu-r)\n","        loss_1 = pi_net_preds*(x_internal_const[:,2].reshape(-1,1) - x_internal_const[:,3].reshape(-1,1))\n","        \n","        # r * wealth\n","        loss_2 = x_internal_const[:,3].reshape(-1,1)*x_internal_const[:,1].reshape(-1,1)\n","        \n","        # sigma^2 * pi^2\n","        loss_3 = (x_internal_const[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n","        \n","        intC_loss = -(loss_1 +loss_2)*du_dx - 0.5*loss_3*d2u_dx2\n","\n","        # # r*x\n","        # loss_1 = x_internal_const[:,3].reshape(-1,1) * x_internal_const[:,1].reshape(-1,1)\n","\n","        # # (mu - r)^2/sigma^2\n","        # loss_2 = ((x_internal_const[:,2].reshape(-1,1) - x_internal_const[:,3].reshape(-1,1))**2) / (x_internal_const[:,4].reshape(-1,1)**2)\n","\n","      return  1.0*intC_loss\n","\n","    def calculatePiLoss(self, x_internal, grads, keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        x_internal = Variable( x_internal , requires_grad=True)\n","        Ls = self.criterion( x_internal, grads)\n","        \n","        return_losses = []\n","        \n","        if not keep_batch:\n","          loss_pi = torch.mean(Ls)           \n","          return loss_pi          \n","        else:\n","          return Ls\n"]},{"cell_type":"markdown","metadata":{"id":"8RRoBgFQINMv"},"source":["#### TrainInternalPiWithDGM\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":514,"status":"ok","timestamp":1655478987591,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"VREn3fanpP1b","outputId":"f8caaa56-a647-4b77-ca13-be0eaf4d3ef4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 173 ms (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["class TrainInternalPiWithDGM():\n","    \n","    def __init__(self , u_equation, pi_equation, pi_equation_traget, BATCH_SIZE , epoch, lr, debug = False, loss_multiply = 1.0):\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.u_model = u_equation        \n","        \n","        self.pi_model = pi_equation\n","        self.pi_model_traget = pi_equation_traget\n","        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.pi_model.pi_net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","\n","        self.epoch = epoch\n","        self.lr = lr\n","\n","        self.loss_multiply = loss_multiply\n","\n","    def get_utility_function_derivatives(self, u_net_val, x_internal, normalize=False):\n","        du = torch.autograd.grad( u_net_val, \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(u_net_val),\n","                                  create_graph=True,\n","                                  retain_graph=True)\n","        \n","        du_dt = du[0][:,0].reshape(-1,1)\n","        du_dx = du[0][:,1].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(du_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(du_dx),\n","                                      retain_graph=True\n","                                      )[0][:,1].reshape(-1,1)\n","\n","        d2u_dx2 = torch.sign(d2u_dx2) * torch.maximum(torch.abs(d2u_dx2), 1e-8*torch.ones_like(d2u_dx2))                                      \n","\n","        return du_dt, du_dx, d2u_dx2 # + 1e-7\n","        \n","    def train(self , eqLossFn = 'calculatePiLoss', sample_method_X = \"U\"):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((self.epoch, 3 ), dtype='float32') * np.nan\n","        \n","        self.train_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n","        \n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        \n","        try:\n","            loss_calc_method = getattr(self.pi_model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(self.epoch):\n","            sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            loss_avg = 0.0\n","            # pdb.set_trace()\n","            x_internal = Variable(sample_batch[0], requires_grad=True)\n","            x_terminal =  Variable(sample_batch[1], requires_grad=True)\n","            \n","            utility_net_val = self.u_model.u_net(x_internal)\n","            du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(utility_net_val, x_internal)\n","\n","            utility_net_val_terminal = self.u_model.u_net(x_terminal)\n","            du_dt_terminal, du_dx_terminal, d2u_dx2_terminal = self.get_utility_function_derivatives(utility_net_val_terminal, x_terminal)\n","\n","            du_dt = (du_dt + du_dt_terminal)/2.0\n","            du_dx = (du_dx + du_dx_terminal)/2.0\n","            d2u_dx2 = (d2u_dx2 + d2u_dx2_terminal)/2.0\n","\n","            loss  = loss_calc_method(x_internal, [du_dt, du_dx, d2u_dx2], keep_batch = False )            \n","            # print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) ]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=0.5) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              self.validation_losses[e,:] = validation_loss_list\n","            \n","            if self.use_early_stop:\n","              loss_to_check = loss\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            optimizer.zero_grad()\n","            loss_avg = loss_avg + float(loss.item())\n","\n","            # print('self.epoch:', e)\n","            loss.backward()\n","            # for param in self.pi_model.pi_net.parameters():\n","            #   print('pi grad', param.grad)\n","            optimizer.step()\n","            \n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                # print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(e , self.lr , loss))\n","\n","        self.stop_epoch = e\n","        self.pi_model_traget.pi_net.load_state_dict(self.pi_model.pi_net.state_dict())\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n"]},{"cell_type":"markdown","metadata":{"id":"hp4BG1ewKF6o"},"source":["#### MertonEquation"]},{"cell_type":"code","execution_count":31,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1852,"status":"ok","timestamp":1655478989440,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"LBMZYQSPaXKy","outputId":"c088ad68-dcdf-4b97-e96a-f4dcac59b443"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.32 s (started: 2022-06-17 15:16:26 +00:00)\n"]}],"source":["import math\n","\n","class MertonEquation():\n","    \n","    def __init__(self , u_net, pi_net, pi_net_target, pi_net_epoch, pi_net_lr, term_utility_function = partial(expTerminalUtilityOfWealth, gamma_discount=1)):\n","\n","        self.u_net = u_net\n","        self.pi_net = pi_net\n","        self.pi_net_target = pi_net_target\n","\n","        self.wgamma = 0.0001\n","        self.term_utility_func = term_utility_function\n","        self.xbreaks = None\n","        self.tbreaks = None\n","\n","        self.MAX_X = 1.0\n","        self.T = 1.0\n","        self.MAX_MU = 0.2\n","        self.MAX_SIGMA = 1.0\n","\n","        self.pi_net_epoch = pi_net_epoch\n","        self.pi_net_lr = pi_net_lr\n","        self.loss_multiply = 1.0\n","\n","        self.FORCE_MU = None\n","        self.FORCE_R = None\n","        self.FORCE_SIGMA = None\n","\n","        self.epoch_of_u = None\n","        self.adapt_pi_epochs = False\n","        self.start_adapt_epochs = [ [500, 1000, 2000, 5000, 10000, 15000 ], [4, 8, 10 ,20, 40, 60] ]\n","        self.was_loss_beaten = False\n","\n","        self.pi_net.train()\n","\n","    def g(self,x):\n","        # Time, Wealth, Mu, R, Sigma\n","        return self.term_utility_func(x[:,1].reshape(-1,1))\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def mu_r_sample(self, size, range_multiplier = 1.0):\n","      mu_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      r_candidate = -self.MAX_MU*range_multiplier*torch.rand([size, 1])+self.MAX_MU*range_multiplier\n","      return (mu_candidate, r_candidate)\n","\n","      # r_sample = torch.where(r_candidate < mu_candidate, r_candidate, mu_candidate)\n","      # mu_sample = torch.where(r_candidate > mu_candidate, r_candidate, mu_candidate)\n","      # return (mu_sample, r_sample)\n","\n","    def apply_forced_mu_r_sigma(self, mu_sample, r_sample, sigma_sample):\n","      if self.FORCE_MU is not None:\n","         mu_sample = self.FORCE_MU*torch.ones_like(mu_sample)            \n","      if self.FORCE_R is not None:\n","        r_sample = self.FORCE_R*torch.ones_like(r_sample)\n","      if self.FORCE_SIGMA is not None:\n","        sigma_sample = self.FORCE_SIGMA*torch.ones_like(sigma_sample)\n","      return mu_sample, r_sample, sigma_sample\n","\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","        '''\n","        Sampling function\n","        '''\n","        if sample_method_X in [\"U\"]:\n","            range_multiplier = 1.0\n","            \n","            ### internal samples of Time, Wealth, Mu, R, Sigma\n","            mu_sample_internal, r_sample_internal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_internal, r_sample_internal, sigma_sample_internal = self.apply_forced_mu_r_sigma(mu_sample_internal, r_sample_internal, sigma_sample_internal)\n","            x_internal = self.to_device(torch.cat(( torch.rand([size,1])*self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_internal, # mu\n","                                                    r_sample_internal, # R\n","                                                    sigma_sample_internal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            ### Terminal time samples\n","            mu_sample_terminal, r_sample_terminal = self.mu_r_sample(size, range_multiplier)\n","            sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([size, 1])+self.MAX_SIGMA*range_multiplier\n","            mu_sample_terminal, r_sample_terminal, sigma_sample_terminal = self.apply_forced_mu_r_sigma(mu_sample_terminal, r_sample_terminal, sigma_sample_terminal)\n","            x_terminal = self.to_device(torch.cat(( torch.zeros(size, 1) + self.T , # Time\n","                                                   -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier, # Wealth\n","                                                    mu_sample_terminal, # mu\n","                                                    r_sample_terminal, # R\n","                                                    sigma_sample_terminal # Sigma\n","                                                   ) , dim = 1 ),to_cpu)\n","            \n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            return x_internal , x_terminal\n","\n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","        \n","    def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","\n","      if self.xbreaks is None and self.tbreaks is None:\n","        return self.sample(sample_method_X, size, to_cpu)\n","\n","      internal_strata_xts = []\n","      terminal_strata_xts = []\n","      \n","      if sample_method_X in [\"U\"]:\n","          range_multiplier = 1.0\n","          xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n","          tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n","          if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n","            xbreaks_used.append(range_multiplier*self.MAX_X)\n","          while xbreaks_used[0] < 0.0:\n","            xbreaks_used.pop(0)\n","          if not xbreaks_used:\n","            xbreaks_used = [0,range_multiplier*self.MAX_X]\n","          if xbreaks_used[0] > 0.0:            \n","            xbreaks_used.insert(0, 0.0)\n","\n","          if tbreaks_used[-1] < self.T:\n","            tbreaks_used.append(self.T)\n","          xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n","          tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n","\n","          total_strat_processed = 0\n","\n","          # internal samples\n","          for stratum_x_count in range(len(xbreaks_used)-1):\n","              \n","            num_samples_in_stratum = 0\n","            if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n","              range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n","              num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n","\n","            for stratum_t_count in range(len(self.tbreaks)-1):\n","\n","              if num_samples_in_stratum == 0: # there is only a T division, so use it\n","                range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n","                num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n","              else:\n","                # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n","                stratum_coverage_on_unit_square = \\\n","                  ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n","                  ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n","                num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n","\n","              range_multiplier = 1.0\n","\n","              ### internal samples of Time, Wealth, Mu, R, Sigma\n","              internal_stratum_t_sample = tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])\n","              internal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_internal, stratum_r_sample_internal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_internal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_internal, stratum_r_sample_internal, stratum_sigma_sample_internal)\n","              x_internal_stratum = self.to_device(torch.cat(( internal_stratum_t_sample , # Time\n","                                                              internal_stratum_x_sample, # Wealth\n","                                                              stratum_mu_sample_internal, # mu\n","                                                              stratum_r_sample_internal, # R\n","                                                               # Sigma\n","                                                            ) , dim = 1 ),to_cpu)\n","              if not internal_strata_xts: \n","                internal_strata_xts = [ x_internal_stratum ] \n","              else:\n","                internal_strata_xts.append(x_internal_stratum) \n","\n","              ### Terminal time samples\n","              terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal = self.mu_r_sample(num_samples_in_stratum, range_multiplier)\n","              stratum_sigma_sample_terminal = -self.MAX_SIGMA*range_multiplier*torch.rand([num_samples_in_stratum, 1])+self.MAX_SIGMA*range_multiplier\n","              stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal = \\\n","                self.apply_forced_mu_r_sigma(stratum_mu_sample_terminal, stratum_r_sample_terminal, stratum_sigma_sample_terminal)\n","              x_terminal_stratum = self.to_device(torch.cat(( torch.zeros(num_samples_in_stratum, 1) + self.T , # Time\n","                                                      terminal_stratum_x_sample, # Wealth\n","                                                      stratum_mu_sample_terminal, # mu\n","                                                      stratum_r_sample_terminal, # R\n","                                                      stratum_sigma_sample_terminal # Sigma\n","                                                    ) , dim = 1 ),to_cpu)\n","              if not terminal_strata_xts:\n","                terminal_strata_xts = [ x_terminal_stratum ] # terminal_stratum_xt[None,:,:]\n","              else:\n","                terminal_strata_xts.append(x_terminal_stratum) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n","\n","              total_strat_processed += 1 \n","              # print((len(internal_strata_xts),xbreaks_used[stratum_x_count],tbreaks_used[stratum_t_count]))\n","\n","          # pdb.set_trace()\n","          # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","          return internal_strata_xts , terminal_strata_xts\n","    \n","      raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","\n","    def get_num_pi_epochs(self):\n","      if self.adapt_pi_epochs and self.was_loss_beaten:\n","        if any([ i < self.epoch_of_u for i in self.start_adapt_epochs[0] ]):\n","          return self.pi_net_epoch * self.start_adapt_epochs[1][[i for i in range(len(self.start_adapt_epochs[0])) if self.start_adapt_epochs[0][i] < self.epoch_of_u][-1]] \n","      return self.pi_net_epoch\n","\n","    def get_utility_function_derivatives(self, u_net_val, x_internal, normalize=False):\n","        du = torch.autograd.grad( u_net_val, \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(u_net_val),\n","                                  create_graph=True,\n","                                  retain_graph=True)\n","        \n","        du_dt = du[0][:,0].reshape(-1,1)\n","        du_dx = du[0][:,1].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(du_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(du_dx),\n","                                      retain_graph=True\n","                                      )[0][:,1].reshape(-1,1)\n","        \n","        d2u_dx2 = torch.sign(d2u_dx2) * torch.maximum(torch.abs(d2u_dx2), 1e-8*torch.ones_like(d2u_dx2))                                      \n","\n","        return du_dt, du_dx, d2u_dx2 \n","\n","    def criterion(self, x_internal, x_terminal, loss_transforms = [torch.square], util_network=None):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # Time / Wealth / Mu / r / Sigma (sample data order)\n","        # pdb.set_trace()\n","        x_internal_before = x_internal.clone()\n","        x_terminal_before = x_terminal.clone()\n","\n","        # pi_used = self.pi_net_target(x_internal).detach().reshape(-1,1)\n","        # G(time, wealth, mu, r, sigma, pi)\n","\n","        x_internal =  Variable(x_internal, requires_grad=True)\n","        x_terminal =  Variable(x_terminal, requires_grad=True)\n","        \n","        u_net_val = util_network(x_internal)\n","        u_net_val_terminal = util_network(x_terminal)\n","       \n","        du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n","        du_dt_terminal, du_dx_terminal, d2u_dx2_terminal = self.get_utility_function_derivatives(u_net_val_terminal, x_terminal)\n","        \n","        du_dt = (du_dt + du_dt_terminal)/2.0\n","        du_dx = (du_dx + du_dx_terminal)/2.0\n","        d2u_dx2 = (d2u_dx2 + d2u_dx2_terminal)/2.0\n","\n","        pi_model = PiEquation(self.pi_net, du_dx, d2u_dx2)\n","        pi_model_target = PiEquation(self.pi_net_target, None, None)                \n","        \n","        pi_trainer = TrainInternalPiWithDGM(self, pi_model, pi_model_target, \n","                                            x_internal.shape[0], \n","                                            self.get_num_pi_epochs(), \n","                                            self.pi_net_lr, \n","                                            debug=True, \n","                                            loss_multiply=1.0)\n","        pi_trainer.use_early_stop = True\n","        pi_trainer.early_stop_patience = min(20,math.ceil(self.pi_net_epoch/10.0))\n","        pi_trainer.train()\n","        \n","\n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]\n","\n","        intC = None\n","        terC = None\n","\n","        if len(x_internal) == 0:\n","          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          # Time, Wealth, Mu, R, Sigma\n","          # pdb.set_trace()\n","          pi_net_preds = self.pi_net_target(x_internal_before)\n","          pi_net_preds = pi_net_preds.detach().reshape(-1,1)\n","          \n","          x_internal =  Variable(x_internal_before.clone(), requires_grad=True)\n","          \n","          u_net_val = util_network(x_internal)\n","          du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n","\n","          x_internal_const = x_internal.detach().clone()\n","          # x_internal_const = x_internal.clone()\n","          \n","          # pi*(mu-r)\n","          intC_loss_1 = pi_net_preds*(x_internal_const[:,2].reshape(-1,1)-x_internal_const[:,3].reshape(-1,1)) \n","          \n","          # r * wealth\n","          intC_loss_2 =  x_internal_const[:,3].reshape(-1,1)*x_internal_const[:,1].reshape(-1,1)  \n","          \n","          # sigma^2 * pi^2\n","          intC_loss_3 = (x_internal_const[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n","\n","          intC_loss = du_dt + (intC_loss_1 + intC_loss_2)*du_dx + 0.5*intC_loss_3*d2u_dx2\n","\n","          # eps = 1e-3\n","          # intC_loss = torch.abs((intC_loss-eps)/eps)\n","          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n","        \n","        \n","        # print('mu:', x_internal[:,2].reshape(-1,1))\n","        # print('r:', x_internal[:,3].reshape(-1,1))\n","        # print('wealth:', x_internal[:,1].reshape(-1,1))\n","        # print('sigma^2', x_internal[:,4].reshape(-1,1)**2)\n","        # print('pi:', pi_net_preds)\n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        x_terminal_before = x_terminal.clone()\n","\n","        x_terminal =  Variable(x_terminal_before, requires_grad=True)\n","        # print('non terminal pred', u_net_val)\n","        # print('terminal utility pred', util_network(x_terminal))\n","        # print('utility target', self.g(x_terminal))\n","        # print('----------------------')\n","        # print('g terminal:', self.g(x_terminal))\n","        # print('unet:', util_network(x_terminal))\n","        # print('diff:', (self.g(x_terminal)-util_network(x_terminal)) )\n","        # print('perecentage error:', (self.g(x_terminal)-util_network(x_terminal))/(self.g(x_terminal) + 1e-7))\n","        # print('------------------------------')\n","        \n","        terminal_target = self.g(x_terminal)\n","        util_net_pred = util_network(x_terminal)\n","\n","        err1 = torch.abs((util_net_pred-terminal_target)/(util_net_pred+1e-7) )\n","        err2 = torch.abs((util_net_pred-terminal_target)/(terminal_target+1e-7) )\n","        terC = torch.where(torch.abs(terminal_target) < torch.abs(util_net_pred), err2, err1)\n","\n","        terC = terminal_target - util_net_pred\n","\n","        terC = [ loss_transform(terC) for loss_transform in loss_transforms ]\n","        \n","        # print('mean d2u_dx2:', d2u_dx2.mean(),'max d2u_dx2:', d2u_dx2.max())\n","        # print('mean du_dx:', du_dx.mean(),'max du_dx:', d2u_dx2.max())\n","        # print('mean du_dt:', du_dt.mean(),'max d2u_dx2:', d2u_dx2.max())\n","        \n","        \n","        return  intC , terC\n","\n","    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False, util_network=None):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        # pdb.set_trace()\n","        x_internal , x_terminal = batch_x\n","        x_internal = Variable( x_internal , requires_grad=True)\n","        x_terminal = Variable( x_terminal , requires_grad=True)\n","        # print('MertonEquation calling self.criterion')\n","        Ls = self.criterion(x_internal , x_terminal, loss_transforms = loss_transforms, util_network=util_network)\n","        intC , terC  = Ls\n","\n","        return_losses = []\n","        # print('internal Loss', torch.mean(intC[0]))\n","        # print('external Loss', torch.mean(terC[0]))\n","\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            loss_equalWeightedByType = torch.mean(intC[lc]) + torch.mean(terC[lc])\n","            return_losses.append( [ loss_equalWeightedByType , \n","                                    torch.mean(intC[lc]) , torch.mean(terC[lc]), \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC.numpy(), terC.numpy()] )\n","        return return_losses\n","\n","    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , loss_transforms = loss_transforms)\n","        intC , terC = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n","            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n","            loss_equalWeightedByType = 100*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt)\n","            return_losses.append( [ transformed_loss , \n","                                    100*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","\n","    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal  = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal, loss_transforms = loss_transforms)\n","        intC , terC  = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n","            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n","            loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt )\n","            return_losses.append( [ transformed_loss , \n","                                    0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","        return return_losses\n","\n","    "]},{"cell_type":"markdown","source":["#### Save info from run"],"metadata":{"id":"vkqBrz2JplP9"}},{"cell_type":"code","source":["def save_model_train(training, eqObject, lr, eqLossFn, sample_method, loss, batch_size, epoch): \n","\n","  model_id_base_str =  f\"{eqObject.__class__.__name__}_{datetime.datetime.now():%Y%m%d%H%M%S}_{batch_size}_{str(torch.round(loss,decimals=2)).replace('.','p')}\"\n","  model_id_base_str = model_id_base_str + f\"_{eqLossFn}_{sample_method}_{epoch}_{str(lr).replace('.','p')}\"  \n","  model_id_base_str = model_id_base_str + f\"_U{eqObject.u_net.neurons}_U{eqObject.u_net.depth}_P{eqObject.pi_net.neurons}_P{eqObject.pi_net.depth}\"\n","  \n","  torch.save(eqObject.u_net.state_dict(),  f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/{model_id_base_str}_U\")\n","  torch.save(eqObject.pi_net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/{model_id_base_str}_P\")\n","\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(training, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{training.hook_interval}_{model_id_base_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(training,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_base_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(training,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_base_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"validation_losses\"))\n","\n","  import numpy as np\n","\n","  training_dict = dict({ \"BATCH_SIZE\" : training.BATCH_SIZE,\n","                         \"use_early_stop\" : training.use_early_stop,\n","                        \"early_stop_patience\" : training.early_stop_patience,\n","                        \"early_stop_delta\" : training.early_stop_delta,\n","                        \"monitored_loss_type\" : training.monitored_loss_type,\n","                        \"use_early_stop\" : training.use_early_stop,\n","                        \"stop_epoch\" : training.stop_epoch            })\n","  equation_dict = dict({ \"wgamma\" : eqObject.wgamma,\n","                        \"xbreaks\" : eqObject.xbreaks,\n","                        \"tbreaks\" : eqObject.tbreaks,\n","                        \"MAX_X\" : eqObject.MAX_X,\n","                        \"T\" : eqObject.T,\n","                        \"MAX_MU\" : eqObject.MAX_MU,\n","                        \"MAX_SIGMA\" : eqObject.MAX_SIGMA,\n","                        \"pi_net_epoch\" : eqObject.pi_net_epoch,\n","                        \"wgamma\" : eqObject.wgamma,\n","                        \"pi_net_lr\" : eqObject.pi_net_lr,\n","                        \"loss_multiply\" : eqObject.loss_multiply,\n","                        \"epoch_of_u\" : eqObject.epoch_of_u,\n","                        \"adapt_pi_epochs\" : eqObject.adapt_pi_epochs,\n","                        \"start_adapt_epochs\" : eqObject.start_adapt_epochs,\n","                        \"was_loss_beaten\" : eqObject.was_loss_beaten  })\n","\n","  np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainingDict_{model_id_base_str}.npy\", training_dict)\n","  np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/equationDict_{model_id_base_str}.npy\", equation_dict) \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bDHKw-yFpqMl","executionInfo":{"status":"ok","timestamp":1655479083724,"user_tz":-60,"elapsed":459,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"609a05db-873d-4a76-aaad-d1f3fce23cd1"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 30.6 ms (started: 2022-06-17 15:18:03 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"65nooklCbsdy"},"source":["#### TrainHJBMertonWithDGM"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655478989442,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"74jqGI3o2GlN","outputId":"d66d3242-caaf-4005-f7b0-ef2d2b2b2c17"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.62 ms (started: 2022-06-17 15:16:27 +00:00)\n"]}],"source":["def attach_pi_used(x, pi_net, requires_grad=True):\n","  pi_used = pi_net(x)  \n","  pi_used.detach_()\n","  # pi_used = pi_used[:,0].reshape(-1,1)\n","  pi_used = pi_used.reshape(-1,1)\n","  \n","  before_x = x.detach().clone()\n","  new_x =  Variable(torch.cat((x, pi_used), dim=1),requires_grad=requires_grad)\n","  return before_x, new_x"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1709,"status":"ok","timestamp":1655479194622,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"OtO8fV7oaXK2","outputId":"7fe7fbaa-2621-4240-a634-4183eb59299d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.15 s (started: 2022-06-17 15:19:52 +00:00)\n"]}],"source":["class TrainHJBMertonWithDGM():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","        self.stop_epoch = 0\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","        \n","\n","        self.net.train()\n","    \n","    def Htx(self, x, gamma=1):\n","      # 3.6a in https://arxiv.org/abs/1912.01455v3\n","      #  wealth * gamma * exp(r*(1-t))\n","      part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1)*(1.0 - x[:,0].reshape(-1,1)))\n","      #  0.5*(1-t)*[(mu - r)/sigma]^2\n","      part_b = 0.5*(1.0 - x[:,0].reshape(-1,1))* ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n","      return -1.0*torch.exp(-part_a - part_b)\n","\n","    def train(self, \n","              epoch, \n","              lr, \n","              eqLossFn = 'calculateLoss', \n","              sample_method_X = \"U\", \n","              key_loss_func = torch.square, \n","              huber_delta = 0.5):\n","        \n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((epoch, 3*4 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        \n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        self.model.adapt_pi_epochs = True\n","\n","        # self.epoch_of_u = None\n","        # self.adapt_pi_epochs = False\n","        # self.start_adapt_epochs = [ [500, 1000, 2000, 5000 ], [4, 8, 10 ,20] ]\n","        # self.was_loss_beaten = False\n","\n","        for e in range(epoch):\n","            # pdb.set_trace()\n","\n","            batch_size_used = self.BATCH_SIZE\n","            if self.model.was_loss_beaten and self.model.adapt_pi_epochs:\n","              batch_size_used = self.BATCH_SIZE*20\n","\n","            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=batch_size_used)\n","\n","            self.model.epoch_of_u = e\n","            losses_L2, losses_ABS = loss_calc_method(sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False, util_network =self.net )\n","            \n","            # pdb.set_trace()\n","            loss , internal , terminal , losses_equalWeightedByType = losses_L2\n","            loss_abs , internal_abs , terminal_abs , losses_equalWeightedByType_abs = losses_ABS\n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) , to_cpu_detach(internal) , to_cpu_detach(terminal) , \n","                                       to_cpu_detach(loss_abs) , to_cpu_detach(internal_abs) , to_cpu_detach(terminal_abs), \n","                                       to_cpu_detach(losses_equalWeightedByType_abs)]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False,\n","                                                                                                     util_network =self.net)\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss_list = validation_loss_list.pop(5) # the L2 loss is duplicated at index 1\n","              self.validation_losses[e,:] = validation_loss_list\n","              # pdb.set_trace()\n","              # print(f\"Epoch {e} - Pi Pred (0.47) {self.model.pi_net(self.validation_sample[0]).item()}\")\n","\n","            \n","            if self.use_early_stop or self.model.adapt_pi_epochs:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                if self.model.was_loss_beaten and self.model.adapt_pi_epochs:\n","                  self.model.was_loss_beaten = False  # this was an increased batch for longer run, so we discount the loss here\n","                  if self.model.adapt_pi_epochs and e>(self.model.start_adapt_epochs[0][0]):\n","                    save_model_train(self, self.model, lr, eqLossFn, sample_method, self.best_loss, batch_size_used, e)  # but we still store it\n","                else:\n","                  self.model.was_loss_beaten = True\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","                self.model.was_loss_beaten = False\n","              if self.early_stop_counter>=self.early_stop_patience and self.use_early_stop:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","          \n","            \n","            before_params = list(self.net.parameters())[0].clone()\n","            # print('before_params', before_params)\n","            optimizer.zero_grad()\n","            \n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","            # for param in self.net.parameters():\n","            #   print('grad:', param.grad)\n","            optimizer.step()\n","            after_params = list(self.net.parameters())[0].clone()\n","            # print('after_params', after_params)\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                \n","                loss_avg = loss_avg/self.hook_interval\n","                \n","                sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","                x_internal , x_terminal = sample_batch\n","                \n","                # pdb.set_trace()\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                # print(f\"Epoch {e} - lr {lr} - Pi Weight {self.model.pi_net.fc_output.weight[0][0]} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                print(f\"Epoch {e} - lr {lr} - key loss: {round(loss.item(),5)} - eqWeighted loss: {round(losses_equalWeightedByType.item(),5)} - L1 loss {round(loss_abs.item(),5)} - Max Loss {round(max_loss_L2.item(),5)}\")\n","                print('internal loss:', internal , 'terminal loss:', terminal)\n","                \n","\n","                # plot the fitted value function vs the closed form (ideally straight line...)\n","                u_internal_sample = x_internal.clone()\n","                mask = (u_internal_sample[:,0] > 0.1) & (u_internal_sample[:,4] > 0.1)\n","                u_internal_sample = u_internal_sample[mask.reshape(-1),:]\n","                \n","                gamma = 1\n","                time = u_internal_sample[:,0].cpu().detach()\n","                wealth = u_internal_sample[:,1].cpu().detach()\n","                mu = u_internal_sample[:,2].cpu().detach()\n","                r = u_internal_sample[:,3].cpu().detach()\n","                sigma = u_internal_sample[:,4].cpu().detach()\n","                \n","                u_internal_sample = Variable(u_internal_sample, requires_grad=True)\n","                unet_preds = self.net(u_internal_sample)\n","                du_dt, du_dx, d2u_dx2 = self.model.get_utility_function_derivatives(unet_preds, u_internal_sample)\n","                \n","                pi_pred_example = self.model.pi_net(u_internal_sample.detach())\n","                \n","                print('mean pi', pi_pred_example.mean(), 'max pi', pi_pred_example.max(), 'min pi', pi_pred_example.min())\n","                print('mean d2u_dx2:', d2u_dx2.mean(),'max abs d2u_dx2:', torch.abs(d2u_dx2).max(), 'min abs d2u_dx2:', torch.abs(d2u_dx2).min())\n","                print('mean du_dx:', du_dx.mean(),'max abs du_dx:', torch.abs(du_dx).max(), 'min abs du_dx:', torch.abs(du_dx).min())\n","                print('mean du_dt:', du_dt.mean(),'max abs d2u_dx2:', torch.abs(du_dt).max(), 'min abs d2u_dx2:', torch.abs(du_dt).min())\n","\n","                u_net_results = unet_preds.detach().cpu().numpy().reshape(-1).tolist()\n","                htx_results = self.Htx(u_internal_sample, gamma).cpu().detach().numpy().reshape(-1).tolist()\n","                dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n","                \n","                print(ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point())\n","                \n","                dataf = pd.DataFrame( { 'pi_net': self.model.pi_net(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n","                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n","\n","                print(ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point())\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                \n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal = self.validation_sample\n","                    xinternal_before, xinternal_expanded = attach_pi_used(xinternal, self.model.pi_net, requires_grad=False)\n","                    xterminal_before, xterminal_expanded = attach_pi_used(xterminal, self.model.pi_net, requires_grad=False)\n","\n","                    xinternal_res = self.model.u_net(xinternal_expanded).detach()\n","                    xterminal_res = self.model.u_net(xterminal_expanded).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\", \"Mu\", \"R\", \"Sigma\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n","    def train_stratified(self , epoch , lr, \n","                         eqLossFn = 'calculateLoss', \n","                         sample_method_X = \"U\", \n","                         key_loss_func = torch.square, \n","                         huber_delta = 0.5\n","                         ):\n","        \n","        self.validation_losses = np.ones((epoch, 3*3 ), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 3*2 + 1), dtype='float32') * np.nan\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            # pdb.set_trace()\n","            internal_xts_bts, terminal_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n","            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n","            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n","            \n","            # pdb.set_trace()\n","            for stratum_count in range(len(internal_xts_bts)):              \n","              sample_batch = (internal_xts_bts[stratum_count], \n","                              terminal_xts_bts[stratum_count])  \n","\n","              # pdb.set_trace()\n","              stratum_losses_L2, stratum_losses_ABS = loss_calc_method(sample_batch, \n","                                                                       loss_transforms = [ key_loss_func, torch.abs ], \n","                                                                       keep_batch = False )\n","              # if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n","              #   pdb.set_trace()\n","              #   pass\n","            \n","              if training_stratum_losses is not None:\n","                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n","              else:\n","                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n","\n","              # pdb.set_trace()  \n","              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n","\n","            # pdb.set_trace()              \n","            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n","            loss = training_value_to_optimize\n","\n","            loss_optimized , internal , terminal, losses_equalWeightedByType, \\\n","            loss_abs , internal_abs , terminal_abs ,losses_equalWeightedByType_abs = training_loss_for_epoch            \n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal ]))\n","\n","            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n","                loss_calc_method( self.validation_sample, \n","                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                  keep_batch = False )\n","              validation_loss = [*to_cpu_detach(losses_L2_validation),\n","                                              *to_cpu_detach(losses_ABS_validation),\n","                                              *to_cpu_detach(losses_Huber_valiation)]\n","              # validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n","              self.validation_losses[e,:] = validation_loss\n","\n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                print('internal loss:', internal , 'terminal loss:', terminal)\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n"]},{"cell_type":"markdown","metadata":{"id":"oy05I1QFh7EM"},"source":["### Test Case"]},{"cell_type":"markdown","metadata":{"id":"U7zqglm1ewTL"},"source":["#### Test Case NO Stratification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"Tf-VoPhf2AJu","outputId":"c7717ac4-4a8b-448e-87ea-b775ebba8849"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-477cf1035b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrainMertonAlloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqLossFn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqLossFn\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msample_method_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_loss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-50-43a16ef44b22>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch, lr, eqLossFn, sample_method_X, key_loss_func, huber_delta)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_of_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mlosses_L2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_ABS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mkey_loss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil_network\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-2e3a9f840592>\u001b[0m in \u001b[0;36mcalculateLoss\u001b[0;34m(self, batch_x, train, loss_transforms, keep_batch, util_network)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mx_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_terminal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# print('MertonEquation calling self.criterion')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mLs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_internal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_terminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil_network\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutil_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mintC\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mterC\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-2e3a9f840592>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(self, x_internal, x_terminal, loss_transforms, util_network)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mpi_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_early_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpi_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_net_epoch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mpi_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-f28bf731511f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, eqLossFn, sample_method_X)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0md2u_dx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md2u_dx2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md2u_dx2_terminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mloss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mloss_calc_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdu_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdu_dx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2u_dx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;31m# print(f\"Pi Net Epoch {e} Loss {round(loss.item(),5)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-a56e0c476598>\u001b[0m in \u001b[0;36mcalculatePiLoss\u001b[0;34m(self, x_internal, grads, keep_batch)\u001b[0m\n\u001b[1;32m     48\u001b[0m         '''        \n\u001b[1;32m     49\u001b[0m         \u001b[0mx_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_internal\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mLs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mreturn_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-a56e0c476598>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(self, x_internal, grads)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0md2u_dx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# du_dt is grads[0], not relevant here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mpi_net_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_internal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mpi_net_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi_net_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e3befd7421c8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# for i, layer in enumerate(self.middle_layer_2):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e3befd7421c8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, s)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"stream","name":"stdout","text":["time: 6min 35s (started: 2022-06-17 15:03:45 +00:00)\n"]}],"source":["seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","# u_net, pi_net, pi_net_epoch, pi_net_lr\n","eqLossFn= 'calculateLoss'\n","sample_method= \"U\"\n","lr = 0.0005\n","lr_for_pi = 0.0001\n","max_pi_epochs = 4 # has to be low!!!\n","\n","u_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 20, depth=20 )\n","u_net.to(torch.device(\"cuda:0\")) \n","\n","pi_net = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 20, depth=10 )\n","pi_net.to(torch.device(\"cuda:0\")) \n","\n","pi_net_target = MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 20, depth=10  )\n","pi_net_target.to(torch.device(\"cuda:0\")) \n","mequation = MertonEquation(u_net, pi_net, pi_net_target, max_pi_epochs, lr_for_pi)\n","mequation.adapt_pi_epochs = True\n","\n","trainMertonAlloc = TrainHJBMertonWithDGM(u_net, mequation, BATCH_SIZE = 2**5 , debug = False )\n","trainMertonAlloc.hook_interval = 400\n","trainMertonAlloc.use_early_stop = True\n","trainMertonAlloc.early_stop_patience = 3000\n","\n","trainMertonAlloc.validation_sample = mequation.sample(sample_method, size=2**8)\n","\n","\n","trainMertonAlloc.train(epoch = 300000 , lr = lr, eqLossFn = eqLossFn , sample_method_X = sample_method, key_loss_func=torch.abs)"]},{"cell_type":"markdown","metadata":{"id":"kowxNupDAwOI"},"source":["List of params for successfull run\n","\n","\n","\n","*   Loss = L1\n","*   Util Net + piNet: depth = 3, NN= 50\n","*   lr = 0.005\n","*   lr_for_pi = 0.002\n","*   max_pi_epochs = 5\n","*   trainMertonAlloc.use_early_stop = False\n","*   epoch = 6100\n","*   loss weights = 1\\*intC + 1\\*terminal\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWqq1hGijWvw"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u35g5sAPbsG_"},"outputs":[],"source":["# check control for closed form:\n","# PI(x,t) = [(mu-r)/(gamma*sigma^2)]*exp(-r*(T-t))\n","# ((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))\n","# gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   # PI\n","\n","gamma = 1\n","internal_sample, terminal_sample = mequation.sample(size=2000, to_cpu=False)\n","mask = (internal_sample[:,0] > 0.1) & (internal_sample[:,4] > 0.1)\n","internal_sample = internal_sample[mask.reshape(-1),:]\n","# time, wealth, mu, r, sigma\n","time = internal_sample[:,0].cpu().detach()\n","wealth = internal_sample[:,1].cpu().detach()\n","mu = internal_sample[:,2].cpu().detach()\n","r = internal_sample[:,3].cpu().detach()\n","sigma = internal_sample[:,4].cpu().detach()\n","\n","# mequation.pi_net(internal_sample)[:,0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-NmMG33xGBu"},"outputs":[],"source":["# closed form value function\n","def Htx(x, gamma=1):\n","  #  wealth * gamma * exp(r*(1-t))\n","  part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1)*(1.0 - x[:,0].reshape(-1,1)))\n","  \n","  #  0.5*(1-t)*[(mu - r)/sigma]^2\n","  part_b = 0.5*(1.0 - x[:,0].reshape(-1,1))* ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n","\n","  return -1.0*torch.exp(-part_a - part_b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Woj8BoAK5OLC"},"outputs":[],"source":["def Htx(x, gamma=1):\n","  #  wealth * gamma * exp(r*(1-t))\n","  part_a = x[:,1].reshape(-1,1)*gamma*torch.exp(x[:,3].reshape(-1,1)*(1.0 - x[:,0].reshape(-1,1)))\n","  \n","  #  0.5*(1-t)*[(mu - r)/sigma]^2\n","  part_b = 0.5*(1.0 - x[:,0].reshape(-1,1))* ((x[:,2].reshape(-1,1) - x[:,3].reshape(-1,1))/(x[:,4].reshape(-1,1)))**2 \n","\n","  return -1.0*torch.exp(-part_a - part_b)\n","  \n","# plot the fitted value function vs the closed form (ideally straight line...)\n","u_internal_sample = torch.cat((internal_sample, mequation.pi_net(internal_sample).reshape(-1,1)), dim=1)\n","u_internal_sample = internal_sample\n","\n","u_net_results = u_net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n","htx_results = Htx(u_internal_sample, gamma).cpu().detach().numpy().reshape(-1).tolist()\n","dataf2 = pd.DataFrame( { 'u_net': u_net_results, 'closed_form': htx_results } )\n","ggplot(dataf2, aes(x='u_net', y='closed_form')) + geom_point()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHfTC6GljYnd"},"outputs":[],"source":["# plot the control function vs the closed form (ideally straight line...)\n","dataf = pd.DataFrame( { 'pi_net': mequation.pi_net(internal_sample).cpu().detach().numpy().reshape(-1).tolist(), \n","                       'closed_form': (((mu-r)/(gamma*(sigma**2)))*np.exp(-r*(1.0-time))).numpy().tolist() } )\n","\n","temp = dataf[dataf['closed_form'] < 0.1]\n","# plt.scatter(temp['pi_net'], temp['closed_form'])\n","g = ggplot(dataf, aes(x='pi_net', y='closed_form')) + geom_point()\n","g.draw()\n","# ggplot(temp, aes(x='pi_net', y='closed_form')) + geom_point()\n","\n","\n","# plt.yscale('log')\n","# plt.xscale('log')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f85Kg2zvhMdM"},"outputs":[],"source":["mequation = MertonEquation(MertonUtilityNet( NL = 1 , NN = 3 ), MertonAlternativePiNet( in_size = 5 , out_size = 1, neurons = 100, depth=5 ), 1, 10000.0)\n","# val_sample_to_use = tuple([ x.cpu().detach() for x in mequation.sample(sample_method_X=\"U\", size=1) ] )\n","val_sample_to_use = mequation.sample(sample_method_X=\"U\", size=1) \n","# # gamma = 1.0 # time = 0.0 # mu = 0.05 # r = 0.02 # sigma = 0.25   \n","val_sample_to_use[0][0,0] = 0.0\n","val_sample_to_use[0][0,2] = 0.05\n","val_sample_to_use[0][0,3] = 0.02\n","val_sample_to_use[0][0,4] = 0.25"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7ce6_TPrsG4"},"outputs":[],"source":["import torch\n","\n","def u(q):\n","  x = q[:,1]\n","  y = q[:, 0]\n","  return x**2 + y**2\n","x = torch.randn(3, requires_grad=True)\n","t = torch.randn(3, requires_grad=True)\n","u_val = u(torch.cat((x, t), axis=1))\n","\n","print(x,t,u_val)\n","print(torch.cat((t,x)))\n","# 1st derivatives\n","dt = torch.autograd.grad(u_val, x, grad_outputs=torch.ones_like(u_val), create_graph=True, allow_unused=True)\n","print(dt[0])\n","dt = torch.autograd.grad(u_val.sum(), torch.cat((t,x)), create_graph=True)\n","print(dt[0])\n","dx = torch.autograd.grad(u_val.sum(), x, create_graph=True)[0]\n","\n","# 2nd derivatives (higher orders require `create_graph=True`)\n","ddx = torch.autograd.grad(dx.sum(), x)[0]\n","ddx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUOACTjkXBM0"},"outputs":[],"source":["!nvidia-smi"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["BErSeefeQwQi","N-GO35FcJPP6","HrivvbmubiiY","fyFbPZr7I5RE","RNhAbZ727RC_","CNsqOm1ithSG","wClW1g9rbm8o","8RRoBgFQINMv"],"machine_shape":"hm","name":"Copy of DGM_HJB_Working_v2.ipynb","provenance":[{"file_id":"1xVdG0rciqMrqeTV_cPf6Cwghe9Ueq0zS","timestamp":1655480279702},{"file_id":"10VwSo586IIztHcGRXJuc9KyXO69w3p3W","timestamp":1655373733114}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}