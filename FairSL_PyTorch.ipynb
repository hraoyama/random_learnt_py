{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FairSL_PyTorch.ipynb","provenance":[{"file_id":"1CMJHEEXtClCh_7Kh6d4z-O_uQdmIJZu7","timestamp":1643228196002}],"collapsed_sections":[],"private_outputs":true,"machine_shape":"hm","background_execution":"on"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Tz7Az8rXJTpf"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import LinearSVC\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","import numpy as np\n","import os\n","import pandas as pd\n","\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm"]},{"cell_type":"code","source":["%pip install gpflow"],"metadata":{"id":"oanGod-5-MMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"g-TIfqWgCjUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BI4p7ZKb0Qz2"},"outputs":[],"source":["paper_name = \"FairML\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"433z6V3T2rB2"},"outputs":[],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/FairBoosting/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/FairBoosting/SVM\")"]},{"cell_type":"code","source":["def dataset_reader(dataset_name):\n","    \n","    if dataset_name == 'loan_defaults':\n","        # https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n","\n","        df = pd.read_excel('/content/drive/MyDrive/Datasets/Credit/credit_data.xls', skiprows=[0])\n","        df = df.drop(columns=['ID'])\n","\n","    elif dataset_name in ['adult', 'adult_multi']:\n","        # https://archive.ics.uci.edu/ml/datasets/adult\n","\n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/Adult/adult.csv')\n","\n","    elif dataset_name == 'german_credit':\n","        # https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n","\n","        german_columns_csv = ['Account Balance', 'Duration of Credit (month)',\n","              'Payment Status of Previous Credit', 'Purpose', 'Credit Amount',\n","              'Value Savings/Stocks', 'Length of current employment',\n","              'Instalment per cent', 'Sex & Marital Status', 'Guarantors',\n","              'Duration in Current address', 'Most valuable available asset',\n","              'Age (years)', 'Concurrent Credits', 'Type of apartment',\n","              'No of Credits at this Bank', 'Occupation', 'No of dependents',\n","              'Telephone', 'Foreign Worker', 'Creditability']\n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/German/german.data', delim_whitespace=True, header=None)\n","        df.columns = german_columns_csv\n","        # extra mapping\n","        sex_and_marital_map = {'A91':'Male', 'A93':'Male', 'A94':'Male', 'A92':'Female', 'A95':'Female'}\n","        df['Sex & Marital Status'] = df['Sex & Marital Status'].map(sex_and_marital_map)\n","        df.rename({'Sex & Marital Status': 'Sex'}, axis=1, inplace=True)\n","\n","    elif dataset_name == 'german_credit_multi':\n","        # https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n","\n","        german_columns_csv = ['Account Balance', 'Duration of Credit (month)',\n","              'Payment Status of Previous Credit', 'Purpose', 'Credit Amount',\n","              'Value Savings/Stocks', 'Length of current employment',\n","              'Instalment per cent', 'Sex & Marital Status', 'Guarantors',\n","              'Duration in Current address', 'Most valuable available asset',\n","              'Age (years)', 'Concurrent Credits', 'Type of apartment',\n","              'No of Credits at this Bank', 'Occupation', 'No of dependents',\n","              'Telephone', 'Foreign Worker', 'Creditability']\n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/German/german.data', delim_whitespace=True, header=None)\n","        df.columns = german_columns_csv\n","\n","        #df = pd.read_csv('/content/drive/MyDrive/Datasets/German/german_credit.csv')\n","\n","    elif dataset_name == 'communities_and_crime':\n","        # http://archive.ics.uci.edu/ml/datasets/communities+and+crime\n","\n","        with open('/content/drive/MyDrive/Datasets/Communities_and_Crime/communities.names') as f:\n","            colnames = []\n","            for line in f.readlines():\n","                if line[:10] == '@attribute':\n","                  colnames.append(line.split(' ')[1])\n","        \n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/Communities_and_Crime/communities.data', header=None)\n","        df.columns = colnames\n","        df.drop(['state', 'county', 'community', 'communityname', 'fold'], axis=1, inplace=True)\n","        \n","    elif dataset_name == 'portuguese_marketing':\n","        # https://archive.ics.uci.edu/ml/datasets/bank+marketing\n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/PortugueseBankMarketing/bank/bank-full.csv', sep=';')\n","\n","    elif dataset_name == 'student_performance':\n","        # https://archive.ics.uci.edu/ml/datasets/student+performance\n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/Student_Performance/student-por.csv', sep=';')\n","    \n","    elif dataset_name == 'compas':\n","        # https://github.com/propublica/compas-analysis\n","        df = pd.read_csv('/content/drive/MyDrive/Datasets/COMPAS/compas-scores-two-years.csv')\n","    \n","    elif dataset_name == 'framingham':\n","        raise NotImplementedError('Need to find data')\n","\n","    elif dataset_name == 'meps':\n","        # https://github.com/Trusted-AI/AIF360/tree/master/aif360/data/raw/meps\n","        target_path = '/usr/local/lib/python3.7/dist-packages/aif360/data/raw/meps'\n","        source_path = '/content/drive/MyDrive/Datasets/MEPS'\n","        from shutil import copyfile\n","        if not os.path.exists(os.path.join(target_path, 'h181.csv')):\n","            copyfile(os.path.join(source_path,'h181.csv'), os.path.join(target_path, 'h181.csv'))\n","        \n","        if not os.path.exists(os.path.join(target_path, 'h192.csv')):\n","            copyfile(os.path.join(source_path,'h192.csv'), os.path.join(target_path, 'h192.csv'))\n","\n","        # Datasets\n","        from aif360.datasets import MEPSDataset19\n","        df = MEPSDataset19().convert_to_dataframe()[0]\n","\n","    else:\n","       raise ValueError('Does not yet support this dataset {}'.format(dataset_name))\n","    \n","    df = df.reset_index(drop=True)\n","    return df\n","\n","\n","def protected_and_target_features(dataset_name):\n","    \n","    if dataset_name == 'adult':\n","        protected_features = ['gender']\n","        target_feature = 'income'\n","\n","    elif dataset_name == 'adult_multi':\n","        protected_features = ['race']\n","        target_feature = 'income'\n","\n","    elif dataset_name == 'loan_defaults':\n","        protected_features = ['SEX']\n","        target_feature = 'default payment next month'\n","\n","    elif dataset_name == 'german_credit':\n","        protected_features = ['Sex']\n","        target_feature = 'Creditability'\n","\n","    elif dataset_name == 'german_credit_multi':\n","        protected_features = ['Sex & Marital Status']\n","        target_feature = 'Creditability'\n","    \n","    elif dataset_name == 'communities_and_crime':\n","        protected_features = ['racepctblack']\n","        target_feature = 'ViolentCrimesPerPop'\n","\n","    elif dataset_name == 'student_performance':\n","        protected_features = ['sex']\n","        target_feature = 'G3'\n","\n","    elif dataset_name == 'portuguese_marketing':\n","        protected_features = ['marital']\n","        target_feature = 'y'\n","    \n","    elif dataset_name == 'compas':\n","        raise ValueError(\"needs to be implemented\")\n","    \n","    elif dataset_name == 'meps':\n","        raise ValueError(\"needs to be implemented\")\n","    else:\n","        raise ValueError(\"No such data set\")\n","\n","    return protected_features, target_feature\n","\n","\n","def preprocess_dataset(dataset_name, drop=False):\n","\n","    data = dataset_reader(dataset_name)\n","    cols_dict = {k : v for k,v in data.dtypes.to_dict().items() if v == np.dtype('O')}\n","\n","    object_cols = sorted([x for x in cols_dict.keys()])\n","    value_cols = sorted([x for x in data.columns if x not in object_cols])\n","\n","    #object_cols = sorted([x for x in cols_dict.keys() if x not in protected_features])\n","    #value_cols = sorted([x for x in data.columns if (x not in object_cols) and (x not in protected_features)])\n","    #object_cols = [x for x in object_cols if x!=target_feature]\n","    #value_cols = [x for x in value_cols if x!=target_feature]\n","\n","    data, drop_list = treat_missing_values(data, drop)\n","    if len(object_cols) > 0:\n","        ohe_data = customOneHotEncoding(data, object_cols, drop_list)\n","        comb_data = pd.concat([data[value_cols], ohe_data], axis=1)\n","    else:\n","        comb_data = data[value_cols]\n","\n","    data = data.reset_index(drop=True)\n","    comb_data = comb_data.reset_index(drop=True)\n","\n","    return data, comb_data\n","\n","def treat_missing_values(data, drop=False):\n","\n","    special_symbols = ['?']\n","    drop_list = []\n","    for col in data.columns:\n","        for sym in special_symbols:\n","            if (data[col] == sym).any():\n","                data[col] = data[col].map(lambda x: x.replace(sym, 'missing'))\n","                drop_list.append(col)\n","\n","    if drop:\n","        data = data.dropna()\n","    else:\n","        data = data.fillna('missing')\n","    \n","    return data, drop_list\n","\n","\n","def customOneHotEncoding(data, object_cols, drop_cols):\n","    \n","    res = []\n","    for col in object_cols:\n","        if col in drop_cols:\n","            ohe = OneHotEncoder(sparse=False, drop=['missing'])\n","        else:\n","            ohe = OneHotEncoder(sparse=False, drop='first')\n","\n","        temp = ohe.fit_transform(data[col].values.reshape(-1, 1))\n","        temp = pd.DataFrame(data=temp, columns=ohe.get_feature_names_out([col])) \n","        res.append(temp)\n","\n","    return pd.concat(res, axis=1)"],"metadata":{"id":"03Hqb_vepW5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2N_AA5RZm4gQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_name = 'loan_defaults'"],"metadata":{"id":"6ZLfoAN0pn7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SAD7pU0upn9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["protected_features, target = protected_and_target_features(dataset_name)\n","protected_feature = protected_features[0]\n","_, processed_data = preprocess_dataset(dataset_name)\n","\n","protect_col = [x for x in processed_data.columns if protected_feature in x][0]\n","target_col = [x for x in processed_data.columns if target in x][0]\n","y = processed_data[target_col]\n","gr = processed_data[protect_col]\n","processed_data.drop(columns=[protect_col, target_col], inplace=True)\n","\n","sc = StandardScaler()\n","processed_data = sc.fit_transform(processed_data)"],"metadata":{"id":"ykHxKTFjpNiL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if dataset_name == 'german_credit':\n","    y.loc[y==2] = -1\n","elif dataset_name in ['adult', 'loan_defaults']:\n","    y.loc[y==0] = -1"],"metadata":{"id":"JKspCF6rJ0sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"p6V5RPc83oT5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    \"\"\"\n","    MovieLens 20M Dataset\n","    Data preparation\n","        treat samples with a rating less than 3 as negative samples\n","    :param dataset_path: MovieLens dataset path\n","    Reference:\n","        https://grouplens.org/datasets/movielens\n","    \"\"\"\n","\n","    def __init__(self, X, y, gr):\n","\n","        self.data = X\n","        self.targets = y\n","        self.groups = gr\n","\n","    def __len__(self):\n","        return self.targets.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.data[index], self.targets[index], self.groups[index]"],"metadata":{"id":"7DHT6i5Mpvzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YWvqRThtpNm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dWciwRI83-uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = MyDataset(processed_data, y.values, gr.values)"],"metadata":{"id":"goLzqLQkpNpP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_length = int(len(dataset) * 0.8)\n","test_length = len(dataset) - train_length\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_length, test_length), generator=torch.Generator().manual_seed(42))"],"metadata":{"id":"-h9rAOD3q0h6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 100\n","train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_data_loader = DataLoader(test_dataset, batch_size=batch_size)"],"metadata":{"id":"7Y1ykxAQq9nO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ALPHA_GRID = [1e-5, 1e-3, 1e-2, 1e-1, 5e-1, 1, 1e1, 3e1, 1e2, 1e3, 3e3, 1e4]\n","FILEPATH = f'/content/drive/MyDrive/data_papers/{paper_name}/FairBoosting/SVM'"],"metadata":{"id":"fyh3E9rKroEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gpflow\n","from scipy.optimize import minimize, LinearConstraint\n","from functools import partial"],"metadata":{"id":"29WvHNui6cGE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(w.data)"],"metadata":{"id":"h4BRSfwhBVul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(y)"],"metadata":{"id":"lXjQDieHLTVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def func_to_min(bws,w1,w2,n,y,mask,target,alpha,C):\n","  b = torch.from_numpy(np.array(bws[-1]))\n","  w = torch.from_numpy(bws[:-1])\n","  n1 = int(w1*n)\n","  loss1 = torch.mean(torch.maximum(torch.zeros(n1), 1 - target[mask]*y[mask]))\n","  loss2 = torch.mean(torch.maximum(torch.zeros(n-n1), 1 - target[~mask]*y[~mask]))\n","  max_loss = torch.maximum(loss1, loss2)\n","  loss = max_loss + 1/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss)) + w2*torch.exp(alpha*(loss2-max_loss))) + C/2*torch.dot(w, w) \n","  return(loss)\n","n1"],"metadata":{"id":"6rCA7w62MDb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rFqiSjvFNqa6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc = LinearSVC(max_iter=5000, C=1)"],"metadata":{"id":"3VAnR8yzAT2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc.fit(train_dataset.dataset.data[train_dataset.indices], train_dataset.dataset.targets[train_dataset.indices])"],"metadata":{"id":"0AM3ohlCAT5N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc.coef_"],"metadata":{"id":"Q-JYh9R_AWkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C = 30\n","lr = 1e-2\n","L = 0.1\n","num_epochs = 1000\n","\n","for alpha in ALPHA_GRID:\n","\n","    w = torch.autograd.Variable(torch.Tensor(svc.coef_.flatten()), requires_grad=True)\n","    b = torch.autograd.Variable(torch.Tensor(svc.intercept_), requires_grad=True)\n","    optimizer = torch.optim.SGD([w, b], lr=lr)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n","    curr_loss = np.inf\n","\n","    device = 'cpu'\n","    for epoch in tqdm(range(num_epochs)):\n","        for i, (fields, target, gr) in enumerate(train_data_loader):\n","            fields, target, gr = fields.to(device), target.to(device), gr.to(device)\n","\n","            y = torch.matmul(fields.float(), w) + b\n","            mask = (gr==1)\n","\n","            n1 = y[mask].size()[0]\n","            n = y.size()[0]\n","            w1 = n1 / n\n","            w2 = 1 - w1\n","\n","            loss1 = torch.mean(torch.maximum(torch.zeros(n1), 1 - target[mask]*y[mask]))\n","            loss2 = torch.mean(torch.maximum(torch.zeros(n-n1), 1 - target[~mask]*y[~mask]))\n","            #loss1 = torch.mean(torch.log(1 + torch.exp(- target[mask]*y[mask])))\n","            #loss2 = torch.mean(torch.log(1 + torch.exp(- target[~mask]*y[~mask])))\n","            \n","            max_loss = torch.maximum(loss1, loss2)\n","            total_loss = max_loss + 1/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss))+ w2*torch.exp(alpha*(loss2-max_loss)))\n","\n","            loss = C * total_loss + 0.001 * torch.mean(torch.dot(w, w)) + L * torch.mean(torch.abs(w))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            \n","            #w.data -= step_size * w.grad.data # step\n","            #b.data -= step_size * b.grad.data # step\n","\n","            #w.grad.data.zero_()\n","            #b.grad.data.zero_()\n","        \n","        if curr_loss > total_loss.detach().item():\n","            curr_loss = total_loss.detach().item()\n","            w_best = w.detach().numpy().copy()\n","            b_best = b.detach().numpy().copy()\n","        else:\n","            scheduler.step()\n","    print(alpha, \"--->\", np.corrcoef(w.detach().numpy(), svc.coef_.flatten())[0][1], \"--->\", np.corrcoef(w_best, svc.coef_.flatten())[0][1])\n","\n","    #w = torch.autograd.Variable(torch.Tensor(w_best), requires_grad=True)\n","    #b = torch.autograd.Variable(torch.Tensor(b_best), requires_grad=True)\n","\n","    filename = dataset_name + '_' + str(alpha) + '_temp2SVM' + '.npy'\n","    with open(os.path.join(FILEPATH, filename), 'wb') as f:\n","        #np.save(f, np.concatenate([w_best, b_best]))\n","        np.save(f, torch.concat([w, b]).detach().numpy())"],"metadata":{"id":"jkmk3W4LNjaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C = 1e-2\n","num_epochs = 100\n","\n","# losses_test = []\n","\n","for alpha in ALPHA_GRID[:1]:\n","\n","    w = torch.autograd.Variable(torch.randn(processed_data.shape[1]), requires_grad=True)\n","    b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n","\n","    w = torch.autograd.Variable(torch.randn(processed_data.shape[1]), requires_grad=True)\n","\n","\n","\n","    # optimizer = torch.optim.SGD([w, b], lr=2e-1)\n","    # # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.95)\n","    # optimizer = gpflow.optimizers.Scipy()\n","\n","    device = 'cpu'\n","    for epoch in tqdm(range(num_epochs)[:2]):\n","        for i, (fields, target, gr) in enumerate(train_data_loader):\n","            \n","            fields, target, gr = fields.to(device), target.to(device), gr.to(device)\n","            y = torch.matmul(fields.float(), w) + b\n","            mask = (gr==1)\n","\n","            n1 = y[mask].size()[0]\n","            n = y.size()[0]\n","            w1 = n1 / n\n","            w2 = 1 - w1\n","\n","            loss1 = torch.mean(torch.maximum(torch.zeros(n1), 1 - target[mask]*y[mask]))\n","            loss2 = torch.mean(torch.maximum(torch.zeros(n-n1), 1 - target[~mask]*y[~mask]))\n","            max_loss = torch.maximum(loss1, loss2)\n","\n","            loss = max_loss + 1/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss)) + w2*torch.exp(alpha*(loss2-max_loss))) + C/2*torch.dot(w, w) \n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            def func_to_min(bws,w1,w2,n,y,mask,target,alpha,C):\n","              b = torch.from_numpy(np.array(bws[-1]))\n","              w = torch.from_numpy(bws[:-1])\n","              n1 = int(w1*n)\n","              loss1 = torch.mean(torch.maximum(torch.zeros(n1), 1 - target[mask]*y[mask]))\n","              loss2 = torch.mean(torch.maximum(torch.zeros(n-n1), 1 - target[~mask]*y[~mask]))\n","              # print(loss1, loss2)\n","              max_loss = torch.maximum(loss1, loss2)\n","              loss = max_loss + 1.0/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss)) + w2*torch.exp(alpha*(loss2-max_loss))) + C/2*torch.dot(w, w) \n","              return(loss)\n","\n","            w.data -= w.grad.data # step\n","            b.data -= b.grad.data # step\n","\n","            search_start = w.data[:].numpy()\n","            search_start = np.append(search_start, b.data[:].numpy())\n","\n","            res = minimize(\n","                partial(func_to_min, w1=w1, w2=w2, n=n, y=y,mask=mask, alpha=alpha, C=C, target=target),\n","                x0=search_start\n","            )\n","\n","            w.data = torch.from_numpy(res.x[:-1])\n","            b.data = torch.from_numpy(res.x[-1])\n","\n","            # loss = max_loss + 1/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss)) + w2*torch.exp(alpha*(loss2-max_loss))) + C/2*torch.dot(w, w) \n","            #scheduler.step()\n","\n","            #w.grad.data.zero_()\n","            #b.grad.data.zero_()\n","            # print(loss)\n","            # losses_test.append(loss)\n","\n","    filename = dataset_name + '_' + str(alpha) + '_temp' + '.npy'\n","    with open(os.path.join(FILEPATH, filename), 'wb') as f:\n","        np.save(f, torch.concat([w, b]).detach().numpy())"],"metadata":{"id":"skvoo566tTm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# def run_optimizer_on_minibatch_size(model, iterations, minibatch_size, xAndY):\n","#     \"\"\"\n","#     Utility function running a Scipy optimizer\n","#     :param model: GPflow model\n","#     :param interations: number of iterations\n","#     \"\"\"\n","#     N = xAndY[0].shape[0]\n","#     tensor_data = tuple(map(tf.convert_to_tensor, xAndY))\n","#     train_dataset = tf.data.Dataset.from_tensor_slices(tensor_data).repeat().shuffle(N)\n","    \n","#     logf = []\n","#     train_iter = iter(train_dataset.batch(minibatch_size))\n","#     training_loss = model.training_loss_closure(train_iter, compile=True)\n","#     optimizer = gpflow.optimizers.Scipy()\n","\n","#     @tf.function   # had to remove this decorator\n","#     def optimization_step():\n","#         optimizer.minimize(training_loss, model.trainable_variables)\n","\n","#     # step = 0\n","#     for step in range(iterations):\n","#         optimization_step()\n","#         if step % 10 == 0:\n","#             elbo = -training_loss().numpy()\n","#             logf.append(elbo)\n","#             print(elbo)\n","#     return logf\n","\n","# from gpflow.ci_utils import ci_niter\n","# maxiter = ci_niter(20000)\n","# logf = run_optimizer_on_minibatch_size(m, maxiter, minibatch_size, (X,Y))"],"metadata":{"id":"hCNhzSxe97pI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def measure_performance(filenames, skip, isTrain=True):\n","    \n","    df = pd.DataFrame(columns=['alpha', 'loss', 'disp', 'accuracy'])\n","\n","    for i, filename in enumerate(filenames):\n","        with open(os.path.join(FILEPATH, filename), 'rb') as f:\n","            params = np.load(f)\n","        w = params[:-1]\n","        b = params[-1]\n","\n","        alpha = float(filename.split('/')[-1][skip:].split('_')[0])\n","\n","        if isTrain:\n","            X = train_dataset.dataset.data[train_dataset.indices]\n","            target = train_dataset.dataset.targets[train_dataset.indices]\n","            gr = train_dataset.dataset.groups[train_dataset.indices]\n","        else:\n","            X = test_dataset.dataset.data[test_dataset.indices]\n","            target = test_dataset.dataset.targets[test_dataset.indices]\n","            gr = test_dataset.dataset.groups[test_dataset.indices]\n","\n","        mask = (gr == 1)\n","        n1 = np.sum(mask)\n","        n2 = np.sum(~mask)\n","\n","        y_ = np.dot(X, w) + b\n","        loss1 = np.mean(np.maximum(np.zeros(n1), 1 - target[mask]*y_[mask]))\n","        loss2 = np.mean(np.maximum(np.zeros(n2), 1 - target[~mask]*y_[~mask]))\n","\n","        correct = np.sum(np.sign(y_).astype(int) == target)\n","        accuracy = correct / len(y_)\n","\n","        w1 = n1/(n1+n2)\n","        w2 = 1 - w1\n","\n","        df.loc[i] = [alpha, w1*loss1 + w2*loss2, np.abs(loss1 - loss2), accuracy]\n","    \n","        # print(w1*loss1 + w2*loss2)\n","\n","    df.sort_values(by=['alpha'], inplace=True)\n","    return df"],"metadata":{"id":"tHTAenXPQBMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_list"],"metadata":{"id":"eHNk1CEppyfc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_list = [x for x in os.listdir(FILEPATH) if ('temp' in x ) and ('loan' in x)]"],"metadata":{"id":"aV6HCOOjpyiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["skip_symbols = len(dataset_name + '_')\n","res = measure_performance(file_list, skip=skip_symbols, isTrain=True)"],"metadata":{"id":"alyMKTwRQBPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.scatter(res['loss'].values, res['disp'].values, s=20+5*np.arange(len(res)))"],"metadata":{"id":"nZj2cX6DWq-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4o18vs2OWrDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sKnel8uUWrF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aOqF4c7DQBUk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5j8SoPDdOnpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TlHRyylBqfLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc = LinearSVC(max_iter=5000, C=1)"],"metadata":{"id":"9hFNMAp489x2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc.fit(train_dataset.dataset.data[train_dataset.indices], train_dataset.dataset.targets[train_dataset.indices])"],"metadata":{"id":"9R9AlcTY890W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc.score(train_dataset.dataset.data[train_dataset.indices], train_dataset.dataset.targets[train_dataset.indices])"],"metadata":{"id":"CUQOGgwXqfSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.sum(svc.coef_**2)"],"metadata":{"id":"RhQ8G1AYL2OG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.corrcoef(w.detach().numpy(), svc.coef_.flatten())"],"metadata":{"id":"CDaqYZV56n5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.sum(w**2)"],"metadata":{"id":"u2Af6BeD6pPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","    Author: Lasse Regin Nielsen\n","\"\"\"\n","\n","from __future__ import division, print_function\n","import os\n","import numpy as np\n","import random as rnd\n","# filepath = os.path.dirname(os.path.abspath(__file__))\n","filepath =\"/content/drive/MyDrive/svm-smo/\"\n","\n","\n","class SVM():\n","    \"\"\"\n","        Simple implementation of a Support Vector Machine using the\n","        Sequential Minimal Optimization (SMO) algorithm for training.\n","    \"\"\"\n","    def __init__(self, max_iter=10000, kernel_type='linear', C=1.0, epsilon=0.001):\n","        self.kernels = {\n","            'linear' : self.kernel_linear,\n","            'quadratic' : self.kernel_quadratic\n","        }\n","        self.max_iter = max_iter\n","        self.kernel_type = kernel_type\n","        self.C = C\n","        self.epsilon = epsilon\n","        \n","    def fit(self, X, y):\n","        # Initialization\n","        n, d = X.shape[0], X.shape[1]\n","        alpha = np.zeros((n))\n","        kernel = self.kernels[self.kernel_type]\n","        count = 0\n","        while True:\n","            count += 1\n","            alpha_prev = np.copy(alpha)\n","            for j in range(0, n):\n","                i = self.get_rnd_int(0, n-1, j) # Get random int i~=j\n","                x_i, x_j, y_i, y_j = X[i,:], X[j,:], y[i], y[j]\n","                k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n","                if k_ij == 0:\n","                    continue\n","                alpha_prime_j, alpha_prime_i = alpha[j], alpha[i]\n","                (L, H) = self.compute_L_H(self.C, alpha_prime_j, alpha_prime_i, y_j, y_i)\n","\n","                # Compute model parameters\n","                self.w = self.calc_w(alpha, y, X)\n","                self.b = self.calc_b(X, y, self.w)\n","\n","                # Compute E_i, E_j\n","                E_i = self.E(x_i, y_i, self.w, self.b)\n","                E_j = self.E(x_j, y_j, self.w, self.b)\n","\n","                # Set new alpha values\n","                alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n","                alpha[j] = max(alpha[j], L)\n","                alpha[j] = min(alpha[j], H)\n","\n","                alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n","\n","            # Check convergence\n","            diff = np.linalg.norm(alpha - alpha_prev)\n","            if diff < self.epsilon:\n","                break\n","\n","            if count >= self.max_iter:\n","                print(\"Iteration number exceeded the max of %d iterations\" % (self.max_iter))\n","                return\n","        # Compute final model parameters\n","        self.b = self.calc_b(X, y, self.w)\n","        if self.kernel_type == 'linear':\n","            self.w = self.calc_w(alpha, y, X)\n","        # Get support vectors\n","        alpha_idx = np.where(alpha > 0)[0]\n","        support_vectors = X[alpha_idx, :]\n","        return support_vectors, count\n","    def predict(self, X):\n","        return self.h(X, self.w, self.b)\n","    def calc_b(self, X, y, w):\n","        b_tmp = y - np.dot(w.T, X.T)\n","        return np.mean(b_tmp)\n","    def calc_w(self, alpha, y, X):\n","        return np.dot(X.T, np.multiply(alpha,y))\n","    # Prediction\n","    def h(self, X, w, b):\n","        return np.sign(np.dot(w.T, X.T) + b).astype(int)\n","    # Prediction error\n","    def E(self, x_k, y_k, w, b):\n","        return self.h(x_k, w, b) - y_k\n","    def compute_L_H(self, C, alpha_prime_j, alpha_prime_i, y_j, y_i):\n","        if(y_i != y_j):\n","            return (max(0, alpha_prime_j - alpha_prime_i), min(C, C - alpha_prime_i + alpha_prime_j))\n","        else:\n","            return (max(0, alpha_prime_i + alpha_prime_j - C), min(C, alpha_prime_i + alpha_prime_j))\n","    def get_rnd_int(self, a,b,z):\n","        i = z\n","        cnt=0\n","        while i == z and cnt<1000:\n","            i = rnd.randint(a,b)\n","            cnt=cnt+1\n","        return i\n","    # Define kernels\n","    def kernel_linear(self, x1, x2):\n","        return np.dot(x1, x2.T)\n","    def kernel_quadratic(self, x1, x2):\n","        return (np.dot(x1, x2.T) ** 2)\n"],"metadata":{"id":"-A-ScWrwfmwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from __future__ import division, print_function\n","import csv, os, sys\n","import numpy as np\n","import pandas as pd\n","# from SVM import SVM\n","# filepath =\"/content/drive/MyDrive/svm-smo/\"\n","os.chdir(filepath)\n","\n","\n","#  filepath = os.path.dirname(os.path.abspath(__file__))\n","\n","\n","def readData(filename, header=True):\n","\n","    csvfile = pd.read_csv(filename, encoding='utf-8')\n","    # data, header = [], None\n","    # with open(filename, 'rb') as csvfile:\n","    #     spamreader = csv.reader(csvfile, delimiter=',')\n","    #     if header:\n","    #         header = spamreader.next()\n","    #     for row in spamreader:\n","    #         data.append(row)\n","    return (np.array(csvfile.to_numpy()), np.asarray(csvfile.columns.values))\n","\n","def calc_acc(y, y_hat):\n","    idx = np.where(y_hat == 1)\n","    TP = np.sum(y_hat[idx] == y[idx])\n","    idx = np.where(y_hat == -1)\n","    TN = np.sum(y_hat[idx] == y[idx])\n","    return float(TP + TN)/len(y)\n","\n","def main(filename='data/iris-virginica.txt', C=1.0, kernel_type='linear', epsilon=0.001):\n","    # Load data\n","    (data, _) = readData('%s/%s' % (filepath, filename), header=False)\n","    data = data.astype(float)\n","\n","    # Split data\n","    X, y = data[:,0:-1], data[:,-1].astype(int)\n","\n","    # Initialize model\n","    model = SVM()\n","\n","    # Fit model\n","    support_vectors, iterations = model.fit(X, y)\n","\n","    # Support vector count\n","    sv_count = support_vectors.shape[0]\n","\n","    # Make prediction\n","    y_hat = model.predict(X)\n","\n","    # Calculate accuracy\n","    acc = calc_acc(y, y_hat)\n","\n","    print(\"Support vector count: %d\" % (sv_count))\n","    print(\"bias:\\t\\t%.3f\" % (model.b))\n","    print(\"w:\\t\\t\" + str(model.w))\n","    print(\"accuracy:\\t%.3f\" % (acc))\n","    print(\"Converged after %d iterations\" % (iterations))\n","\n","if __name__ == '__main__':\n","    if ('--help' in sys.argv) or ('-h' in sys.argv):\n","        print(\"\")\n","        print(\"Trains a support vector machine.\")\n","        print(\"Usage: %s FILENAME C kernel eps\" % (sys.argv[0]))\n","        print(\"\")\n","        print(\"FILENAME: Relative path of data file.\")\n","        print(\"C:        Value of regularization parameter C.\")\n","        print(\"kernel:   Kernel type to use in training.\")\n","        print(\"          'linear' use linear kernel function.\")\n","        print(\"          'quadratic' use quadratic kernel function.\")\n","        print(\"eps:      Convergence value.\")\n","    else:\n","        kwargs = {}\n","        # if len(sys.argv) > 1:\n","        #     kwargs['filename'] = sys.argv[1]\n","        # if len(sys.argv) > 2:\n","        #     kwargs['C'] = float(sys.argv[2])\n","        # if len(sys.argv) > 3:\n","        #     kwargs['kernel_type'] = sys.argv[3]\n","        # if len(sys.argv) > 4:\n","        #     kwargs['epsilon'] = float(sys.argv[4])\n","        # if len(sys.argv) > 5:\n","        #     sys.exit(\"Not correct arguments provided. Use %s -h for more information\"\n","        #              % (sys.argv[0]))\n","        main(**kwargs)\n"],"metadata":{"id":"zB5rCvQ2ogmH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'%s/%s' % (filepath, filename)"],"metadata":{"id":"_mwj2lr2qWdG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# svc.fit(train_dataset.dataset.data[train_dataset.indices], train_dataset.dataset.targets[train_dataset.indices])"],"metadata":{"id":"EpijMYXesudJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset.dataset.data[train_dataset.indices].shape"],"metadata":{"id":"d51J4br1y945"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset.dataset.targets[train_dataset.indices].astype(int)"],"metadata":{"id":"u9Ir_VC2zASt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iris = pd.read_csv('%s/%s' % (filepath, 'data/iris-virginica.txt'), encoding='utf-8')"],"metadata":{"id":"u1EVlaIIzJcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataset.indices)"],"metadata":{"id":"6PDxBRb6zT5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = (train_dataset.dataset.data[train_dataset.indices[:500]], train_dataset.dataset.targets[train_dataset.indices[:500]].astype(int))\n","print(X.shape,y.shape)"],"metadata":{"id":"ff4QKVAM37iH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, y = (train_dataset.dataset.data[train_dataset.indices[2000:2500]], train_dataset.dataset.targets[train_dataset.indices[2000:2500]].astype(int))\n","\n","model = SVM(max_iter=500)\n","\n","# Fit model\n","support_vectors, iterations = model.fit(X, y)\n","# Support vector count\n","sv_count = support_vectors.shape[0]\n","# Make prediction\n","y_hat = model.predict(X)\n","# Calculate accuracy\n","acc = calc_acc(y, y_hat)\n","\n","print(\"Support vector count: %d\" % (sv_count))\n","print(\"bias:\\t\\t%.3f\" % (model.b))\n","print(\"w:\\t\\t\" + str(model.w))\n","print(\"accuracy:\\t%.3f\" % (acc))\n","print(\"Converged after %d iterations\" % (iterations))\n"],"metadata":{"id":"8nfpZKWEzVQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NBiNaZnU5Oy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc = LinearSVC(max_iter=5000, C=1)"],"metadata":{"id":"h8gAy0c-5PC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc.fit(train_dataset.dataset.data[train_dataset.indices[:1500]], train_dataset.dataset.targets[train_dataset.indices[:1500]])"],"metadata":{"id":"y-ShBCRK5PC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc.score(train_dataset.dataset.data[train_dataset.indices[:1500]], train_dataset.dataset.targets[train_dataset.indices[:1500]])"],"metadata":{"id":"rlSCWt9j5PC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C = 1e-2\n","num_epochs = 100\n","\n","for alpha in ALPHA_GRID:\n","\n","    w = torch.autograd.Variable(torch.randn(processed_data.shape[1]), requires_grad=True)\n","    b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n","    optimizer = scipy.optim.SGD([w, b], lr=2e-1)\n","\n","    \n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.95)\n","\n","    device = 'cpu'\n","    for epoch in tqdm(range(num_epochs)[:2]):\n","        for i, (fields, target, gr) in enumerate(train_data_loader):\n","\n","\n","            fields, target, gr = fields.to(device), target.to(device), gr.to(device)\n","\n","            y = torch.matmul(fields.float(), w) + b\n","            mask = (gr==1)\n","\n","            n1 = y[mask].size()[0]\n","            n = y.size()[0]\n","            w1 = n1 / n\n","            w2 = 1 - w1\n","\n","            loss1 = torch.mean(torch.maximum(torch.zeros(n1), 1 - target[mask]*y[mask]))\n","            loss2 = torch.mean(torch.maximum(torch.zeros(n-n1), 1 - target[~mask]*y[~mask]))\n","            max_loss = torch.maximum(loss1, loss2)\n","\n","            loss = max_loss + 1/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss)) + w2*torch.exp(alpha*(loss2-max_loss))) + C/2*torch.dot(w, w) \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            #scheduler.step()\n","            #w.data -= step_size * w.grad.data # step\n","            #b.data -= step_size * b.grad.data # step\n","\n","            #w.grad.data.zero_()\n","            #b.grad.data.zero_()\n","    filename = dataset_name + '_' + str(alpha) + '_temp' + '.npy'\n","    with open(os.path.join(FILEPATH, filename), 'wb') as f:\n","        np.save(f, torch.concat([w, b]).detach().numpy())"],"metadata":{"id":"8VEMXmpE9LcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["C = 1e-2\n","num_epochs = 100\n","\n","for alpha in ALPHA_GRID:\n","\n","    w = torch.autograd.Variable(torch.randn(processed_data.shape[1]), requires_grad=True)\n","    b = torch.autograd.Variable(torch.rand(1), requires_grad=True)\n","    optimizer = torch.optim.SGD([w, b], lr=2e-1)\n","\n","    \n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.95)\n","\n","    device = 'cpu'\n","    for epoch in tqdm(range(num_epochs)[:2]):\n","        for i, (fields, target, gr) in enumerate(train_data_loader):\n","\n","\n","            fields, target, gr = fields.to(device), target.to(device), gr.to(device)\n","\n","            y = torch.matmul(fields.float(), w) + b\n","            mask = (gr==1)\n","\n","            n1 = y[mask].size()[0]\n","            n = y.size()[0]\n","            w1 = n1 / n\n","            w2 = 1 - w1\n","\n","            loss1 = torch.mean(torch.maximum(torch.zeros(n1), 1 - target[mask]*y[mask]))\n","            loss2 = torch.mean(torch.maximum(torch.zeros(n-n1), 1 - target[~mask]*y[~mask]))\n","            max_loss = torch.maximum(loss1, loss2)\n","\n","            loss = max_loss + 1/alpha * torch.log(w1*torch.exp(alpha*(loss1-max_loss)) + w2*torch.exp(alpha*(loss2-max_loss))) + C/2*torch.dot(w, w) \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            #scheduler.step()\n","            #w.data -= step_size * w.grad.data # step\n","            #b.data -= step_size * b.grad.data # step\n","\n","            #w.grad.data.zero_()\n","            #b.grad.data.zero_()\n","    filename = dataset_name + '_' + str(alpha) + '_temp' + '.npy'\n","    with open(os.path.join(FILEPATH, filename), 'wb') as f:\n","        np.save(f, torch.concat([w, b]).detach().numpy())"],"metadata":{"id":"p_QjlI7b0OGY"},"execution_count":null,"outputs":[]}]}