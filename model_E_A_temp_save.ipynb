{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_E_A_temp_save.ipynb","provenance":[{"file_id":"1wmrmlcZwMgk0tLmb-3rxssy5_xpQ7siK","timestamp":1629739738892}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1lCcKHCM1O0VPV7MxeFKBS-wUgomqknES","authorship_tag":"ABX9TyNAOwm29gSbNSvNKYcPJqWx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6EWYZ7i3I1AW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629812131908,"user_tz":-60,"elapsed":8659,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"a89f55d9-093b-4fbc-cd45-e8741143926b"},"source":["%pip install keras-tuner\n","%pip install keras_self_attention"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.0.3-py3-none-any.whl (96 kB)\n","\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n","Collecting kt-legacy\n","  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.0.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.0.3 kt-legacy-1.0.4\n","Collecting keras_self_attention\n","  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.6.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=07bcb7bad665d83e23d78f7667486ca10f2f822d40ac297e7ec45b1fbfe3af06\n","  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.50.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"56KS77UD34pw","executionInfo":{"status":"ok","timestamp":1629812131909,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"f28102b9-15e0-4a05-af92-f6b356f8d3e9"},"source":["!nvidia-smi -L"],"execution_count":3,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-125baf2d-83de-4de9-f985-b8017135d24a)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iUTGIn5nImcI","executionInfo":{"status":"ok","timestamp":1629812134154,"user_tz":-60,"elapsed":2249,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["import tensorflow as tf\n","import site\n","import pandas as pd\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n","from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n","import os\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mC-h14gTJJeE","executionInfo":{"status":"ok","timestamp":1629812134158,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"1fa9d487-d620-41ec-d8ce-0344bbbb9c36"},"source":["tf.__version__"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.6.0'"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"j64-lcWdJO6G","executionInfo":{"status":"ok","timestamp":1629812134740,"user_tz":-60,"elapsed":587,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","import errno\n","import os\n","from collections import defaultdict\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.summary import create_file_writer\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import itertools\n","import multiprocessing"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNqzrfwimwIG","executionInfo":{"status":"ok","timestamp":1629812135021,"user_tz":-60,"elapsed":285,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def coShuffled_vectors(X, Y):\n","    if tf.shape(X)[0] == tf.shape(Y)[0]:\n","        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n","        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n","        return (tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs))\n","    else:\n","        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n","\n","\n","def getNpArrayFromH5(hf_Data):\n","    X_train = hf_Data['Train_Data']  # Get train set\n","    X_train = np.array(X_train)\n","    Y_train = hf_Data['Label']  # Get train label\n","    Y_train = np.array(Y_train)\n","    return X_train, Y_train\n","\n","# data extraction\n","def getData(is500=True, shuffle=False, ise2e=False, include_secondary=False, validation_split=None, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Train_Data\" if ise2e else \"Fold_10_Train_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Test_Data\" if ise2e else \"Fold_10_Test_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    X_validation = Y_validation = None\n","    if validation_split is not None:\n","        # sklearn split shuffles anyway\n","        X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_split)\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eData(is500=True, shuffle=False, include_secondary=False, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Secondary_Data_1136.h5', 'r') if include_secondary else h5.File(\n","        f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eDataJustSecondary(shuffle=False,isColab=False):\n","    hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_just_Secondary_Data_1000.h5', 'r')\n","    hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_just_Secondary_Data_1000.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_just_Secondary_Data_1000.h5', 'r')\n","    \n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDKkbD1fJVpN","executionInfo":{"status":"ok","timestamp":1629812137757,"user_tz":-60,"elapsed":782,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def plot_history(history):\n","    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n","    for k, v in history.history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        else:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()\n","\n","\n","def get_layer_by_name(layers, name, return_first=True):\n","    matching_named_layers = [l for l in layers if l.name == name]\n","    if not matching_named_layers:\n","        return None\n","    return matching_named_layers[0] if return_first else matching_named_layers\n","\n","\n","def get_combined_features_from_models(\n","        to_combine,\n","        X_train, Y_train,\n","        X_test, Y_test,\n","        reverse_one_hot=False,\n","        normalize_X_func=None):\n","    models = []\n","    models_dict = {}\n","    X_trains_out = []\n","    X_test_out = []\n","    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n","\n","    if reverse_one_hot:\n","        Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train) + 1\n","        Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test) + 1\n","    else:\n","        Y_train_new = Y_train.copy()\n","        Y_test_new = Y_test.copy()\n","\n","    for model_file_name, layer_name, kwargs in to_combine:\n","        model_here = None\n","        if isinstance(model_file_name, tf.keras.models.Model):\n","            model_here = model_file_name\n","            model_file_name = model_here.name\n","        else:\n","            if model_file_name in models_dict.keys():\n","                model_here = models_dict[model_file_name]\n","            else:\n","                model_here = tf.keras.models.load_model(model_file_name,\n","                                                        **kwargs) if kwargs is not None else tf.keras.models.load_model \\\n","                    (model_file_name)\n","\n","        features_model = Model(model_here.input,\n","                               get_layer_by_name(model_here.layers, layer_name).output)\n","        if normalize_X_func is None:\n","            X_trains_out.append(np.array(features_model.predict(X_train), dtype='float64'))\n","            X_test_out.append(np.array(features_model.predict(X_test), dtype='float64'))\n","        else:\n","            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train)), dtype='float64'))\n","            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test)), dtype='float64'))\n","        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n","        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n","        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n","        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n","        models.append(((model_file_name, layer_name), (model_here, features_model)))\n","        models_dict[model_file_name] = model_here\n","\n","    X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n","    X_test_new = np.concatenate(tuple(X_test_out), axis=1)\n","\n","    data_train = (X_train_new, Y_train_new)\n","    data_test = (X_test_new, Y_test_new)\n","\n","    return models, data_train, data_test, XY_dict\n","\n","\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","\n","def source_model(model_func, model_name, input_shape):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    return m\n","\n","\n","def compile_and_fit_model_with_tb(model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_every_epoch=True,\n","                                  save_final=False,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    tb_callback = TensorBoard(log_dir=f'{m.name}_logs', histogram_freq=kwargs.pop(\"histogram_freq\", 1))\n","    if save_every_epoch:\n","        tb_callback.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=[tb_callback], verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","    # m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n","\n","\n","def compile_and_fit_model(model_func,\n","                          model_name,\n","                          input_shape,\n","                          X_train,\n","                          Y_train,\n","                          save_every_epoch=True,\n","                          save_final=False,\n","                          **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_every_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","\n","\n","def compile_model_and_fit_with_custom_loop(model_func,\n","                                           model_name,\n","                                           input_shape,\n","                                           X_train,\n","                                           Y_train,\n","                                           **kwargs):\n","    make_dir_if_not_exist(model_name)\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    train_writer = create_file_writer(f'{m.name}_logs/train/')\n","    test_writer = create_file_writer(f'{m.name}_logs/test/')\n","    train_step = test_step = 0\n","\n","    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","    optimizer = tf.keras.optimizers.Adam()\n","    num_epochs = kwargs.get(\"epochs\", 10)\n","\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    BATCH_SIZE = kwargs.get(\"batch_size\", 32)\n","    X_test, Y_test = kwargs.get(\"validation_data\", (None, None))\n","    if X_test is None:\n","        raise ValueError(\"Missing X validation data\")\n","    if Y_test is None:\n","        raise ValueError(\"Missing Y validation data\")\n","\n","    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n","    train_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    train_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n","    test_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    test_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","\n","    for epoch in range(num_epochs):\n","        # Iterate through training set\n","        for batch_idx, (x, y) in enumerate(train_dataset_tf):\n","            with tf.GradientTape() as tape:\n","                y_pred = m(x, training=True)\n","                loss = loss_fn(y, y_pred)\n","\n","            gradients = tape.gradient(loss, m.trainable_weights)\n","            optimizer.apply_gradients(zip(gradients, m.trainable_weights))\n","            acc_metric.update_state(y, y_pred)\n","\n","            with train_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=train_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=train_step,\n","                )\n","                train_step += 1\n","        # Reset accuracy in between epochs (and for testing and test)\n","        acc_metric.reset_states()\n","        # Iterate through test set\n","        for batch_idx, (x, y) in enumerate(test_dataset_tf):\n","            y_pred = m(x, training=False)\n","            loss = loss_fn(y, y_pred)\n","            acc_metric.update_state(y, y_pred)\n","            with test_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=test_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=test_step,\n","                )\n","                test_step += 1\n","\n","        acc_metric.reset_states()  # Reset accuracy in between epochs (and for testing and test)\n","\n","    return m\n","\n","\n","def reinitialize_weights(model):\n","    for ix, layer in enumerate(model.layers):\n","        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n","            weight_initializer = model.layers[ix].kernel_initializer\n","            bias_initializer = model.layers[ix].bias_initializer\n","\n","            old_weights, old_biases = model.layers[ix].get_weights()\n","\n","            model.layers[ix].set_weights([\n","                weight_initializer(shape=old_weights.shape),\n","                bias_initializer(shape=len(old_biases))])\n","    return model\n","\n","\n","def reverse_tensor(X):\n","    return tf.gather(X, tf.reverse(tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32), (0,)))\n","\n","\n","def run_mirrored_strategy(model_func, base_batch_size, nepochs, x_train, y_train, x_test, y_test, **kwargs):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        model = model_func()\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adam(),\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","            metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n","        )\n","    batch_size_mirr_strat = base_batch_size * strategy.num_replicas_in_sync\n","    history = model.fit(x_train, y_train, epochs=nepochs, batch_size=batch_size_mirr_strat,\n","                        validation_data=(x_test, y_test),\n","                        **kwargs)\n","    return model, history\n","\n","\n","def sparse_setdiff(a1, a2):\n","    a1a = a1.reshape(a1.shape[0], -1)\n","    a2a = a2.reshape(a2.shape[0], -1)\n","    spa2a = [np.where(x)[0].tolist() for x in a2a]\n","    spa1a = [np.where(x)[0].tolist() for x in a1a]\n","    idxs_to_keep = []\n","    for idx, sample in enumerate(spa1a):\n","        try:\n","            spa2a.index(sample)\n","        except ValueError:\n","            # not in list\n","            idxs_to_keep.append(idx)\n","    return a1[idxs_to_keep], idxs_to_keep\n","\n","\n","def unpacking_apply_along_axis(all_args):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but with arguments in a tuple\n","    instead.\n","\n","    This function is useful with multiprocessing.Pool().map(): (1)\n","    map() only handles functions that take a single argument, and (2)\n","    this function can generally be imported from a module, as required\n","    by map().\n","    \"\"\"\n","    (func1d, axis, arr, args, kwargs) = all_args\n","    # return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n","\n","\n","def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but takes advantage of multiple\n","    cores.\n","    \"\"\"\n","    # Effective axis where apply_along_axis() will be applied by each\n","    # worker (any non-zero axis number would work, so as to allow the use\n","    # of `np.array_split()`, which is only done on axis 0):\n","    effective_axis = 1 if axis == 0 else axis\n","    if effective_axis != axis:\n","        arr = arr.swapaxes(axis, effective_axis)\n","\n","    # Chunks for the mapping (only a few chunks):\n","    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n","              for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n","\n","    pool = multiprocessing.Pool()\n","    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n","    # Freeing the workers:\n","    pool.close()\n","    pool.join()\n","\n","    return np.concatenate(individual_results)\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"njaKwGybk9P7","executionInfo":{"status":"ok","timestamp":1629812156428,"user_tz":-60,"elapsed":301,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def model_A_CNN_256(model_name, inshape, num_classes = 13):\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same' ,input_shape=inshape))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Flatten())\n","\n","    model.add(tf.keras.layers.Dense(128))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(64))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n","    model._name = model_name\n","\n","    return model\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbS3EHxGl8d2","executionInfo":{"status":"ok","timestamp":1629812158285,"user_tz":-60,"elapsed":174,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def model_E_RNN(model_name, input_shape = (1000, 8,),num_classes = 13):\n","\n","    # RNN part\n","    inputs = Input(shape=input_shape)\n","    lstm_one = Bidirectional \\\n","        (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(inputs)\n","    lstm_two = Bidirectional \\\n","        (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(lstm_one)\n","    attention = SeqWeightedAttention()(lstm_two)\n","    attention = Flatten()(attention)\n","    rnnoutput = Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(attention)\n","    rnnoutput = BatchNormalization()(rnnoutput)\n","    rnnoutput = GaussianNoise(1)(rnnoutput)\n","    rnnoutput = Dropout(0.4)(rnnoutput)\n","\n","    # Dense Feed-forward\n","    dense_one = Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros')(rnnoutput)\n","    dense_one = LeakyReLU()(dense_one)\n","    dense_one = Dropout(0.5)(dense_one)\n","    dense_one = BatchNormalization()(dense_one)\n","    dense_two = Dense(64, kernel_initializer='RandomNormal', bias_initializer='zeros')(dense_one)\n","    dense_two = LeakyReLU()(dense_two)\n","    dense_two = Dropout(0.4)(dense_two)\n","\n","    # Output\n","    output = Dense(num_classes, activation='softmax')(dense_two)\n","    model = Model([inputs], output, name = model_name)\n","    return model\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"c34W707uoTwe","executionInfo":{"status":"ok","timestamp":1629812159819,"user_tz":-60,"elapsed":179,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def model_combination(model_name, input_shape,num_classes = 13):\n","    model = Sequential([\n","        tf.keras.Input(shape=input_shape),\n","        BatchNormalization(),\n","        Dense(256, kernel_initializer='RandomNormal', bias_initializer='zeros'),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-3)),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(32, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-2)),\n","        LeakyReLU(),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='softmax')\n","    ], name=model_name)\n","    return model"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5sn5eSWrnQS","executionInfo":{"status":"ok","timestamp":1629812162966,"user_tz":-60,"elapsed":170,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def compile_and_fit_model_basic(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor='val_accuracy',\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(patience=patience_count))\n","\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","\n","def compile_and_fit_model_basic_noVal(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor='accuracy',\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(patience=patience_count))\n","\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ughZslAbEmHm","executionInfo":{"status":"ok","timestamp":1629812169168,"user_tz":-60,"elapsed":172,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"97c8307c-b7da-4a26-9027-37d7fb782ddd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-WZhP7P9LrOD","executionInfo":{"status":"ok","timestamp":1629812178676,"user_tz":-60,"elapsed":6422,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["# 'new' data \n","X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData(is500=False,\n","                                                                                                    include_secondary=False,\n","                                                                                                    isColab=True)\n","X_train_1000e_w2nd, Y_train_1000e_w2nd, X_test_1000e_w2nd, Y_test_1000e_w2nd, X_val_1000e_w2nd, Y_val_1000e_w2nd = getE2eData(is500=False, include_secondary=True, isColab=True)\n","X_train_1000e_j2nd, Y_train_1000e_j2nd, X_test_1000e_j2nd, Y_test_1000e_j2nd, X_val_1000e_j2nd, Y_val_1000e_j2nd = getE2eDataJustSecondary(isColab=True)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-uBWRfUCkBL","executionInfo":{"status":"ok","timestamp":1629812179504,"user_tz":-60,"elapsed":274,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["# merge into a new train:\n","X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n","Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )    \n","\n","X_new_train_j2nd = np.concatenate( (X_train_1000e_j2nd, X_val_1000e_j2nd), axis=0 )\n","Y_new_train_j2nd = np.concatenate( (Y_train_1000e_j2nd, Y_val_1000e_j2nd), axis=0 )    \n","\n","X_new_train_w2nd = np.concatenate( (X_train_1000e_w2nd, X_val_1000e_w2nd), axis=0 )\n","Y_new_train_w2nd = np.concatenate( (Y_train_1000e_w2nd, Y_val_1000e_w2nd), axis=0 )  "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oqF7iuACoVR","executionInfo":{"status":"ok","timestamp":1629812181383,"user_tz":-60,"elapsed":176,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"0f118eb4-3819-47c8-ea24-8e1c8440b6df"},"source":["X_new_train[0].shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 8)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"NIRNv1gdGjcY","executionInfo":{"status":"ok","timestamp":1629812182913,"user_tz":-60,"elapsed":170,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n","from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","import site\n","import pandas as pd\n","import os\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHzA2yddFcwk","outputId":"f480a826-81b3-4ae5-8c62-32f132eb672b"},"source":["# Model E\n","rnn_E, history_rnn_E = compile_and_fit_model_basic(model_E_RNN,\n","                                                  f\"rnn_E\",\n","                                                  X_new_train[0].shape,\n","                                                  X_new_train,\n","                                                  Y_new_train,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=50,\n","                                                  batch_size=128,\n","                                                  epochs=250,\n","                                                  class_weight=None,\n","                                                   validation_data=(X_val_1000e, Y_val_1000e)\n","                                                   )\n","    \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Epoch 1/250\n","61/61 - 921s - loss: 2.3759 - accuracy: 0.1797 - val_loss: 2.5145 - val_accuracy: 0.1050\n","INFO:tensorflow:Assets written to: rnn_E_model_001_0.105/assets\n","Epoch 2/250\n","61/61 - 903s - loss: 2.1414 - accuracy: 0.2433 - val_loss: 2.5124 - val_accuracy: 0.1470\n","INFO:tensorflow:Assets written to: rnn_E_model_002_0.147/assets\n","Epoch 3/250\n","61/61 - 883s - loss: 2.0054 - accuracy: 0.2915 - val_loss: 2.2389 - val_accuracy: 0.2124\n","INFO:tensorflow:Assets written to: rnn_E_model_003_0.212/assets\n","Epoch 4/250\n","61/61 - 885s - loss: 1.9060 - accuracy: 0.3244 - val_loss: 2.1495 - val_accuracy: 0.2427\n","INFO:tensorflow:Assets written to: rnn_E_model_004_0.243/assets\n","Epoch 5/250\n","61/61 - 892s - loss: 1.8142 - accuracy: 0.3588 - val_loss: 2.0350 - val_accuracy: 0.2940\n","INFO:tensorflow:Assets written to: rnn_E_model_005_0.294/assets\n","Epoch 6/250\n","61/61 - 903s - loss: 1.7364 - accuracy: 0.3934 - val_loss: 5.9732 - val_accuracy: 0.1354\n","Epoch 7/250\n","61/61 - 910s - loss: 1.6211 - accuracy: 0.4342 - val_loss: 2.6263 - val_accuracy: 0.2660\n","Epoch 8/250\n","61/61 - 904s - loss: 1.5065 - accuracy: 0.4760 - val_loss: 2.5930 - val_accuracy: 0.3302\n","INFO:tensorflow:Assets written to: rnn_E_model_008_0.330/assets\n","Epoch 9/250\n","61/61 - 910s - loss: 1.4121 - accuracy: 0.5248 - val_loss: 1.7627 - val_accuracy: 0.4364\n","INFO:tensorflow:Assets written to: rnn_E_model_009_0.436/assets\n","Epoch 10/250\n","61/61 - 913s - loss: 1.3236 - accuracy: 0.5487 - val_loss: 1.0492 - val_accuracy: 0.6184\n","INFO:tensorflow:Assets written to: rnn_E_model_010_0.618/assets\n","Epoch 11/250\n","61/61 - 890s - loss: 1.2472 - accuracy: 0.5746 - val_loss: 1.0321 - val_accuracy: 0.6161\n","Epoch 12/250\n","61/61 - 880s - loss: 1.1803 - accuracy: 0.5901 - val_loss: 1.1223 - val_accuracy: 0.6278\n","INFO:tensorflow:Assets written to: rnn_E_model_012_0.628/assets\n","Epoch 13/250\n","61/61 - 883s - loss: 1.1121 - accuracy: 0.6197 - val_loss: 0.9405 - val_accuracy: 0.6873\n","INFO:tensorflow:Assets written to: rnn_E_model_013_0.687/assets\n","Epoch 14/250\n","61/61 - 889s - loss: 1.0448 - accuracy: 0.6460 - val_loss: 0.7434 - val_accuracy: 0.7281\n","INFO:tensorflow:Assets written to: rnn_E_model_014_0.728/assets\n","Epoch 15/250\n","61/61 - 901s - loss: 1.0160 - accuracy: 0.6498 - val_loss: 0.7213 - val_accuracy: 0.7293\n","INFO:tensorflow:Assets written to: rnn_E_model_015_0.729/assets\n","Epoch 16/250\n","61/61 - 926s - loss: 0.9512 - accuracy: 0.6714 - val_loss: 0.7122 - val_accuracy: 0.7246\n","Epoch 17/250\n","61/61 - 939s - loss: 0.9143 - accuracy: 0.6907 - val_loss: 0.5903 - val_accuracy: 0.7795\n","INFO:tensorflow:Assets written to: rnn_E_model_017_0.779/assets\n","Epoch 18/250\n","61/61 - 934s - loss: 0.8889 - accuracy: 0.6979 - val_loss: 0.5830 - val_accuracy: 0.7888\n","INFO:tensorflow:Assets written to: rnn_E_model_018_0.789/assets\n","Epoch 19/250\n","61/61 - 936s - loss: 0.8507 - accuracy: 0.7078 - val_loss: 0.6270 - val_accuracy: 0.7655\n","Epoch 20/250\n","61/61 - 930s - loss: 0.8339 - accuracy: 0.7187 - val_loss: 0.5495 - val_accuracy: 0.7911\n","INFO:tensorflow:Assets written to: rnn_E_model_020_0.791/assets\n","Epoch 21/250\n","61/61 - 924s - loss: 0.8041 - accuracy: 0.7240 - val_loss: 0.6572 - val_accuracy: 0.7655\n","Epoch 22/250\n","61/61 - 904s - loss: 0.7593 - accuracy: 0.7437 - val_loss: 0.5990 - val_accuracy: 0.7818\n","Epoch 23/250\n","61/61 - 898s - loss: 0.7321 - accuracy: 0.7483 - val_loss: 0.5364 - val_accuracy: 0.8133\n","INFO:tensorflow:Assets written to: rnn_E_model_023_0.813/assets\n","Epoch 24/250\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VUsaFm6uqGC-"},"source":["import json\n","# Get the dictionary containing each metric and the loss for each epoch\n","history_rnn_E_dict = history_rnn_E.history\n","# Save it under the form of a json file\n","json.dump(history_rnn_E_dict, open('/content/drive/MyDrive/ncRNA/data/history/history_rnn_E_dict_rr.json', 'w'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ee9OuBB7O-r-"},"source":["# Model A\n","cnn_256, history_cnn_256 = compile_and_fit_model_basic(model_A_CNN_256,\n","                                                  f\"cnn_256\",\n","                                                  X_new_train[0].shape,\n","                                                  X_new_train,\n","                                                  Y_new_train,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=100,\n","                                                  batch_size=1024,\n","                                                  epochs=500,\n","                                                  class_weight=None,\n","                                                  validation_data=(X_val_1000e, Y_val_1000e))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFSSn2R6QZab"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hIGS_7kcQae3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oveuq70EIxh2"},"source":[""],"execution_count":null,"outputs":[]}]}