{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"orderbook_cj_expand.ipynb","provenance":[],"collapsed_sections":["BErSeefeQwQi","bvy0WvxDGCxk","GXVvC9OfGLnS","ux5exyOUVj-b","ocP3T6vXXxbW"],"machine_shape":"hm","background_execution":"on","authorship_tag":"ABX9TyMPREVfrv9njycQzY2Vu85x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"BErSeefeQwQi"},"source":["## Setup packages "]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22209,"status":"ok","timestamp":1659459646915,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"L8DGCgVxR2AB","outputId":"56c3c9f4-f49b-4c98-d39a-d3e9c6270bf1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13541,"status":"ok","timestamp":1659459660451,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"3xIx5C6UQn4u","outputId":"5ca5ddaa-ae4a-40cc-c46e-728514a9afbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting progressbar\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","Building wheels for collected packages: progressbar\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=f4e9bd2e6330a1b6cf2809102a404117723433564975fa9f3652b9272e500213\n","  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n","Successfully built progressbar\n","Installing collected packages: progressbar\n","Successfully installed progressbar-2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.7.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.1.1)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.3.1\n","time: 163 µs (started: 2022-08-02 17:01:00 +00:00)\n"]}],"source":["%pip install progressbar\n","%pip install plotnine\n","%pip install torch\n","%pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1967,"status":"ok","timestamp":1659459662414,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"EfIU_eNp3Zio","outputId":"d1e91d40-4776-4a59-d83a-58104dff7b32"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.77 s (started: 2022-08-02 17:01:00 +00:00)\n"]}],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2239,"status":"ok","timestamp":1659459664648,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ZmUjYbArAuQT","outputId":"06eb6d93-71e5-4130-ea29-c94c7cab9c47"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.53 s (started: 2022-08-02 17:01:02 +00:00)\n"]}],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659459664649,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"BI4p7ZKb0Qz2","outputId":"d07784c6-be67-4311-aec6-54ac60484933"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 785 µs (started: 2022-08-02 17:01:04 +00:00)\n"]}],"source":["paper_name = \"dgm_cj_expand\""]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1659459665346,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"433z6V3T2rB2","outputId":"8f11e8b6-c8ca-4ffe-ba9c-4cf68ac10f28"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 582 ms (started: 2022-08-02 17:01:04 +00:00)\n"]}],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n","make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659459665347,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"uat0pG8aR3Rh","outputId":"b82dc8af-ae19-43c3-88aa-10219bde6101"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 9.37 ms (started: 2022-08-02 17:01:05 +00:00)\n"]}],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2899,"status":"ok","timestamp":1659459668243,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"KpFjo3MkLus9","outputId":"3ee0a1fb-d977-467c-b7e8-9ad008d0a38f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.31 s (started: 2022-08-02 17:01:05 +00:00)\n"]}],"source":["import torch \n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from scipy.stats import norm\n","from matplotlib import cm\n","import pdb\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1236,"status":"ok","timestamp":1659459669472,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"CbfN42gpGZhC","outputId":"a60873bf-3e70-410a-bdff-cc7145609958"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.84 s (started: 2022-08-02 17:01:07 +00:00)\n"]}],"source":["import plotly.graph_objects as go\n","import plotly.express as px\n","from pprint import pprint as pp"]},{"cell_type":"markdown","source":["## Shared Functions and Input Parameters across all implementations"],"metadata":{"id":"ozH2pdHlGA3U"}},{"cell_type":"markdown","metadata":{"id":"bvy0WvxDGCxk"},"source":["### Shared functions across models"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1659459669473,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"sLsA5AvqpMM7","outputId":"36f21f78-a9df-45ad-907e-46502ff83db4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.86 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}],"source":["import torch\n","\n","def to_cpu_detach(x):\n","  if isinstance(x, list):\n","    return [ y.detach().cpu().item() for y in x ]\n","  else:\n","    return x.detach().cpu().item()"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"KWqq1hGijWvw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659459669934,"user_tz":-60,"elapsed":467,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"a46727b4-4884-4e44-9445-aa3cc5847ae2"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 34.1 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}],"source":["def save_model_final(training, eqObject, lr, eqLossFn, sample_method, batch_size): \n","\n","  model_id_base_str =  f\"{eqObject.__class__.__name__}_{datetime.datetime.now():%Y%m%d%H%M%S}_{batch_size}\"\n","  model_id_base_str = model_id_base_str + f\"_{eqLossFn}_{sample_method}_{str(lr).replace('.','p')}\"  \n","  model_id_base_str = model_id_base_str + f\"_U{eqObject.u_net.neurons}_U{eqObject.u_net.depth}_P{eqObject.pi_net.neurons}_P{eqObject.pi_net.depth}\"\n","  \n","  torch.save(eqObject.u_net.state_dict(),  f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_base_str}_U\")\n","  torch.save(eqObject.pi_net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_base_str}_P\")\n","\n","  try:\n","      train_losses = getattr(training,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/trainlosses_{model_id_base_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(training,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/validationlosses_{model_id_base_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(training.__class__.__name__, \"validation_losses\"))\n","\n","  # import numpy as np\n","\n","  # training_dict = dict({ \"BATCH_SIZE\" : training.BATCH_SIZE,\n","  #                        \"use_early_stop\" : training.use_early_stop,\n","  #                       \"early_stop_patience\" : training.early_stop_patience,\n","  #                       \"early_stop_delta\" : training.early_stop_delta,\n","  #                       \"monitored_loss_type\" : training.monitored_loss_type,\n","  #                       \"use_early_stop\" : training.use_early_stop,\n","  #                       \"stop_epoch\" : training.stop_epoch            })\n","  # equation_dict = dict({ \n","  #                       \"xbreaks\" : eqObject.xbreaks,\n","  #                       \"tbreaks\" : eqObject.tbreaks,\n","  #                       \"MAX_X\" : eqObject.MAX_X,\n","  #                       \"T\" : eqObject.T,\n","  #                       \"MAX_MU\" : eqObject.MAX_MU,\n","  #                       \"MAX_SIGMA\" : eqObject.MAX_SIGMA,\n","  #                       \"pi_net_epoch\" : eqObject.pi_net_epoch,\n","  #                       \"pi_net_lr\" : eqObject.pi_net_lr,\n","  #                       # \"loss_multiply\" : eqObject.loss_multiply,\n","  #                       \"epoch_of_u\" : eqObject.epoch_of_u,\n","  #                       \"adapt_pi_epochs\" : eqObject.adapt_pi_epochs,\n","  #                       \"start_adapt_epochs\" : eqObject.start_adapt_epochs,\n","  #                       \"was_loss_beaten\" : eqObject.was_loss_beaten  })\n","\n","  # np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/trainingDict_{model_id_base_str}.npy\", training_dict)\n","  # np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/equationDict_{model_id_base_str}.npy\", equation_dict) \n"]},{"cell_type":"markdown","source":["### Shared input Parameters"],"metadata":{"id":"GXVvC9OfGLnS"}},{"cell_type":"code","source":["# drift of underlying\n","mu = 0.0\n","# volatility of underlying BM\n","sigma = 0.01\n","# inventory penalty\n","phi = 1E-5  # function of sigma on a portfolio?!\n","# arrival rate/s of market orders\n","lambda_ask = 1.0\n","lambda_bid = 1.0\n","lambda_up = lambda_ask\n","lambda_down = lambda_bid\n","# exponential likelihood or order hit as function of depth\n","kappa_ask = 100.0\n","kappa_bid = 100.0\n","kappa_up = kappa_ask\n","kappa_down = kappa_bid\n","kappa = (kappa_ask+kappa_bid)/2.0\n","# fee for taking liquidity\n","alpha = 1E-4\n","# time-position (domain on which to solve PDE)\n","# Time parameters\n","t_low = 0 + 1e-10  # time lower bound\n","T = 30.0\n","# max postions\n","qbar_long = 3\n","qbar_short = -3\n","q_up = qbar_long\n","q_down = qbar_short\n","q_high = qbar_long\n","q_low = qbar_short"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k3KlxnuWETYP","executionInfo":{"status":"ok","timestamp":1659459669935,"user_tz":-60,"elapsed":11,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"a96bd602-c15a-4fdd-f442-fb5a22874213"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 11.7 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"markdown","source":["## Architectures"],"metadata":{"id":"87XH4c-xVgqM"}},{"cell_type":"markdown","source":["### Terminal Losses & Rounding Ticks"],"metadata":{"id":"ux5exyOUVj-b"}},{"cell_type":"code","source":["def terminalV(q, alpha):\n","    return -alpha * q ** 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLSpaInbVjC1","executionInfo":{"status":"ok","timestamp":1659459669935,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"8c528ab2-85cc-4a89-e6d6-0b1ac40a19fc"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 657 µs (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"code","source":["def nearest_nth(x, fraction_of_1 = 1.0/0.25):\n","    return round(x*fraction_of_1)/fraction_of_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmDmd3kWMRYa","executionInfo":{"status":"ok","timestamp":1659459669935,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"ad32a97a-8917-4ae2-bd64-bbd0aa79de2f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 923 µs (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"markdown","source":["### NN Architectures\n","\n","[implement from github for the ResNet](https://github.com/Plemeur/DGM/blob/master/first_net.py)\n","\n","[Optional CrossProd layer also from github](https://johaupt.github.io/blog/cross_layer_network.html)"],"metadata":{"id":"ocP3T6vXXxbW"}},{"cell_type":"code","source":["## Basic DNN structure\n","class BasicDNNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh, init_func = torch.nn.init.xavier_uniform_):\n","        super(BasicDNNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        self.Input = 5 + 1  # wealth, time, mu, r, sigma, pi\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        init_func(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            init_func(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        init_func(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        h = self.act(self.fc_input(x))\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out "],"metadata":{"id":"R5-nGuBuXJL-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659459669935,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"61f8cea9-3bd0-47e3-a947-190c8b56ea5c"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 11.6 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"code","source":["## Crossprod layer\n","\n","class Cross(nn.Module):\n","    def __init__(self, input_features, \n","                 batch_normalize=False,\n","                 kaiming_he_init = False):\n","        super().__init__()\n","        self.input_features = input_features\n","\n","        self.batch_normalize = batch_normalize\n","        if self.batch_normalize == True:\n","          self.batch_norm = torch.nn.BatchNorm1d(input_features)\n","\n","        self.weights = nn.Parameter(torch.Tensor(input_features))\n","        # Kaiming/He initialization with a=0\n","        if kaiming_he_init:\n","          nn.init.normal_(self.weights, mean=0, std=math.sqrt(2/input_features))\n","        else:\n","          nn.init.constant_(self.weights, 1.)\n","        \n","        self.bias = nn.Parameter(torch.Tensor(input_features))\n","        nn.init.constant_(self.bias, 0.)\n","        \n","    def forward(self, x0, x):\n","        x0xl = torch.bmm(x0.unsqueeze(-1), x.unsqueeze(-2))\n","        if self.batch_normalize == True:\n","          return self.batch_norm(torch.tensordot(x0xl, self.weights, [[-1],[0]]) + self.bias + x)\n","        return torch.tensordot(x0xl, self.weights, [[-1],[0]]) + self.bias + x\n","    \n","    # Define some output to give when layer \n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}'.format(\n","            self.input_features, self.input_features\n","        )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgAeucd7X05t","executionInfo":{"status":"ok","timestamp":1659459669935,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"bd2b445e-5a2f-4ca0-87c1-2eacb95f3206"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 13.1 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"code","source":["from pprint import pprint as pp\n","cl = Cross(10, batch_normalize=True)\n","input = torch.tensor([ float(x) for x in range(10)]).reshape(-1,1)\n","pp(type(input))\n","pp(input.shape)\n","pp(cl(input, input).shape)\n","pp(cl(input, input).reshape(-1,1).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qegwynzyZwSL","executionInfo":{"status":"ok","timestamp":1659459669936,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"c23370fa-b51a-49cb-d136-8a0552f0c9c7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","torch.Size([10, 1])\n","torch.Size([10, 10])\n","torch.Size([100, 1])\n","time: 127 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"code","source":["## Setting up ResNet like layers\n","\n","class LinearWithXavier(nn.Module):\n","    \"\"\" Copy of linear module from Pytorch, modified to have a Xavier init,\n","        TODO : figure out what to do with the bias\"\"\"\n","    def __init__(self, in_features, out_features, bias=True, batch_normalize=True):\n","        super(LinearWithXavier, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.batch_normalize = batch_normalize\n","        \n","        if self.batch_normalize == True:\n","          self.batch_norm = torch.nn.BatchNorm1d(out_features)\n","        \n","        if bias:\n","            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","    \n","    def reset_parameters(self):\n","        torch.nn.init.xavier_uniform_(self.weight)\n","        if self.bias is not None:\n","            torch.nn.init.uniform_(self.bias, -1, 1) #boundary matter?\n","    \n","    def forward(self, input):\n","        if self.batch_normalize == True:\n","          return self.batch_norm(torch.nn.functional.linear(input, self.weight, self.bias))\n","        return torch.nn.functional.linear(input, self.weight, self.bias)\n","    \n","    def extra_repr(self):\n","        return 'in_features={}, out_features={}, bias={}'.format(\n","            self.in_features, self.out_features, self.bias is not None\n","        )\n","\n","\n","class DGM_layer(nn.Module):\n","    \"\"\" See readme for paper source\"\"\"\n","    def __init__(self, in_features, out_feature, residual=False, batch_normalize=False):\n","        super(DGM_layer, self).__init__()\n","        self.residual = residual\n","\n","        self.Z = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UZ = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.G = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UG = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.R = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UR = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","        self.H = LinearWithXavier(out_feature, out_feature, batch_normalize=batch_normalize)\n","        self.UH = LinearWithXavier(in_features, out_feature, bias=False, batch_normalize=batch_normalize)\n","\n","    def forward(self, x, s):\n","        z = torch.tanh(self.UZ(x) + self.Z(s))\n","        g = torch.tanh(self.UG(x) + self.G(s))\n","        r = torch.tanh(self.UR(x) + self.R(s))\n","        h = torch.tanh(self.UH(x) + self.H(s * r))\n","        return (1 - g) * h + z * s\n","\n","\n","class ResNetBasedNN(nn.Module):\n","\n","    def __init__(self, in_size, out_size, neurons, depth, \n","                 is_softmax_out = False,\n","                 cross_first = False,\n","                 activation_func = torch.tanh):\n","        super(ResNetBasedNN, self).__init__()\n","        self.neurons=neurons\n","        self.depth=depth\n","        self.activation_func = activation_func\n","        self.dim = in_size\n","\n","        self.cross_layer = Cross(in_size) if cross_first else None\n","        self.input_layer = LinearWithXavier(in_size*in_size, neurons) if cross_first else LinearWithXavier(in_size, neurons)\n","        self.middle_layer = nn.ModuleList([DGM_layer(in_size, neurons) for i in range(depth)])\n","        # self.middle_layer_2 = nn.ModuleList([DGM_layer(in_size, neurons, batch_normalize=False) for i in range(2)])\n","        self.final_layer = LinearWithXavier(neurons, out_size, batch_normalize=False) if not is_softmax_out else torch.nn.Softmax(neurons, out_size)\n","\n","    def forward(self, X):\n","        s = None\n","        \n","        if self.cross_layer:\n","          s = self.activation_func(self.input_layer(self.cross_layer(X,X)))\n","        else:\n","          s = self.activation_func(self.input_layer(X))   \n","\n","        for i, layer in enumerate(self.middle_layer):\n","            s = self.activation_func(layer(X, s))\n","        \n","        # s = torch.nn.functional.gelu(self.input_layer(X))\n","        # s = torch.nn.functional.elu(layer(X, s))\n","\n","        return self.final_layer(s)\n","        # return torch.pow(self.final_layer(s), 1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Veh4439cYAT3","executionInfo":{"status":"ok","timestamp":1659459669936,"user_tz":-60,"elapsed":7,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"f5fb2b4a-9aa8-42eb-b936-4f86234ddaa7"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 97.9 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"hp4BG1ewKF6o"},"source":["### Equation Classes"]},{"cell_type":"code","source":["pp(torch.round(torch.tensor([-1.4, -1.45, -1.95, 3.32, 3.54, 3.69, 3.78])).int())\n","pp(torch.tensor([-1.4, -1.45, -1.95, 3.32, 3.54, 3.69, 3.78]).int())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KA1DE9vjufkV","executionInfo":{"status":"ok","timestamp":1659459669937,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"2650e401-152c-4bbf-c20d-15067f5eed0d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-1, -1, -2,  3,  4,  4,  4], dtype=torch.int32)\n","tensor([-1, -1, -1,  3,  3,  3,  3], dtype=torch.int32)\n","time: 21.4 ms (started: 2022-08-02 17:01:09 +00:00)\n"]}]},{"cell_type":"code","execution_count":21,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/","height":133},"executionInfo":{"elapsed":1253,"status":"error","timestamp":1659459671183,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"LBMZYQSPaXKy","outputId":"d9919fc5-0633-4092-e5b0-08114539fb9f"},"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-1d5015b98c00>\"\u001b[0;36m, line \u001b[0;32m266\u001b[0m\n\u001b[0;31m    [ (i+1)*self.S_TICK for i in range(self.NUM_DELTAS) ]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}],"source":["# https://www.abebooks.co.uk/products/isbn/9781107091146\n","\n","import math\n","from functools import partial\n","\n","class CJEquation10p4():\n","    \n","    def __init__(self , \n","                 u_net, \n","                 stock_start = 1.0, \n","                 tick_size = 0.0025, \n","                 max_q = 10.0, \n","                 num_of_deltas = 3,\n","                 term_utility_function = terminalV):\n","\n","        self.u_net = u_net\n","        \n","        self.pi_net_epoch = 1\n","        self.epoch_of_u = None\n","        self.adapt_pi_epochs = False\n","        # self.start_adapt_epochs = [ [500, 1000, 2000, 5000, 10000, 15000 ], [4, 8, 10 ,20, 40, 60] ]\n","        self.start_adapt_epochs = [ [15000], [60] ]\n","        self.was_loss_beaten = False\n","\n","        # self.wgamma = 0.0001\n","        # self.term_utility_func = term_utility_function\n","        \n","        self.loss_multiply = 1.0\n","\n","        self.S_MULT = 1.6  # worst case is 60% loss, best case is 60% profit\n","        self.PHI_MIN = 1e-6    # sample this on a reverse log scale perhaps...\n","        self.PHI_MAX = 0.1\n","        self.LAMBDA_MIN = 0.01 # sample this on a reverse log scale perhaps...\n","        self.LAMBDA_MAX = 10.0\n","        self.KAPPA_MIN = 1.0  # sample this on a reverse log scale perhaps...\n","        self.KAPPA_MAX = 1000.0\n","        self.ALPHA_MIN = 1e-6  # sample this on a reverse log scale perhaps...\n","        self.ALPHA_MAX = 1.0\n","        self.T_MIN = 0 + 1e-10\n","        self.T_MAX = 1.0\n","        self.QBAR_ULT = max_q\n","        self.NUM_DELTAS = num_of_deltas\n","        self.S_START = stock_start\n","        self.S_TICK = tick_size  # 40 ticks is 10% of the stock...\n","        self.X_MIN_MULT = -max_q # assuming your min cash position is your worst position x the stock's value...\n","        self.X_MAX_MULT = max_q # assuming your max cash position is your worst position x the stock's value...\n","\n","        self.SIGMA_FIXED = 0.2\n","        self.stock_round = partial(nearest_nth, fraction_of_1 = 1.0/self.S_TICK)\n","\n","\n","    def g(self,x):\n","        # Time, Wealth, Mu, R, Sigma\n","        return self.term_utility_func(x[:,1].reshape(-1,1),x[:,7].reshape(-1,1))\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def sample_parameter(self, size, min_value, max_value, apply_log = False):\n","        if not apply_log:\n","          return min_value + (max_value-min_value)*torch.rand([size, 1])\n","        else:\n","          return torch.exp(np.log(min_value)  + (np.log(max_value)-np.log(min_value))*torch.rand([size, 1]))\n","\n","    def sample(self , size = 2**8, to_cpu = False, \n","               sample_log_scale = [False, True, True, True, True, True, True, False, False, False, False] ):\n","        '''\n","        Sampling function\n","\n","        ## sampling x dimesions : nsamples x nofparameters\n","        # 0 = stock\n","        # 1 = carry inventory penalty (phi)\n","        # 2 = arrival rate/s of market orders on the ask (lambda_ask)\n","        # 3 = arrival rate/s of market orders on the bid (lambda_bid)\n","        # 4 = exponential likelihood or order hit as function of depth on the ask (kappa_ask)\n","        # 5 = exponential likelihood or order hit as function of depth on the bid (kappa_bid)\n","        # 6 = fee for taking liquidity at the final stage... (alpha)\n","        # 7 = time (domain on which to solve PDE)\n","        # 8 = max long position (qbar_long) \n","        # 9 = max short position (qbar_short) as a negative number\n","        # 10 = x past cash position (could be positive or negative...)\n","        # 11 = q current inventory\n","        '''\n","        qbar_long_sample = torch.round(self.sample_parameter(size, 1.0, self.QBAR_ULT, sample_log_scale[8]), decimals=0).int()\n","        # sample_log_scale[10] should always be False \n","        q_sample_internal = torch.round(self.sample_parameter(size, -qbar_long_sample, qbar_long_sample, False)).int()\n","\n","        stock_sample_internal = None\n","        if sample_log_scale[0]:\n","          # we will sample as many up stocks as down stocks... (cluster around the start)\n","          stock_sample_internal = torch.cat((self.S_START * self.sample_parameter(size/2, 1.0, self.S_MULT, True),\n","                                             self.S_START * (2.0 - (self.sample_parameter(size/2, 1.0, self.S_MULT, True))) ), \n","                                            dim = 0 )\n","        else:\n","          stock_sample_internal = self.S_START * self.sample_parameter(size, 2.0-self.S_MULT, self.S_MULT, False)\n","\n","        x_internal = self.to_device(torch.cat(( stock_sample_internal,\n","                                                self.sample_parameter(size, self.PHI_MIN, self.PHI_MAX, sample_log_scale[1]),                 \n","                                                self.sample_parameter(size, self.LAMBDA_MIN, self.LAMBDA_MAX, sample_log_scale[2]),                 \n","                                                self.sample_parameter(size, self.LAMBDA_MIN, self.LAMBDA_MAX, sample_log_scale[3]),  \n","                                                self.sample_parameter(size, self.KAPPA_MIN, self.KAPPA_MAX, sample_log_scale[4]),   \n","                                                self.sample_parameter(size, self.KAPPA_MIN, self.KAPPA_MAX, sample_log_scale[5]),   \n","                                                self.sample_parameter(size, self.ALPHA_MIN, self.ALPHA_MAX, sample_log_scale[6]),   \n","                                                self.sample_parameter(size, self.T_MIN, self.T_MAX, sample_log_scale[7]),   \n","                                                qbar_long_sample,\n","                                                -qbar_long_sample,\n","                                                self.S_START-self.sample_parameter(size, \n","                                                                                   self.X_MIN_MULT*self.S_START, \n","                                                                                   self.X_MAX_MULT*self.S_START, \n","                                                                                   sample_log_scale[9]),\n","                                               q_sample_internal                                               \n","                                        ) , dim = 1 ), to_cpu)\n","        qbar_long_sample_terminal = torch.round(self.sample_parameter(size, 1.0, self.QBAR_ULT, sample_log_scale[8]),decimals=0).int()\n","        q_sample_terminal = torch.round(self.sample_parameter(size, -qbar_long_sample_terminal, qbar_long_sample_terminal, False)).int()\n","\n","        stock_sample_terminal = None\n","        if sample_log_scale[0]:\n","          # we will sample as many up stocks as down stocks... (cluster around the start)\n","          stock_sample_terminal = torch.cat((self.S_START * self.sample_parameter(size/2, 1.0, self.S_MULT, True),\n","                                             self.S_START * (2.0 - (self.sample_parameter(size/2, 1.0, self.S_MULT, True))) ), \n","                                            dim = 0 )\n","        else:\n","          stock_sample_terminal = self.S_START * self.sample_parameter(size, 2.0-self.S_MULT, self.S_MULT, False)\n","\n","        x_terminal = self.to_device(torch.cat(( stock_sample_terminal,\n","                                                self.sample_parameter(size, self.PHI_MIN, self.PHI_MAX, sample_log_scale[1]),                 \n","                                                self.sample_parameter(size, self.LAMBDA_MIN, self.LAMBDA_MAX, sample_log_scale[2]),                 \n","                                                self.sample_parameter(size, self.LAMBDA_MIN, self.LAMBDA_MAX, sample_log_scale[3]),  \n","                                                self.sample_parameter(size, self.KAPPA_MIN, self.KAPPA_MAX, sample_log_scale[4]),   \n","                                                self.sample_parameter(size, self.KAPPA_MIN, self.KAPPA_MAX, sample_log_scale[5]),   \n","                                                self.sample_parameter(size, self.ALPHA_MIN, self.ALPHA_MAX, sample_log_scale[6]),  \n","                                                self.T_MAX*torch.ones([size, 1]),                                                 \n","                                                qbar_long_sample_terminal,\n","                                                -qbar_long_sample_terminal,\n","                                                self.S_START-self.sample_parameter(size, \n","                                                                                   self.X_MIN_MULT*self.S_START, \n","                                                                                   self.X_MAX_MULT*self.S_START, \n","                                                                                   sample_log_scale[9]),\n","                                               q_sample_terminal\n","                                        ) , dim = 1 ),to_cpu)\n","        \n","        # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","        return x_internal , x_terminal\n","\n","\n","    def get_num_pi_epochs(self):\n","      if self.adapt_pi_epochs:\n","        if any([ i < self.epoch_of_u for i in self.start_adapt_epochs[0] ]):\n","          return self.pi_net_epoch * self.start_adapt_epochs[1][[i for i in range(len(self.start_adapt_epochs[0])) if self.start_adapt_epochs[0][i] < self.epoch_of_u][-1]] \n","      return self.pi_net_epoch\n","\n","    \n","    def get_utility_function_derivatives(self, u_net_val, x_internal, normalize=False):\n","        \n","        du = torch.autograd.grad( u_net_val, \n","                                  x_internal, \n","                                  grad_outputs=torch.ones_like(u_net_val),\n","                                  create_graph=True,\n","                                  retain_graph=True)\n","        \n","        du_dt = du[0][:,7].reshape(-1,1)\n","        du_dx = du[0][:,0].reshape(-1,1)     \n","\n","        d2u_dx2 = torch.autograd.grad(du_dx, \n","                                      x_internal , \n","                                      grad_outputs=torch.ones_like(du_dx),\n","                                      retain_graph=True\n","                                      )[0][:,0].reshape(-1,1)\n","        \n","        # d2u_dx2 = torch.sign(d2u_dx2) * torch.maximum(torch.abs(d2u_dx2), 1e-8*torch.ones_like(d2u_dx2))                                      \n","\n","        return du_dt, du_dx, d2u_dx2 \n","   \n","    def criterion(self, x_internal, x_terminal, loss_transform = torch.abs, util_network=None):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # mu / sigma / phi / lambda_bid / lambda_ask / kappa_bid / kappa_ask / alpha / time of day / qmax long / qmin short (sample data order)\n","        x_internal_before = x_internal.clone()\n","        x_terminal_before = x_terminal.clone()\n","\n","        x_internal =  Variable(x_internal, requires_grad=True)\n","        x_terminal =  Variable(x_terminal, requires_grad=True)\n","        \n","        if util_network is None:\n","          util_network = self.u_net\n","\n","        u_net_val = util_network(x_internal)\n","        u_net_val_terminal = util_network(x_terminal)\n","       \n","        du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n","        # du_dt_terminal, du_dx_terminal, d2u_dx2_terminal = self.get_utility_function_derivatives(u_net_val_terminal, x_terminal)\n","        # du_dt = (du_dt + du_dt_terminal)/2.0\n","        # du_dx = (du_dx + du_dx_terminal)/2.0\n","        # d2u_dx2 = (d2u_dx2 + d2u_dx2_terminal)/2.0\n","\n","        val_func_non_opt = du_dt + \\\n","                            0.5*self.SIGMA_FIXED*self.SIGMA_FIXED*d2u_dx2 - \\\n","                            x_internal[:,1].reshape(-1,1)*torch.square(x_internal[:,10].reshape(-1,1)) \n","\n","        # here we train control only once per self.num_of_value_iters training epochs of value net\n","        control_model_both = ControlEquationBothSidesDeltas(self.control_net, \n","                                                      self.util_network, \n","                                                      self.q_low, \n","                                                      self.q_high, \n","                                                      grads)\n","\n","        control_model_both.curr_epoch = self.epoch_of_u\n","\n","        #control_model_minus.curr_epoch = self.epoch_of_u\n","        \n","        control_trainer = TrainInternalControl(self, \n","                                               control_model_both, \n","                                                x_internal.shape[0], \n","                                                self.get_num_control_epochs(), \n","                                                self.control_net_lr,\n","                                                hook_interval=self.hook_interval, sync_flag=self.sync_flag)           \n","        control_trainer.train(sample_batch=(x_internal, x_terminal))\n","\n","        delta_plus_used, delta_minus_used = self.control_net(x_internal)\n","\n","        ## sampling x dimesions : nsamples x nofparameters\n","        # 0 = stock\n","        # 1 = carry inventory penalty (phi)\n","        # 2 = arrival rate/s of market orders on the ask (lambda_ask)\n","        # 3 = arrival rate/s of market orders on the bid (lambda_bid)\n","        # 4 = exponential likelihood or order hit as function of depth on the ask (kappa_ask)\n","        # 5 = exponential likelihood or order hit as function of depth on the bid (kappa_bid)\n","        # 6 = fee for taking liquidity at the final stage... (alpha)\n","        # 7 = time (domain on which to solve PDE)\n","        # 8 = max long position (qbar_long) \n","        # 9 = max short position (qbar_short) as a negative number\n","        # 10 = x past cash position (could be positive or negative...)\n","        # 11 = q current inventory\n","\n","        q = x_internal[:,3].reshape(-1, 1)\n","        sigma = x_internal[:,4].reshape(-1, 1)\n","        phi = x_internal[:,5].reshape(-1, 1)\n","        kappa_plus = x_internal[:,7].reshape(-1, 1)\n","        kappa_minus = x_internal[:,8].reshape(-1, 1)\n","        lamda_plus = x_internal[:,9].reshape(-1, 1)\n","        lamda_minus = x_internal[:,10].reshape(-1, 1)\n","\n","        batch_size = x_internal.shape[0]\n","          \n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]\n","\n","        intC = None\n","        terC = None\n","\n","        if len(x_internal) == 0:\n","          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          \n","\n","          control_preds = self.control_net_plus(x_internal)\n","\n","\n","\n","        delta_plus_options =  [ -(i+1)*self.S_TICK for i in range(self.NUM_DELTAS)].append(0.0)\n","        \n","        \n","                              [ (i+1)*self.S_TICK for i in range(self.NUM_DELTAS) ]\n","\n","        delta_plus = x_internal[:,2].reshape(-1,1)\n","\n","\n","\n","\n","\n","        if loss_transform is None:\n","          loss_transform = torch.abs\n","\n","        intC = None\n","        terC = None\n","\n","        if len(x_internal) == 0:\n","          intC = torch.tensor(0).cuda().float() \n","        else:\n","          # # pdb.set_trace()\n","          # pi_net_preds = self.pi_net_target(x_internal_before)\n","          # pi_net_preds = pi_net_preds.detach().reshape(-1,1)\n","          \n","          # x_internal =  Variable(x_internal_before.clone(), requires_grad=True)\n","          \n","          # u_net_val = util_network(x_internal)\n","          # du_dt, du_dx, d2u_dx2 = self.get_utility_function_derivatives(u_net_val, x_internal)\n","\n","          x_internal_const = x_internal.detach().clone()\n","          # x_internal_const = x_internal.clone()\n","          \n","          # pi*(mu-r)\n","          intC_loss_1 = pi_net_preds*(x_internal_const[:,2].reshape(-1,1)-x_internal_const[:,3].reshape(-1,1)) \n","          \n","          # r * wealth\n","          intC_loss_2 =  x_internal_const[:,3].reshape(-1,1)*x_internal_const[:,1].reshape(-1,1)  \n","          \n","          # sigma^2 * pi^2\n","          intC_loss_3 = (x_internal_const[:,4].reshape(-1,1)**2)*(pi_net_preds**2)\n","\n","          intC_loss = du_dt + (intC_loss_1 + intC_loss_2)*du_dx + 0.5*intC_loss_3*d2u_dx2\n","\n","          # eps = 1e-3\n","          # intC_loss = torch.abs((intC_loss-eps)/eps)\n","          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n","        \n","        \n","        # print('mu:', x_internal[:,2].reshape(-1,1))\n","        # print('r:', x_internal[:,3].reshape(-1,1))\n","        # print('wealth:', x_internal[:,1].reshape(-1,1))\n","        # print('sigma^2', x_internal[:,4].reshape(-1,1)**2)\n","        # print('pi:', pi_net_preds)\n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        x_terminal_before = x_terminal.clone()\n","\n","        x_terminal =  Variable(x_terminal_before, requires_grad=True)\n","        # print('non terminal pred', u_net_val)\n","        # print('terminal utility pred', util_network(x_terminal))\n","        # print('utility target', self.g(x_terminal))\n","        # print('----------------------')\n","        # print('g terminal:', self.g(x_terminal))\n","        # print('unet:', util_network(x_terminal))\n","        # print('diff:', (self.g(x_terminal)-util_network(x_terminal)) )\n","        # print('perecentage error:', (self.g(x_terminal)-util_network(x_terminal))/(self.g(x_terminal) + 1e-7))\n","        # print('------------------------------')\n","        \n","        terminal_target = self.g(x_terminal)\n","        util_net_pred = util_network(x_terminal)\n","\n","        err1 = torch.abs((util_net_pred-terminal_target)/(util_net_pred+1e-7) )\n","        err2 = torch.abs((util_net_pred-terminal_target)/(terminal_target+1e-7) )\n","        terC = torch.where(torch.abs(terminal_target) < torch.abs(util_net_pred), err2, err1)\n","\n","        terC = terminal_target - util_net_pred\n","\n","        terC = [ loss_transform(terC) for loss_transform in loss_transforms ]\n","        \n","        # print('mean d2u_dx2:', d2u_dx2.mean(),'max d2u_dx2:', d2u_dx2.max())\n","        # print('mean du_dx:', du_dx.mean(),'max du_dx:', d2u_dx2.max())\n","        # print('mean du_dt:', du_dt.mean(),'max d2u_dx2:', d2u_dx2.max())\n","        \n","        return  intC , terC\n","\n","    def calculateLoss(self, batch_x, train = True, loss_transform = torch.abs, keep_batch = False, util_network=None):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        # pdb.set_trace()\n","        x_internal , x_terminal = batch_x\n","        x_internal = Variable( x_internal , requires_grad=True)\n","        x_terminal = Variable( x_terminal , requires_grad=True)\n","        # print('MertonEquation calling self.criterion')\n","\n","        if util_network is None:\n","          util_network = self.u_net\n","\n","        Ls = self.criterion(x_internal , x_terminal, loss_transform = loss_transform, util_network=util_network)\n","        intC , terC  = Ls\n","\n","        return_losses = []\n","        # print('internal Loss', torch.mean(intC[0]))\n","        # print('external Loss', torch.mean(terC[0]))\n","\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            loss_equalWeightedByType = torch.mean(intC[lc]) + torch.mean(terC[lc])\n","            return_losses.append( [ loss_equalWeightedByType , \n","                                    torch.mean(intC[lc]) , torch.mean(terC[lc]), \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC.numpy(), terC.numpy()] )\n","        return return_losses\n","\n","    # def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","    #     '''\n","    #     Helper function that Samples and Calculate loss,\n","    #     This is adapted in that it changes the weights on the losses\n","    #     and the distribution of sampling to maximize the loss provided \n","    #     the KL distance of the loss is within positive constraints\n","    #     beta represents the constraints on the weights\n","    #     gamma represents the constraints on the sampling distribution\n","    #     (each representing an upper bound the KL distribution)\n","    #     '''        \n","    #     # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","    #     x , x_terminal  = batch_x\n","    #     x = Variable( x, requires_grad=True)\n","    #     Ls = self.criterion( x , x_terminal, loss_transforms = loss_transforms)\n","    #     intC , terC  = Ls\n","\n","    #     if self.weights is None:\n","    #       self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","    #     return_losses = []\n","    #     for lc in range(len(loss_transforms)):\n","    #       if not keep_batch:\n","    #         intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n","    #         terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n","    #         loss_equalWeightedByType = 0.5*torch.mean(intC[lc]) + 0.5*torch.mean(terC[lc]) \n","    #         transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt )\n","    #         return_losses.append( [ transformed_loss , \n","    #                                 0.5*torch.mean(intC[lc]) , 0.5*torch.mean(terC[lc]),\n","    #                                 loss_equalWeightedByType ] )            \n","    #       else:\n","    #         return_losses.append( [intC[lc].numpy(), terC[lc].numpy()] )\n","    #     return return_losses\n","    "]},{"cell_type":"code","source":["class ControlEquationBothSidesDeltas:\n","\n","    def __init__(self,  control_net, \n","                        value_net, \n","                        delta_minus_max, \n","                        delta_plus_max, \n","                        max_position, \n","                        value_func_no_control_part = None,\n","                        curr_epoch=0): \n","                        # grads, \n","        \n","        using_fixed_deltas = False\n","        if isinstance(delta_minus_max, list) &  isinstance(delta_plus_max, list):\n","          using_fixed_deltas = True\n","        elif not isinstance(delta_minus_max, list) and not isinstance(delta_plus_max, list):\n","          using_fixed_deltas = False\n","        else:\n","          raise ValueError(\"deltas must be either both numeric or both numeric lists\")\n","\n","        # infinity_delta = \n","        self.control_net = control_net\n","        self.value_net = value_net\n","        self.curr_epoch = curr_epoch\n","        self.q_bar_max_long = max_position\n","        self.q_bar_min_short = -max_position\n","        self.value_func_no_control_part = value_func_no_control_part\n","        # self.grads = grads\n","\n","        self.delta_minus_max = delta_minus_max\n","        self.delta_plus_max = delta_plus_max\n","        self.delta_combinations = torch.tensor([(x,y) for x in self.delta_minus_max for y in self.delta_plus_max ]) if using_fixed_deltas else None\n","        self.using_fixed_deltas = using_fixed_deltas\n","\n","\n","    def criterion(self, x_internal, fixed_value_func = None, loss_transform = torch.abs):\n","      \n","        intC_loss = None\n","        if len(x_internal) == 0:\n","          intC_loss = torch.tensor(0).cuda().float()  \n","        else:\n","          # x_internal\n","          ## sampling x dimesions : nsamples x nofparameters\n","          # 0 = stock\n","          # 1 = carry inventory penalty (phi)\n","          # 2 = arrival rate/s of market orders on the ask (lambda_ask)\n","          # 3 = arrival rate/s of market orders on the bid (lambda_bid)\n","          # 4 = exponential likelihood or order hit as function of depth on the ask (kappa_ask)\n","          # 5 = exponential likelihood or order hit as function of depth on the bid (kappa_bid)\n","          # 6 = fee for taking liquidity at the final stage... (alpha)\n","          # 7 = time (domain on which to solve PDE)\n","          # 8 = max long position (qbar_long) \n","          # 9 = max short position (qbar_short) as a negative number        \n","          # 10 = x past cash position (could be positive or negative...)\n","          # 11 = q current inventory\n","          if fixed_value_func is None:\n","            fixed_value_func = self.value_func_no_control_part\n","\n","          control_preds = self.control_net(x_internal)  \n","\n","          delta_plus_used = None\n","          delta_minus_used = None\n","          if self.using_fixed_deltas:\n","            control_pred_idx = torch.argmax(control_preds, dim=0) # picks the sup delta plus and sup delta minus\n","            delta_plus_used, delta_minus_used  = [ torch.tensor(x) for x in self.delta_combinations[control_pred_idx,:].reshape(-1,1).tolist() ] \n","          else:\n","            delta_plus_used, delta_minus_used = control_preds  \n","\n","          q_starting_inventory = x_internal[:,11].reshape(-1, 1)\n","\n","          x_internal_for_both_deltas = x_internal.detach().clone() \n","          x_internal_for_both_deltas.requires_grad = False # should already be the case..\n","          baseH_for_both = self.value_net(x_internal_for_both_deltas)\n","\n","          x_internal_for_delta_plus = x_internal.detach().clone()\n","          x_internal_for_delta_plus.requires_grad = False # should already be the case..\n","          x_internal_for_delta_plus[:,10] = x_internal_for_delta_plus[:,10] + x_internal_for_delta_plus[:,0] + delta_plus_used\n","          x_internal_for_delta_plus[:,11] = x_internal_for_delta_plus[:,11] - 1\n","          hValue_for_delta_plus = self.value_net(x_internal_for_delta_plus)\n","\n","          x_internal_for_delta_minus = x_internal.detach().clone()\n","          x_internal_for_delta_minus.requires_grad = False # should already be the case..\n","          x_internal_for_delta_minus[:,10] = x_internal_for_delta_minus[:,10] - x_internal_for_delta_plus[:,0] - delta_minus_used\n","          x_internal_for_delta_minus[:,11] = x_internal_for_delta_minus[:,11] + 1\n","          hValue_for_delta_minus = self.value_net(x_internal_for_delta_minus)\n","\n","          lambda_plus = x_internal[:,2].reshape(-1, 1)\n","          lambda_minus = x_internal[:,3].reshape(-1, 1)\n","\n","          kappa_plus = x_internal[:,4].reshape(-1, 1)\n","          kappa_minus = x_internal[:,5].reshape(-1, 1)\n","\n","          delta_plus_control_contrib =  lambda_plus * (q_starting_inventory > self.q_bar_min_short) * torch.ones_like(q_starting_inventory) * \\\n","                                        torch.exp(-kappa_plus * delta_plus_used) * (hValue_for_delta_plus - baseH_for_both)\n","\n","          delta_minus_control_contrib =  lambda_minus * (q_starting_inventory < self.q_bar_max_long) * torch.ones_like(q_starting_inventory) * \\\n","                                         torch.exp(-kappa_minus * delta_minus_used) * (hValue_for_delta_minus - baseH_for_both)\n","          \n","          intC_loss = fixed_value_func + delta_plus_control_contrib + delta_minus_control_contrib\n","          intC_loss = loss_transform(intC_loss)\n","\n","        return intC_loss\n","\n","    def calculateControlLoss(self, x_internal, keep_batch = False, fixed_value_func = None, loss_transform = torch.abs):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        Ls = self.criterion(x_internal, fixed_value_func = fixed_value_func, loss_transform = loss_transform)\n","        \n","        if not keep_batch:          \n","          return torch.mean(Ls)           \n","        else:\n","          return Ls\n"],"metadata":{"id":"xx737osCrWN7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659459823367,"user_tz":-60,"elapsed":314,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"cde18ded-fcda-46dc-b07b-40419eb9be0d"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 86.7 ms (started: 2022-08-02 17:03:43 +00:00)\n"]}]},{"cell_type":"code","source":["# a = [1.2, 4.3, 5.3, 3.2]\n","# b = [3.4, 1.2, 32.2, 0.5]\n","# # torch.ones_like(torch.tensor(a)) * (torch.tensor(a) > torch.tensor(b))\n","# (torch.tensor(a) > 2.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PiZJtmTSODdw","executionInfo":{"status":"ok","timestamp":1659304031203,"user_tz":-60,"elapsed":377,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"15ac82d5-b60a-4538-c0ba-1ace71526086"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([False,  True,  True,  True])"]},"metadata":{},"execution_count":25},{"output_type":"stream","name":"stdout","text":["time: 5.74 ms (started: 2022-07-31 21:47:11 +00:00)\n"]}]},{"cell_type":"markdown","source":["### Training Architectures"],"metadata":{"id":"aIM34Y1TvrPk"}},{"cell_type":"code","source":["class TrainBothInternalControls():\n","    \n","    def __init__(self , \n","                 u_equation, \n","                 pi_equation,\n","                 BATCH_SIZE, \n","                 epoch, \n","                 lr, \n","                 debug = False, \n","                 loss_multiply = 1.0):\n","        \n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.u_model = u_equation        \n","        self.pi_model = pi_equation\n","        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","\n","        self.epoch = epoch\n","        self.lr = lr\n","\n","        self.loss_multiply = loss_multiply\n","\n","    def train(self, eqLossFn = 'calculateControlLoss', passed_internal_sample = None, loss_transform = torch.abs):\n","\n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n","        \n","        self.train_losses = np.ones((self.epoch, 2 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n","        # optimizer.zero_grad()\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.pi_model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(self.epoch):\n","          sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE) if passed_internal_sample is None else passed_internal_sample\n","          # pdb.set_trace()\n","          x_internal = Variable(sample_batch[0], requires_grad=True)\n","          x_terminal =  Variable(sample_batch[1], requires_grad=True)\n","\n","          loss_internal  = loss_calc_method(x_internal, \n","                                            keep_batch = False, \n","                                            fixed_value_func = self.pi_equation.value_func_no_control_part,\n","                                            loss_transform = loss_transform) \n","          #  fixed_value_func will get defaulted to this anyway, this is just to make it explicit\n","          loss_terminal  = loss_calc_method(x_terminal, \n","                                            keep_batch = False, \n","                                            fixed_value_func = self.pi_equation.value_func_no_control_part,\n","                                            loss_transform = loss_transform) \n","          # this is where we could consider the terminal loss as well in adjusting the gradients...\n","\n","          self.train_losses[e,:] = [ to_cpu_detach(loss_internal), to_cpu_detach(loss_terminal) ]\n","\n","          if self.debug == True and (self.validation_sample is not None):\n","            loss_validation = loss_calc_method( self.validation_sample, \n","                                                  keep_batch = False,\n","                                                  loss_transform = loss_transform )\n","            self.validation_losses[e,:] = [ to_cpu_detach(loss_validation) ]\n","          \n","          if self.use_early_stop:\n","            loss_to_check = loss\n","            if loss_to_check < (self.best_loss-self.early_stop_delta):\n","              self.best_loss = loss_to_check\n","              self.early_stop_counter = 0\n","            else:\n","              self.early_stop_counter += 1\n","            if self.early_stop_counter>=self.early_stop_patience:\n","              # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n","              break\n","\n","          optimizer.zero_grad()\n","          # print('self.epoch:', e)\n","          loss.backward()\n","          optimizer.step()\n","          \n","          if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","              print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(e , self.lr , float(loss.item())))\n","\n","          self.stop_epoch = e\n","          # self.pi_model_traget.pi_net.load_state_dict(self.pi_model.pi_net.state_dict())\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n"],"metadata":{"id":"LBnB_FJzNtQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrainCJWithDGM():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        \n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","        self.stop_epoch = 0\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","        \n","        self.net.train()\n","\n","    def train(self, eqLossFn = 'calculateLoss', passed_internal_sample = None, loss_transform = torch.abs):\n","\n","        if self.validation_sample is not None:\n","          self.validation_losses = np.ones((self.epoch, 1 ), dtype='float32') * np.nan\n","        \n","        self.train_losses = np.ones((self.epoch, 2 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.pi_model.pi_net.parameters(), self.lr)\n","        # optimizer.zero_grad()\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.pi_model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.pi_model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(self.epoch):\n","          sample_batch = self.u_model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE) if passed_internal_sample is None else passed_internal_sample\n","          # pdb.set_trace()\n","          x_internal = Variable(sample_batch[0], requires_grad=True)\n","          x_terminal =  Variable(sample_batch[1], requires_grad=True)\n","\n","          loss_internal  = loss_calc_method(x_internal, \n","                                            keep_batch = False, \n","                                            fixed_value_func = self.pi_equation.value_func_no_control_part,\n","                                            loss_transform = loss_transform) \n","          #  fixed_value_func will get defaulted to this anyway, this is just to make it explicit\n","          loss_terminal  = loss_calc_method(x_terminal, \n","                                            keep_batch = False, \n","                                            fixed_value_func = self.pi_equation.value_func_no_control_part,\n","                                            loss_transform = loss_transform) \n","          # this is where we could consider the terminal loss as well in adjusting the gradients...\n","\n","          self.train_losses[e,:] = [ to_cpu_detach(loss_internal), to_cpu_detach(loss_terminal) ]\n","\n","          if self.debug == True and (self.validation_sample is not None):\n","            loss_validation = loss_calc_method( self.validation_sample, \n","                                                  keep_batch = False,\n","                                                  loss_transform = loss_transform )\n","            self.validation_losses[e,:] = [ to_cpu_detach(loss_validation) ]\n","          \n","          if self.use_early_stop:\n","            loss_to_check = loss\n","            if loss_to_check < (self.best_loss-self.early_stop_delta):\n","              self.best_loss = loss_to_check\n","              self.early_stop_counter = 0\n","            else:\n","              self.early_stop_counter += 1\n","            if self.early_stop_counter>=self.early_stop_patience:\n","              # print(f\"Pi Early Stop at epoch {e}: {loss_to_check} with patience {self.early_stop_patience}\")\n","              break\n","\n","          optimizer.zero_grad()\n","          # print('self.epoch:', e)\n","          loss.backward()\n","          optimizer.step()\n","          \n","          if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","              print(\"Pi Epoch {} - lr {} -  key loss: {}\".format(e , self.lr , float(loss.item())))\n","\n","          self.stop_epoch = e\n","          # self.pi_model_traget.pi_net.load_state_dict(self.pi_model.pi_net.state_dict())\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\", \"Mu\", \"R\", \"Sigma\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n"],"metadata":{"id":"ulO4C_i3UUxP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659459758634,"user_tz":-60,"elapsed":290,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"7c467638-77cc-4f91-d341-b4b07c7868a0"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 104 ms (started: 2022-08-02 17:02:38 +00:00)\n"]}]},{"cell_type":"code","source":["# a = list(range(4))\n","# b = np.linspace(10.0, 14.0,5)\n","# pp((a,b))\n","# torch.tensor([ (x,y) for x in a for y in b]).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KriJGlwR_3l","executionInfo":{"status":"ok","timestamp":1659187738812,"user_tz":-60,"elapsed":4,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"acbea2da-0683-4d75-b956-6067e157d5ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["([0, 1, 2, 3], array([10., 11., 12., 13., 14.]))\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2])"]},"metadata":{},"execution_count":26},{"output_type":"stream","name":"stdout","text":["time: 6.73 ms (started: 2022-07-30 13:28:59 +00:00)\n"]}]}]}