{"cells":[{"cell_type":"markdown","metadata":{"id":"BErSeefeQwQi"},"source":["### Setup packages "]},{"cell_type":"code","source":["from pprint import pprint as pp"],"metadata":{"id":"5o3jM5mDxKG8","executionInfo":{"status":"ok","timestamp":1660129916376,"user_tz":-60,"elapsed":453,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23663,"status":"ok","timestamp":1660129940037,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"L8DGCgVxR2AB","outputId":"78a1344d-cffd-4467-ac39-a988f3f80365"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14285,"status":"ok","timestamp":1660129954312,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"3xIx5C6UQn4u","outputId":"83200b33-5bae-484c-d51e-476500764c84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting progressbar\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","Building wheels for collected packages: progressbar\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12082 sha256=5eee8b4b9cc946b1d31045e1e81f007aecfa305e32c22f3eacbd89e981f94feb\n","  Stored in directory: /root/.cache/pip/wheels/f0/fd/1f/3e35ed57e94cd8ced38dd46771f1f0f94f65fec548659ed855\n","Successfully built progressbar\n","Installing collected packages: progressbar\n","Successfully installed progressbar-2.5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.21.6)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.7.3)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.3.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->plotnine) (4.1.1)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2022.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n","Installing collected packages: ipython-autotime\n","Successfully installed ipython-autotime-0.3.1\n","time: 165 Âµs (started: 2022-08-10 11:12:34 +00:00)\n"]}],"source":["%pip install progressbar\n","%pip install plotnine\n","%pip install torch\n","%pip install ipython-autotime\n","%load_ext autotime"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1872,"status":"ok","timestamp":1660129956167,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"EfIU_eNp3Zio","outputId":"54bcbcca-8d2e-4df5-8bf1-9a75066154dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.83 s (started: 2022-08-10 11:12:34 +00:00)\n"]}],"source":["from plotnine import *\n","from plotnine.themes import *"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2795,"status":"ok","timestamp":1660129958956,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ZmUjYbArAuQT","outputId":"a88443bd-186a-4749-9583-466d4a81c57e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.9 s (started: 2022-08-10 11:12:35 +00:00)\n"]}],"source":["import tensorflow as tf\n","from scipy.io import loadmat\n","import random\n","import math\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{"id":"PieVKPfHHYQ6"},"source":["_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1660129958956,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"433z6V3T2rB2","outputId":"3a0af727-1f29-4325-bf66-a6cc1c94212e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 5.54 ms (started: 2022-08-10 11:12:38 +00:00)\n"]}],"source":["import os, sys\n","import errno\n","\n","# make a directory if it does not exist\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","# make directories if they do not exist\n","\n","# make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660129958956,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"uat0pG8aR3Rh","outputId":"22dae33f-56b3-4ce7-a876-74d5253fd282"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 9.57 ms (started: 2022-08-10 11:12:38 +00:00)\n"]}],"source":["# Set up the imports\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","\n","import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","import random\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2668,"status":"ok","timestamp":1660129961620,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"KpFjo3MkLus9","outputId":"7eb91f64-24cc-4696-8a31-9247207b5584"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.43 s (started: 2022-08-10 11:12:38 +00:00)\n"]}],"source":["import torch \n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from scipy.stats import norm\n","from matplotlib import cm\n","import pdb\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2665,"status":"ok","timestamp":1660129964279,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"CbfN42gpGZhC","outputId":"05f54760-42d9-450e-a592-c7194f1f25fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.2 s (started: 2022-08-10 11:12:41 +00:00)\n"]}],"source":["import plotly.graph_objects as go\n","import plotly.express as px\n"]},{"cell_type":"markdown","metadata":{"id":"bvy0WvxDGCxk"},"source":["### Shared functions across models"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1660129964282,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ONB5NopRa3fD","outputId":"22978783-697d-47b3-92a5-94a6441c0991"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 50.5 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["# a set up that just maximizes the loss s.t. loss < eps (maximizeloss_weights_st) using the weights on the losses\n","from scipy.optimize import LinearConstraint, NonlinearConstraint\n","from scipy.optimize import Bounds\n","from functools import partial\n","from scipy.optimize import minimize\n","from functools import wraps\n","\n","def negative(f):\n","    @wraps(f)\n","    def g(*args,**kwargs):\n","        return - f(*args,**kwargs)\n","    # g.__name__ = f'negative({f.__name__})'\n","    return g\n","# kl_loss = nn.KLDivLoss(size_average=None, reduction=\"batchmean\")\n","\n","# we can add more minimization functions here later (e.g. SS diff)\n","def KLDiffHere( varX, loss_terms, log_target = False, reduction = \"mean\"):  \n","  target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*torch.tensor(loss_terms)\n","  input = torch.tensor(varX*loss_terms)\n","  loss_pointwise = target * (torch.log(target) - torch.log(input))\n","  if reduction == \"mean\":  # default\n","      loss = loss_pointwise.mean()\n","  elif reduction == \"batchmean\":  # mathematically correct\n","      loss = loss_pointwise.sum() / input.size(0)\n","  elif reduction == \"sum\":\n","      loss = loss_pointwise.sum()\n","  else:  # reduction == \"none\"\n","      loss = loss_pointwise  \n","  return loss\n","\n","  # return torch.nn.KLDivLoss(varX*loss_terms,np.array([1./len(loss_terms)]*len(loss_terms))*loss_terms)\n","\n","def minimize_weights_st(loss_terms, loss_func):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  x0 = [0.25]*len(loss_terms)\n","  res = minimize( partial(loss_func, loss_terms=loss_terms), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n","\n","def maximizeloss_weights_st(loss_terms, loss_func, eps):\n","  bounds = Bounds([0]*len(loss_terms), [1.0]*len(loss_terms))\n","  linear_constraint = LinearConstraint([[1]*len(loss_terms)], [1.0], [1.0])\n","  nonlinear_constraint  = NonlinearConstraint(negative(partial(loss_func, loss_terms=loss_terms)),1E-9,eps)\n","  # even though zero is the KL minimum it helps to put a negative number here to explore\n","\n","  x0 = [1.0/len(loss_terms)]*len(loss_terms)\n","  res = minimize( negative(partial(loss_func, loss_terms=loss_terms)), \n","                  x0, \n","                  method='trust-constr', \n","                  constraints=[linear_constraint, nonlinear_constraint],\n","                  options={'verbose': 0}, \n","                  bounds=bounds )\n","  return res\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":521,"status":"ok","timestamp":1660129964790,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"RM0IVdZ_TXW3","outputId":"a1c8de58-5d6f-4196-9780-5f6fca4356c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.33334757 0.33333761 0.33331482]\n","time: 237 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["r1 = maximizeloss_weights_st( [ 34.25, 100.12, 23.45] , KLDiffHere, 1E9)\n","print(r1.x)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1660129964790,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"iyacROFeXgNp","outputId":"91ab00fb-c745-4b71-ec80-2045969b65f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 20.1 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["import torch\n","from torch.distributions import Normal\n","\n","std_norm_cdf = Normal(0, 1).cdf\n","std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n","\n","def bs_price(right, K, S, T, sigma, r):\n","    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n","    d_2 = d_1 - sigma * torch.sqrt(T)\n","    \n","    if right == \"C\":\n","        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n","        return C\n","        \n","    elif right == \"P\":\n","        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n","        return P"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1660129964791,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"sLsA5AvqpMM7","outputId":"6b9822f9-a1f5-48e7-e5cb-89634e26484b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.91 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["import torch\n","\n","def to_cpu_detach(x):\n","  if isinstance(x, list):\n","    return [ y.detach().cpu().item() for y in x ]\n","  else:\n","    return x.detach().cpu().item()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1660129964791,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"PC-E2SeX46A9","outputId":"d142f51d-7864-47af-c3a4-3b3074ac1cd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.75 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["def huber_loss_zero_target(x, delta = 1.0):\n","  loss_function = torch.nn.HuberLoss(delta=delta)\n","  return loss_function(x, torch.zeros_like(x))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1660129964791,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"1D2S9Y3vG45V","outputId":"ba7d1fe5-28c3-43e7-da8f-6f2752ec03ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 7.64 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["def save_model_train_intermediate(lr, net,  epoch, eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        gamma = getattr(eqObject,\"gamma\")\n","        gamma_str = str(gamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{gamma_str}\"\n","    except AttributeError:\n","        pass\n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/intermediate/{model_id_str}\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1660129964792,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"MNYJyHWpeL66","outputId":"39ca339a-917e-4c97-c711-6ee977572bf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 77 ms (started: 2022-08-10 11:12:43 +00:00)\n"]}],"source":["def save_model_train(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        gamma = getattr(eqObject,\"gamma\")\n","        gamma_str = str(gamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{gamma_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1660129964792,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"euik2c9_NaTM","outputId":"2e2e9755-54b7-44b9-9089-ef7503cd6dc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 18.2 ms (started: 2022-08-10 11:12:44 +00:00)\n"]}],"source":["def save_model_train_stratified_intermediate(lr, net,  epoch, eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        gamma = getattr(eqObject,\"gamma\")\n","        gamma_str = str(gamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{gamma_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        xbreaks = getattr(eqObject,\"xbreaks\")\n","        xbreaks_str = str(len(xbreaks))\n","        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        tbreaks = getattr(eqObject,\"tbreaks\")\n","        tbreaks_str = str(len(tbreaks))\n","        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/intermediate/{model_id_str}\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1660129964792,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ipogSsVTbv0k","outputId":"65fadcd9-34a5-4353-c2f9-d70dd89e55ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 66 ms (started: 2022-08-10 11:12:44 +00:00)\n"]}],"source":["def save_model_train_stratified(lr, net,  eqLossFn, sample_method, trainObj, eqType, eqObject = None ):\n","\n","  model_id_str =  f\"{eqType}_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainObj.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\"\n","  \n","  if eqObject is not None:\n","    try:\n","        beta = getattr(eqObject,\"beta\")\n","        beta_str = str(beta).replace('.','p')\n","        model_id_str = model_id_str + f\"_beta{beta_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        gamma = getattr(eqObject,\"gamma\")\n","        gamma_str = str(gamma).replace('.','p')\n","        model_id_str = model_id_str + f\"_gamma{gamma_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        xbreaks = getattr(eqObject,\"xbreaks\")\n","        xbreaks_str = str(len(xbreaks))\n","        model_id_str = model_id_str + f\"_StSaXbrks{xbreaks_str}\"\n","    except AttributeError:\n","        pass\n","    try:\n","        tbreaks = getattr(eqObject,\"tbreaks\")\n","        tbreaks_str = str(len(tbreaks))\n","        model_id_str = model_id_str + f\"_StSaTbrks{tbreaks_str}\"\n","    except AttributeError:\n","        pass\n","  \n","  torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{model_id_str}\")\n","  df_at_hookintervals = None\n","  train_losses = None\n","  validation_losses = None\n","  try:\n","      df_at_hookintervals = getattr(trainObj, \"history_surfaces_hooks\")\n","      if df_at_hookintervals is not None:\n","        df_at_hookintervals.to_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationHook_{trainObj.hook_interval}_{model_id_str}.csv\", index=False)\n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"history_surfaces_hooks\"))\n","\n","  try:\n","      train_losses = getattr(trainObj,\"train_losses\")\n","      if train_losses is not None:\n","        train_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/trainlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","      # raise NotImplementedError(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"train_losses\"))\n","\n","  try:\n","      validation_losses = getattr(trainObj,\"validation_losses\")\n","      if validation_losses is not None:\n","        validation_losses.tofile(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/validationlosses_{model_id_str}.csv\", sep = ',')    \n","  except AttributeError:\n","      print(\"Class `{}` does not have `{}`\".format(trainObj.__class__.__name__, \"validation_losses\"))"]},{"cell_type":"markdown","metadata":{"id":"Tz5tUJuYaXKu"},"source":["### Single Stock European Call option - sampling methodology\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HrivvbmubiiY"},"source":["#### EuropeanOptionNet"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1660130762550,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"JRraqOG4aXKx","outputId":"d9680886-4379-482c-ac88-43b23b151ebe"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 16.4 ms (started: 2022-08-10 11:26:02 +00:00)\n"]}],"source":["class EuropeanOptionNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh  ):\n","        super(EuropeanOptionNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        ### Number of stocks + time\n","        ### ( t , xi)\n","        self.Input = 1 + 1\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        # pdb.set_trace()\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out \n","    "]},{"cell_type":"markdown","metadata":{"id":"wClW1g9rbm8o"},"source":["#### EuropeanBlackScholesSingleStock"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1660130767586,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"JtraPjmPwhDj","outputId":"9d7c98dc-678f-4686-cddb-dcccabdf6fd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 820 Âµs (started: 2022-08-10 11:26:06 +00:00)\n"]}],"source":["# bs_price(\"C\", torch.Tensor([ 40.0, 50.0, 60.0]), 50.0, torch.Tensor([ 1.0, 1.0, 1.0]), torch.Tensor([ 0.25, 0.25, 0.25]), 0.05 )"]},{"cell_type":"code","execution_count":22,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1359,"status":"ok","timestamp":1660130769467,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"LBMZYQSPaXKy","outputId":"9996d6fc-4303-48cc-dfb9-129add110c9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.29 s (started: 2022-08-10 11:26:07 +00:00)\n"]}],"source":["import math\n","\n","class EuropeanBlackScholesSingleStock():\n","    \n","    def __init__(self , net, is_call = True):\n","\n","        self.C = 0.0           \n","        self.R = 0.05         # Interest Rate (Yearly)\n","\n","        self.SIGMA = 0.25  # Volatility (Yearly)\n","        self.RU = 1.0      # stock correlation\n","        \n","        self.K = 50.0              # Strike Price \n","        self.T = 1.0               # Maturation time (in YEAR)\n","        self.MAX_X = self.K*3.0   # MAX price\n","        ## for accept reject purpose!\n","        ## free boundry problems\n","        self.net = net\n","        \n","        self.weights = None\n","        self.eps = 1E9\n","        self.weights_tbl = []\n","\n","        self.gamma = 0.0001\n","        self.beta = 0.0001\n","\n","        self.is_call = is_call\n","        self.log_normal_dist = torch.distributions.LogNormal(self.R-self.C, self.SIGMA)\n","        self.log_normal_dist_5 = torch.distributions.LogNormal(self.R-self.C, self.SIGMA*5.0)\n","\n","        self.xbreaks = None\n","        self.tbreaks = None\n","\n","\n","    def reset_weights(self):\n","        self.weights = None\n","        self.eps = 1E9\n","        self.weights_tbl = []\n","\n","    def g(self , x):\n","        # pay off function - 1 is the stock dimension, 0 is the time dimension\n","        if self.is_call:\n","          return torch.max( x[:,1].reshape(-1,1) - self.K , torch.zeros([len(x),1]).cuda() ) \n","        else:\n","          return torch.max( self.K - x[:,1].reshape(-1,1) , torch.zeros([len(x),1]).cuda() ) \n","\n","    def mu(self, x):\n","        ## should test it! output dimension is important !\n","        return (self.R-self.C)*x.reshape(-1,1)\n","\n","    def sigma(self , x):\n","        return self.SIGMA*x.reshape(-1,1)\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","        '''\n","        Sampling function\n","        '''\n","        # 4 samples returned:\n","        # internal, boundary, initial, terminal\n","        if sample_method_X in [\"U\",\"UE3\"]:\n","            range_multiplier = 3.0 if sample_method_X == \"UE3\" else 1.0\n","            # internal samples\n","            x = self.to_device(torch.cat(( torch.rand([size,1])*self.T , -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ),to_cpu)\n","            ### Terminal time samples\n","            x_terminal = self.to_device(torch.cat( ( torch.zeros(size, 1) + self.T , -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier ) , dim = 1 ),to_cpu)\n","            ### initial time samples\n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier ) , dim = 1 ).cuda()\n","            ### initial time samples\n","            x_initial = self.to_device(torch.cat( ( torch.zeros(size, 1), self.K*torch.ones( size, 1)) , dim = 1 ),to_cpu)\n","            ### non-zero\n","            stock_space = self.K*0.8*torch.rand([size, 1]) if self.is_call else self.K*(1.2 + torch.rand([size, 1]))\n","            x_nonzero = self.to_device(torch.cat( ( torch.rand([size,1])*self.T, stock_space ) , dim = 1 ),to_cpu)\n","            compare = self.net(x_nonzero) \n","            mask = compare < 0\n","            x_nonzero = x_nonzero[mask.reshape(-1),:]\n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            return x , x_terminal , x_initial, x_nonzero\n","    \n","        if sample_method_X in [\"LN\", \"LN5\"]:\n","            ln_dist = self.log_normal_dist_5 if sample_method_X == \"LN5\" else self.log_normal_dist\n","            # internal samples\n","            x = self.to_device(torch.cat(( torch.rand([size,1])*self.T , torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0]))   ) , dim = 1 ),to_cpu)\n","            ### Terminal time samples\n","            x_terminal = self.to_device(torch.cat( ( torch.zeros(size, 1) + self.T , torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0])) ) , dim = 1 ), to_cpu)\n","            ### initial time samples\n","            x_initial = self.to_device(torch.cat( ( torch.zeros(size, 1), self.K*torch.ones( size, 1)) , dim = 1 ),to_cpu)\n","            # x_initial = torch.cat( ( torch.zeros(size, 1),            torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0])) ), dim = 1 ).cuda()\n","            ### non-zero\n","            stock_space = self.K*0.8*torch.maximum(ln_dist.sample((size,)).reshape(-1,1),torch.Tensor([0.0]))  if self.is_call else self.K*(1.2 + torch.maximum(ln_dist.sample((size,)).reshape(-1,1),torch.Tensor([0.0])) )\n","            x_nonzero = self.to_device(torch.cat( ( torch.rand([size,1])*self.T, stock_space ) , dim = 1 ),to_cpu)\n","            compare = self.net(x_nonzero) \n","            mask = compare < 0\n","            x_nonzero = x_nonzero[mask.reshape(-1),:]\n","            return (x , x_terminal , x_initial, x_nonzero)\n","\n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","        \n","    def sample_stratified(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","\n","      if self.xbreaks is None and self.tbreaks is None:\n","        return self.sample(sample_method_X, size, to_cpu)\n","\n","      # internal_strata_xts = self.to_device(torch.tensor([]),to_cpu)\n","      # terminal_strata_xts = self.to_device(torch.tensor([]),to_cpu)\n","      # initial_strata_xts = self.to_device(torch.tensor([]),to_cpu)\n","      # nonzero_strata_xts = self.to_device(torch.tensor([]),to_cpu)\n","      internal_strata_xts = []\n","      terminal_strata_xts = []\n","      initial_strata_xts = []\n","      nonzero_strata_xts = []\n","\n","      # pdb.set_trace()\n","      \n","      if sample_method_X in [\"U\",\"UE3\"]:\n","\n","          range_multiplier = 3.0 if sample_method_X == \"UE3\" else 1.0\n","          xbreaks_used = self.xbreaks[:] if self.xbreaks is not None else [0,range_multiplier*self.MAX_X]\n","          tbreaks_used = self.tbreaks[:] if self.tbreaks is not None else [0,self.T]\n","          if xbreaks_used[-1] < range_multiplier*self.MAX_X:\n","            xbreaks_used.append(range_multiplier*self.MAX_X)\n","          while xbreaks_used[0] < 0.0:\n","            xbreaks_used.pop(0)\n","          if not xbreaks_used:\n","            xbreaks_used = [0,range_multiplier*self.MAX_X]\n","          if xbreaks_used[0] > 0.0:            \n","            xbreaks_used.insert(0, 0.0)\n","\n","          if tbreaks_used[-1] < self.T:\n","            tbreaks_used.append(self.T)\n","          xbreaks_range = xbreaks_used[-1]-xbreaks_used[0]\n","          tbreaks_range = tbreaks_used[-1]-tbreaks_used[0]\n","          \n","          # if len(xbreaks_used)<1:\n","          #   # pdb.set_trace()\n","          #   pass\n","\n","          total_strat_processed = 0\n","          # internal samples\n","          for stratum_x_count in range(len(xbreaks_used)-1):\n","              \n","            num_samples_in_stratum = 0\n","            if len(xbreaks_used) > 2:  # x division takes priority so assign it if there is no T division\n","              range_ratio_x_stratum = (xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range\n","              num_samples_in_stratum = math.ceil(range_ratio_x_stratum*size)\n","\n","            for stratum_t_count in range(len(self.tbreaks)-1):\n","\n","              if num_samples_in_stratum == 0: # there is only a T division, so use it\n","                range_ratio_t_stratum = (tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range\n","                num_samples_in_stratum = math.ceil(range_ratio_t_stratum*size)\n","              else:\n","                # there is both an X and a T division, assign the number of samples uniformly, assuming same scale of X and T\n","                stratum_coverage_on_unit_square = \\\n","                  ((xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])/xbreaks_range)*\\\n","                  ((tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count])/tbreaks_range)\n","                num_samples_in_stratum = math.ceil(stratum_coverage_on_unit_square * size)\n","              # pdb.set_trace()\n","\n","              ### internal samples\n","              internal_stratum_t_sample = self.to_device(tbreaks_used[stratum_t_count] + torch.rand([num_samples_in_stratum,1])*(tbreaks_used[stratum_t_count+1]-tbreaks_used[stratum_t_count]), to_cpu)\n","              internal_stratum_x_sample = self.to_device(xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count]), to_cpu)\n","              internal_stratum_xt = torch.cat(( internal_stratum_t_sample , internal_stratum_x_sample) , dim = 1 )\n","              if internal_stratum_xt.numel()<1:\n","                # pdb.set_trace()\n","                pass\n","              \n","              if not internal_strata_xts: #.numel()<1:\n","                internal_strata_xts = [ internal_stratum_xt ] # internal_stratum_xt[None,:,:]\n","              else:\n","                internal_strata_xts.append(internal_stratum_xt)  # torch.vstack((internal_strata_xts,internal_stratum_xt[None,:,: ]))\n","\n","              ### Terminal time samples\n","              terminal_stratum_x_sample = xbreaks_used[stratum_x_count] + torch.rand([num_samples_in_stratum,1])*(xbreaks_used[stratum_x_count+1]-xbreaks_used[stratum_x_count])\n","              terminal_stratum_xt = self.to_device(torch.cat( ( torch.zeros(num_samples_in_stratum, 1) + self.T , terminal_stratum_x_sample ) , dim = 1 ),to_cpu)\n","\n","              if not terminal_strata_xts:\n","                terminal_strata_xts = [ terminal_stratum_xt ] # terminal_stratum_xt[None,:,:]\n","              else:\n","                terminal_strata_xts.append(terminal_stratum_xt) # torch.vstack((terminal_strata_xts,terminal_stratum_xt[None,:,: ]))\n","\n","              ### initial time samples\n","              # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier ) , dim = 1 ).cuda()\n","              ### initial time samples\n","              initial_stratum_xt = self.to_device(torch.cat( ( torch.zeros(num_samples_in_stratum, 1), self.K*torch.ones( num_samples_in_stratum, 1)) , dim = 1 ),to_cpu)\n","              if not initial_strata_xts: #.numel()<1:\n","                initial_strata_xts = [initial_stratum_xt] # initial_stratum_xt[None,:,:]\n","              else:\n","                initial_strata_xts.append(initial_stratum_xt) # torch.vstack((initial_strata_xts,initial_stratum_xt[None,:,: ]))\n","\n","              ### non-zero value samples\n","              stratum_mapped_stock_space = None\n","              if self.is_call:\n","                stratum_mapped_stock_space = self.K*(1.0/xbreaks_used[-1])* np.array([ xbreaks_used[stratum_x_count], xbreaks_used[stratum_x_count+1]])\n","              else:\n","                stratum_mapped_stock_space = self.K + self.K*(1.0/xbreaks_used[-1])*np.array([ xbreaks_used[stratum_x_count], xbreaks_used[stratum_x_count+1]])              \n","              nonzero_stratum_x_sample = stratum_mapped_stock_space[0] + torch.rand([num_samples_in_stratum, 1])*(stratum_mapped_stock_space[1]-stratum_mapped_stock_space[0])\n","              nonzero_stratum_xt = self.to_device(torch.cat( ( torch.rand([num_samples_in_stratum,1])*self.T, nonzero_stratum_x_sample ) , dim = 1 ),to_cpu)\n","              compare = self.net(nonzero_stratum_xt) \n","              mask = compare < 0\n","              nonzero_stratum_xt = nonzero_stratum_xt[mask.reshape(-1),:]\n","              if not nonzero_strata_xts: #.numel()<1:\n","                nonzero_strata_xts = [nonzero_stratum_xt] #nonzero_stratum_xt[None,:,:]\n","              else:\n","                # pdb.set_trace()\n","                nonzero_strata_xts.append(nonzero_stratum_xt) # torch.vstack((nonzero_strata_xts,nonzero_stratum_xt[None,:,: ]))\n","\n","              total_strat_processed += 1 \n","              # print((len(internal_strata_xts),xbreaks_used[stratum_x_count],tbreaks_used[stratum_t_count]))\n","\n","              # if len(np.where([x.numel()!=4 for x in internal_strata_xts])[0]) >0:\n","              #   pdb.set_trace()\n","              #   pass\n","              # if len(internal_strata_xts) == 3:\n","              #   pdb.set_trace() # the problem is the next one\n","              #   pass\n","\n","          # pdb.set_trace()\n","          # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","          return internal_strata_xts , terminal_strata_xts , initial_strata_xts, nonzero_strata_xts\n","    \n","      raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","\n","\n","    def criterion(self, x , x_terminal , x_initial, x_nonzero, loss_transforms = [torch.square]):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # pdb.set_trace()     \n","        d = torch.autograd.grad(\n","            self.net(x), \n","            x, \n","            grad_outputs=torch.ones_like(self.net(x)) ,\n","            create_graph=True )\n","        dt  = d[0][:,0].reshape(-1,1)\n","        dx1 = d[0][:,1].reshape(-1,1)\n","        # du/dxdx\n","        dx1x1 = torch.autograd.grad(dx1, \n","                                    x , \n","                                    grad_outputs=torch.ones_like(dx1) ,\n","                                    create_graph = True)[0][:,1].reshape(-1,1)\n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]\n","        intC = None\n","        terC = None\n","        iniC = None\n","        nzC = None\n","\n","        if len(x) == 0:\n","          # print('zero batch size for domain!')\n","          intC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          # x is above the free boundary ( so immediate pay-off is positive )\n","          intC_loss = dt + self.mu(x[:,1])*( dx1 ) + 0.5*( (self.sigma(x[:,1])*self.sigma(x[:,1]))*dx1x1 ) - self.R*self.net(x)\n","          intC = [ loss_transform(intC_loss) for loss_transform in loss_transforms ] \n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        terC = [ loss_transform( self.g(x_terminal) - self.net(x_terminal) ) for loss_transform in loss_transforms ]\n","\n","        # pdb.set_trace()\n","        # Initial Condition - should be equal (both in- and out of the money)\n","        # Time is time to maturity \n","        initial_px_est = self.net(x_initial)\n","        iniC = [ loss_transform( initial_px_est - bs_price(\"C\" if self.is_call else \"P\", \n","                                                           torch.Tensor([self.K]), \n","                                                           torch.Tensor([self.K]), \n","                                                           torch.Tensor([self.T]), \n","                                                           torch.Tensor([self.SIGMA]), \n","                                                           torch.Tensor([self.R])).to(initial_px_est.device)  ) for \\\n","                 loss_transform in loss_transforms ]\n","        # closed_form_initial_pxs = bs_price(\"C\" if self.is_call else \"P\", self.K, x_initial[:,1], x_initial[:,0], torch.Tensor([self.SIGMA]).to(initial_px_est.device), self.R ).to(initial_px_est.device)\n","        # iniC = loss_transform( initial_px_est - closed_form_initial_pxs )\n","\n","        if len(x_nonzero) == 0:\n","          nzC = [ torch.tensor(0).cuda().float() for loss_transform in loss_transforms ] \n","        else:\n","          nzC = [ loss_transform(self.net(x_nonzero)) for loss_transform in loss_transforms ]\n","        return  intC , terC , iniC, nzC\n","\n","    def calculateLoss(self, batch_x, train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","\n","        # pdb.set_trace()\n","        x , x_terminal , x_initial, x_nonzero = batch_x\n","        x = Variable( x , requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , x_initial, x_nonzero, loss_transforms = loss_transforms )\n","        intC , terC , iniC, nzC = Ls\n","\n","        numActive = np.sum([1 if xb.numel()>0 else 0 for xb in batch_x ])\n","        # DOm = torch.mean(DO).detach().cpu().float().item()\n","        # TCm = torch.mean(TC).detach().cpu().float().item()\n","        # BCm = torch.mean(BC).detach().cpu().float().item()\n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            loss_equalWeightedByType = (1./numActive*torch.mean(intC[lc]) + 1./numActive*torch.mean(terC[lc]) + 1./numActive*torch.mean(iniC[lc]) + 1./numActive*torch.mean(nzC[lc]))\n","            return_losses.append( [ loss_equalWeightedByType , \n","                                    1./numActive*torch.mean(intC[lc]) , 1./numActive*torch.mean(terC[lc]) , 1./numActive*torch.mean(iniC[lc]), 1./numActive*torch.mean(nzC[lc]) , \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC.numpy(), terC.numpy(), iniC.numpy(), nzC.numpy()] )\n","        return return_losses\n","\n","    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal , x_initial, x_nonzero = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , x_initial, x_nonzero, loss_transforms = loss_transforms)\n","        intC , terC , iniC, nzC = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n","\n","        numActive = np.sum([1 if xb.numel()>0 else 0 for xb in batch_x ])\n","\n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * torch.pow((1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * intC[lc])), self.gamma/self.beta) \n","            terCt = self.weights[0,1] * torch.pow((1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * terC[lc])), self.gamma/self.beta) \n","            iniCt = self.weights[0,2] * torch.pow((1.0/iniC[lc].numel() if iniC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * iniC[lc])), self.gamma/self.beta) \n","            nzCt = self.weights[0,3] * torch.pow((1.0/nzC[lc].numel() if nzC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.beta * nzC[lc])), self.gamma/self.beta) \n","            loss_equalWeightedByType = (1./numActive*torch.mean(intC[lc]) + 1./numActive*torch.mean(terC[lc]) + 1./numActive*torch.mean(iniC[lc]) + 1./numActive*torch.mean(nzC[lc]))\n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt + iniCt + nzCt)\n","            return_losses.append( [ transformed_loss , \n","                                    1./numActive*torch.mean(intC[lc]) , 1./numActive*torch.mean(terC[lc]) , 1./numActive*torch.mean(iniC[lc]), 1./numActive*torch.mean(nzC[lc]) , \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy(), iniC[lc].numpy(), nzC[lc].numpy()] )\n","        return return_losses\n","\n","        # loss_equalWeightedByType = (1./numActive*torch.mean(intC) + 1./numActive*torch.mean(terC) + 1./numActive*torch.mean(iniC) + 1./numActive*torch.mean(nzC))\n","        # return   transformed_loss, 1./numActive*torch.mean(intC) , 1./numActive*torch.mean(terC) , 1./numActive*torch.mean(iniC) , loss_equalWeightedByType\n","\n","\n","    def calculateLossKLMinMaxGamma(self , batch_x , train = True, loss_transforms = [ torch.square ], keep_batch = False):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        # x , x_terminal , x_initial = self.sample(sample_method_X, size)\n","        x , x_terminal , x_initial, x_nonzero = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , x_initial, x_nonzero, loss_transforms = loss_transforms)\n","        intC , terC , iniC, nzC = Ls\n","\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(intC[0].device)/len(Ls)\n","        \n","        # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n","        numActive = np.sum([1 if xb.numel()>0 else 0 for xb in batch_x ])\n","\n","        return_losses = []\n","        for lc in range(len(loss_transforms)):\n","          if not keep_batch:\n","            intCt = self.weights[0,0] * (1.0/intC[lc].numel() if intC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * intC[lc])) \n","            terCt = self.weights[0,1] * (1.0/terC[lc].numel() if terC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * terC[lc])) \n","            iniCt = self.weights[0,2] * (1.0/iniC[lc].numel() if iniC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * iniC[lc])) \n","            nzCt = self.weights[0,3] * (1.0/nzC[lc].numel() if nzC[lc].numel() > 0 else 0.0) * torch.sum( torch.exp(self.gamma * nzC[lc])) \n","            loss_equalWeightedByType = (1./numActive*torch.mean(intC[lc]) + 1./numActive*torch.mean(terC[lc]) + 1./numActive*torch.mean(iniC[lc]) + 1./numActive*torch.mean(nzC[lc]))\n","            transformed_loss = 1.0/self.gamma * torch.log(intCt + terCt + iniCt + nzCt)\n","            return_losses.append( [ transformed_loss , \n","                                    1./numActive*torch.mean(intC[lc]) , 1./numActive*torch.mean(terC[lc]) , 1./numActive*torch.mean(iniC[lc]), 1./numActive*torch.mean(nzC[lc]) , \n","                                    loss_equalWeightedByType ] )            \n","          else:\n","            return_losses.append( [intC[lc].numpy(), terC[lc].numpy(), iniC[lc].numpy(), nzC[lc].numpy()] )\n","        return return_losses\n","\n","    "]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1660130769468,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"PgcsDuDFNULd","outputId":"9cd2a5b0-0c3a-4d6a-839a-c54c50f0c122"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 736 Âµs (started: 2022-08-10 11:26:08 +00:00)\n"]}],"source":["# torch.tensor([[123,2414,5151],[2312,31,41],[21,5111,1414]]).numel()"]},{"cell_type":"markdown","metadata":{"id":"65nooklCbsdy"},"source":["#### TrainEuropeanBlackScholesSingleStock"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1660130770216,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"OtO8fV7oaXK2","outputId":"3888cffa-7bc2-413a-a12e-dd478614c5c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 798 ms (started: 2022-08-10 11:26:08 +00:00)\n"]}],"source":["class TrainEuropeanBlackScholesSingleStock():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","\n","        self.loss_to_beat = None\n","        self.loss_to_start = None\n","        self.improvement_from_loss_to_start = 100.0\n","        \n","\n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\", key_loss_func = torch.square, huber_delta = 0.5):\n","        \n","        self.validation_losses = np.ones((epoch, 6*3 - 1), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 6*2 -1 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            # pdb.set_trace()\n","            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            losses_L2, losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n","            # pdb.set_trace()\n","            loss , internal , terminal , initial, nonzero, losses_equalWeightedByType = losses_L2\n","            loss_abs , internal_abs , terminal_abs , initial_abs, nonzero_abs, losses_equalWeightedByType_abs = losses_ABS\n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal , initial, nonzero]))\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) , to_cpu_detach(internal) , to_cpu_detach(terminal) , to_cpu_detach(initial), to_cpu_detach(nonzero), \n","                                       to_cpu_detach(loss_abs) , to_cpu_detach(internal_abs) , to_cpu_detach(terminal_abs) , to_cpu_detach(initial_abs), to_cpu_detach(nonzero_abs), to_cpu_detach(losses_equalWeightedByType_abs)]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = loss_calc_method( self.validation_sample, \n","                                                                                                     loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                                                                                     keep_batch = False )\n","              validation_loss_list = [*to_cpu_detach(losses_L2_validation),\n","                                      *to_cpu_detach(losses_ABS_validation),\n","                                      *to_cpu_detach(losses_Huber_valiation)]\n","              validation_loss_list = validation_loss_list.pop(5) # the L2 loss is duplicated at index 1\n","              self.validation_losses[e,:] = validation_loss_list\n","            \n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            loss_to_check = float(loss.item())\n","            loss_avg = loss_avg + loss_to_check\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","\n","                loss_avg = loss_avg/self.hook_interval\n","                if self.loss_to_start is None:\n","                  self.loss_to_start = loss_to_check\n","                \n","                if self.loss_to_beat is None:\n","                  self.loss_to_beat = loss_to_check \n","\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                # print(f\"Stage Setting {loss_to_check},{self.loss_to_beat},{self.loss_to_start/self.improvement_from_loss_to_start}\")\n","\n","                if loss_to_check < self.loss_to_beat:\n","                  if loss_to_check < self.loss_to_start/self.improvement_from_loss_to_start:\n","                    print(f\"Saving intermediate {loss_to_check},{self.loss_to_beat},{self.loss_to_start/self.improvement_from_loss_to_start}\")\n","                    self.loss_to_beat = loss_to_check\n","                    #  save the thing                    \n","                    save_model_train_intermediate(lr, self.net,  e, eqLossFn, sample_method_X, self, \"EuCallSs\", self.model )\n","\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                self.history_initial.append( initial )\n","                self.history_nonzero.append( nonzero )\n","\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","                    xinitial_res = self.model.net(xinitial).detach()\n","                    xnonzero_res = self.model.net(xnonzero).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    df_initial = self.create_result_df(e, xinitial, xinitial_res, \"INITIAL\")\n","                    df_nonzero = self.create_result_df(e, xnonzero, xnonzero_res, \"NONZERO\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal, df_initial, df_nonzero],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal, df_initial,df_nonzero],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","    def train_stratified(self , epoch , lr, \n","                         eqLossFn = 'calculateLoss', \n","                         sample_method_X = \"U\", \n","                         key_loss_func = torch.square, \n","                         huber_delta = 0.5\n","                         ):\n","        \n","        self.validation_losses = np.ones((epoch, 6*3 - 1), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 6*2), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            internal_xts_bts, terminal_xts_bts, initial_xts_bts, nonzero_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n","            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n","            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n","            \n","            # pdb.set_trace()\n","            for stratum_count in range(len(internal_xts_bts)):              \n","              # sample_batch = (internal_xts_bts[stratum_count,:,:], \n","              #                 terminal_xts_bts[stratum_count,:,:], \n","              #                 initial_xts_bts[stratum_count,:,:], \n","              #                 nonzero_xts_bts[stratum_count,:,:])  \n","              sample_batch = (internal_xts_bts[stratum_count], \n","                              terminal_xts_bts[stratum_count], \n","                              initial_xts_bts[stratum_count], \n","                              nonzero_xts_bts[stratum_count])  \n","\n","              stratum_losses_L2, stratum_losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n","              if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n","                pdb.set_trace()\n","                pass\n","            \n","              if training_stratum_losses is not None:\n","                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n","              else:\n","                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n","\n","              # pdb.set_trace()  \n","              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n","\n","            # pdb.set_trace()              \n","            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n","            loss = training_value_to_optimize\n","\n","            loss_optimized , internal , terminal , initial, nonzero, losses_equalWeightedByType, \\\n","            loss_abs , internal_abs , terminal_abs , initial_abs, nonzero_abs, losses_equalWeightedByType_abs = training_loss_for_epoch            \n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal , initial, nonzero]))\n","\n","            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n","                loss_calc_method( self.validation_sample, \n","                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                  keep_batch = False )\n","              validation_loss = [*to_cpu_detach(losses_L2_validation),\n","                                              *to_cpu_detach(losses_ABS_validation),\n","                                              *to_cpu_detach(losses_Huber_valiation)]\n","              validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n","              self.validation_losses[e,:] = validation_loss\n","\n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","\n","            loss_to_check = float(loss.item())\n","            loss_avg = loss_avg + loss_to_check\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","\n","                if self.loss_to_start is None:\n","                  self.loss_to_start = loss_to_check\n","                \n","                if self.loss_to_beat is None:\n","                  self.loss_to_beat = loss_to_check \n","\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                # print(f\"Stage Setting {loss_to_check},{self.loss_to_beat},{self.loss_to_start/self.improvement_from_loss_to_start}\")\n","\n","                if loss_to_check < self.loss_to_beat:\n","                  if loss_to_check < self.loss_to_start/self.improvement_from_loss_to_start:\n","                    self.loss_to_beat = loss_to_check\n","                    #  save the thing\n","                    print(f\"Saving intermediate {loss_to_check},{self.loss_to_beat},{self.loss_to_start/self.improvement_from_loss_to_start}\")\n","                    save_model_train_stratified_intermediate(lr, self.net,  e, eqLossFn, sample_method_X, self, \"EuCallSs\", self.model )\n","\n","\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                self.history_initial.append( initial )\n","                self.history_nonzero.append( nonzero )\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","                    xinitial_res = self.model.net(xinitial).detach()\n","                    xnonzero_res = self.model.net(xnonzero).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    df_initial = self.create_result_df(e, xinitial, xinitial_res, \"INITIAL\")\n","                    df_nonzero = self.create_result_df(e, xnonzero, xnonzero_res, \"NONZERO\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal, df_initial, df_nonzero],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal, df_initial,df_nonzero],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"Ahvlme_8k2PU","executionInfo":{"status":"ok","timestamp":1660130770216,"user_tz":-60,"elapsed":4,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vGBBQEcik2f2"},"source":["#### Extract EuropeanBlackScholesSingleStock models and run them"]},{"cell_type":"code","source":["os.listdir(\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ciUlAevKzJsA","executionInfo":{"status":"ok","timestamp":1660130776698,"user_tz":-60,"elapsed":1185,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"f08cd1ca-b26e-4daf-de6d-41a301f44d08"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['EuCallSs_20220508213150_calculateLoss_U_39999_0p0001_5_100',\n"," 'EuCallSs_20220508223001_calculateLoss_LN_39999_0p0001_5_100',\n"," 'EuCallSs_20220508230212_calculateLossUsingKLMinMax_U_39999_0p0001_5_100',\n"," 'EuCallSs_20220508232650_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509080833_calculateLoss_U_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509083018_calculateLoss_LN_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509085253_calculateLossUsingKLMinMax_U_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509091521_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100',\n"," 'EuCallSs_20220510143114_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001',\n"," 'EuCallSs_20220510145422_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001',\n"," 'EuCallSs_20220510151626_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07',\n"," 'EuCallSs_20220510160809_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p0001',\n"," 'EuCallSs_20220510163042_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07',\n"," 'EuCallSs_20220510165222_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p1',\n"," 'EuCallSs_20220510180749_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p001',\n"," 'EuCallSs_20220513204142_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514003309_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514042446_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514073022_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514104452_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220513164145_calculateLoss_U_39999_0p0001_5_100_gamma0p0001_stSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514172657_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_Huber0p1_20220514210611_calculateLoss_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_Huber0p1_20220515005345_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_Huber0p1_20220515042748_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'AmCallSs_20220524114755_calculateLoss_U_5388_0p01_3_30',\n"," 'EuCallSs_20220712152840_calculateLoss_U_9_1e-05_5_100_beta0p0001_gamma0p0001_StSaXbrks40_StSaTbrks40',\n"," 'EuCallSs_20220712155645_calculateLoss_U_39_1e-06_5_100_beta0p0001_gamma0p0001_StSaXbrks20_StSaTbrks20',\n"," 'EuCallSs_PiNN_20220810111049_calculateLoss_U_39999_0p0001_5_100']"]},"metadata":{},"execution_count":25},{"output_type":"stream","name":"stdout","text":["time: 819 ms (started: 2022-08-10 11:26:15 +00:00)\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6790,"status":"ok","timestamp":1660130817364,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"outputId":"498eefff-d690-4d44-8820-317aad96a1ba","id":"ffgHp2Ipk2f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.3892239332199097\n","4.895481586456299\n","1.6320843696594238\n","11.720319747924805\n","1.5553274154663086\n","time: 6.5 s (started: 2022-08-10 11:26:50 +00:00)\n"]}],"source":["seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","EuCallModel_SL_U = EuropeanOptionNet(NL = 5, NN = 100)\n","bsSampler = EuropeanBlackScholesSingleStock(EuCallModel_SL_U) # we just use this for sampling\n","test_data = bsSampler.sample(\"U\", 2000, to_cpu=True)[0]\n","bs_price_test = bs_price(\"C\", torch.Tensor([ bsSampler.K  ]), test_data[:,1], test_data[:,0], torch.Tensor([bsSampler.SIGMA]), bsSampler.R )\n","\n","# print(os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals\"))\n","EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/EuCallSs_20220508213150_calculateLoss_U_39999_0p0001_5_100\"))\n","mae_bs = torch.mean(torch.abs(EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach() - bs_price_test)).item()\n","pp(mae_bs)\n","\n","EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/EuCallSs_20220508223001_calculateLoss_LN_39999_0p0001_5_100\"))\n","mae_bs = torch.mean(torch.abs(EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach() - bs_price_test)).item()\n","pp(mae_bs)\n","\n","EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/EuCallSs_20220510145422_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001\"))\n","mae_bs = torch.mean(torch.abs(EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach() - bs_price_test)).item()\n","pp(mae_bs)\n","\n","EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/EuCallSs_Huber0p1_20220509091521_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100\"))\n","mae_bs = torch.mean(torch.abs(EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach() - bs_price_test)).item()\n","pp(mae_bs)\n","\n","EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/EuCallSs_PiNN_20220810111049_calculateLoss_U_39999_0p0001_5_100\"))\n","mae_bs = torch.mean(torch.abs(EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach() - bs_price_test)).item()\n","pp(mae_bs)"]},{"cell_type":"code","source":["EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/dgm_lossfunctional/model_finals/EuCallSs_20220508223001_calculateLoss_LN_39999_0p0001_5_100\"))\n","mae_data = pd.DataFrame( { \"Error\" : (EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach() - bs_price_test).reshape(-1,).numpy().tolist(), \n","                            \"t\" : test_data[:,0].numpy().tolist(),\n","                            \"S\" : test_data[:,1].numpy().tolist(),\n","                            \"R\" : bsSampler.R,\n","                            \"SIGMA\" : bsSampler.SIGMA,\n","                            \"K\" : bsSampler.K,\n","                            \"T\" : bsSampler.T                          \n","                          } )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dI3RZ0Ma3noG","executionInfo":{"status":"ok","timestamp":1660071859394,"user_tz":-60,"elapsed":279,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"e381c21f-4932-4238-8bfd-ce0fc9ec179f"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 27 ms (started: 2022-08-09 19:04:19 +00:00)\n"]}]},{"cell_type":"code","source":["mae_data.head(4)\n","# test_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"wfroTNv4347A","executionInfo":{"status":"ok","timestamp":1660071860542,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"78c5f148-de1a-4bc2-fdbc-a34aef8309f3"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Error         t           S     R  SIGMA     K    T\n","0  -2.154125  0.887434  103.456177  0.05   0.25  50.0  1.0\n","1   0.071004  0.893520   25.428116  0.05   0.25  50.0  1.0\n","2 -13.557930  0.731394  131.209869  0.05   0.25  50.0  1.0\n","3 -28.413155  0.530823  148.554855  0.05   0.25  50.0  1.0"],"text/html":["\n","  <div id=\"df-aa671212-8a37-4c65-9659-a78966f079d1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Error</th>\n","      <th>t</th>\n","      <th>S</th>\n","      <th>R</th>\n","      <th>SIGMA</th>\n","      <th>K</th>\n","      <th>T</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-2.154125</td>\n","      <td>0.887434</td>\n","      <td>103.456177</td>\n","      <td>0.05</td>\n","      <td>0.25</td>\n","      <td>50.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.071004</td>\n","      <td>0.893520</td>\n","      <td>25.428116</td>\n","      <td>0.05</td>\n","      <td>0.25</td>\n","      <td>50.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-13.557930</td>\n","      <td>0.731394</td>\n","      <td>131.209869</td>\n","      <td>0.05</td>\n","      <td>0.25</td>\n","      <td>50.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-28.413155</td>\n","      <td>0.530823</td>\n","      <td>148.554855</td>\n","      <td>0.05</td>\n","      <td>0.25</td>\n","      <td>50.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa671212-8a37-4c65-9659-a78966f079d1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-aa671212-8a37-4c65-9659-a78966f079d1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-aa671212-8a37-4c65-9659-a78966f079d1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":68},{"output_type":"stream","name":"stdout","text":["time: 10.5 ms (started: 2022-08-09 19:04:20 +00:00)\n"]}]},{"cell_type":"code","source":["# [x for x in dir(bsSampler) if \"_\" not in x]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGfUutpg-xiU","executionInfo":{"status":"ok","timestamp":1660071673565,"user_tz":-60,"elapsed":277,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"311ec186-b78b-477b-a30f-1673d0a0c7cc"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['C',\n"," 'K',\n"," 'R',\n"," 'RU',\n"," 'SIGMA',\n"," 'T',\n"," 'beta',\n"," 'calculateLoss',\n"," 'calculateLossKLMinMaxGamma',\n"," 'calculateLossUsingKLMinMax',\n"," 'criterion',\n"," 'eps',\n"," 'g',\n"," 'gamma',\n"," 'mu',\n"," 'net',\n"," 'sample',\n"," 'sigma',\n"," 'tbreaks',\n"," 'weights',\n"," 'xbreaks']"]},"metadata":{},"execution_count":66},{"output_type":"stream","name":"stdout","text":["time: 4.82 ms (started: 2022-08-09 19:01:13 +00:00)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"lwMI2fskAzgQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVXHtBHEAzo4"},"source":["#### Eu Call - [  (U/LN) x (StandardLoss/KLMinMax)  ;  (U/LN) x (HuberLoss/KLMinMaxHuber) ]"]},{"cell_type":"markdown","metadata":{"id":"dw8UfLyxAzo5"},"source":["##### Test comparison on 1/2 saved models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BD3L-ZGAzo5"},"outputs":[],"source":["EuCallModel_SL_U = EuropeanOptionNet(NL = 5, NN = 100)\n","bsSampler = EuropeanBlackScholesSingleStock(EuCallModel_SL_U) # we just use this for sampling\n","test_data = bsSampler.sample(\"U\", 2**4, to_cpu=True)[0]\n","# print(os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals\"))\n","EuCallModel_SL_U.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/EuCallSs_20220508213150_calculateLoss_U_39999_0p0001_5_100\"))\n","\n","print(EuCallModel_SL_U.forward(test_data).reshape(1,-1).detach().numpy())\n","print(bs_price(\"C\", torch.Tensor([ bsSampler.K  ]), test_data[:,1], test_data[:,0], torch.Tensor([bsSampler.SIGMA]), bsSampler.R ))\n"]},{"cell_type":"markdown","metadata":{"id":"YK6B9ahWAzo6"},"source":["##### Set up data frames and save to file"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8IzwucrAzo6","executionInfo":{"status":"ok","timestamp":1660072265128,"user_tz":-60,"elapsed":224,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"97dc0485-3eab-4d55-e170-1f7b1f9ee8d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.44 ms (started: 2022-08-09 19:11:04 +00:00)\n"]}],"source":["def create_surface_result_df(sampled_data, exact_results, fits, sample_type):\n","  df = pd.DataFrame(sampled_data.cpu().detach().numpy(), columns = [\"Time\", \"S1\"])\n","  df[\"Sample\"] = sample_type\n","  df[\"Estimate\"] = fits.cpu().detach().numpy()\n","  df[\"Exact\"] = exact_results.cpu().detach().numpy()\n","  return df"]},{"cell_type":"code","source":["paper_name = \"dgm_lossfunctional\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"luJbgdYTBKpc","executionInfo":{"status":"ok","timestamp":1660072290002,"user_tz":-60,"elapsed":214,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"56335b6a-400e-4e23-e2e9-03148d9ca63c"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 680 Âµs (started: 2022-08-09 19:11:29 +00:00)\n"]}]},{"cell_type":"code","execution_count":71,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zN-Qn3-RAzo6","executionInfo":{"status":"ok","timestamp":1660072290869,"user_tz":-60,"elapsed":231,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"e8830b73-b880-48ea-e79a-fbbe74c168a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["['EuCallSs_20220508213150_calculateLoss_U_39999_0p0001_5_100',\n"," 'EuCallSs_20220508223001_calculateLoss_LN_39999_0p0001_5_100',\n"," 'EuCallSs_20220508230212_calculateLossUsingKLMinMax_U_39999_0p0001_5_100',\n"," 'EuCallSs_20220508232650_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509080833_calculateLoss_U_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509083018_calculateLoss_LN_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509085253_calculateLossUsingKLMinMax_U_39999_0p0001_5_100',\n"," 'EuCallSs_Huber0p1_20220509091521_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100',\n"," 'EuCallSs_20220510143114_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001',\n"," 'EuCallSs_20220510145422_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001',\n"," 'EuCallSs_20220510151626_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07',\n"," 'EuCallSs_20220510160809_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p0001',\n"," 'EuCallSs_20220510163042_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07',\n"," 'EuCallSs_20220510165222_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p1',\n"," 'EuCallSs_20220510180749_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p001',\n"," 'EuCallSs_20220513204142_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514003309_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514042446_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514073022_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514104452_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220513164145_calculateLoss_U_39999_0p0001_5_100_gamma0p0001_stSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_20220514172657_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_Huber0p1_20220514210611_calculateLoss_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_Huber0p1_20220515005345_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'EuCallSs_Huber0p1_20220515042748_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n"," 'AmCallSs_20220524114755_calculateLoss_U_5388_0p01_3_30',\n"," 'EuCallSs_20220712152840_calculateLoss_U_9_1e-05_5_100_beta0p0001_gamma0p0001_StSaXbrks40_StSaTbrks40',\n"," 'EuCallSs_20220712155645_calculateLoss_U_39_1e-06_5_100_beta0p0001_gamma0p0001_StSaXbrks20_StSaTbrks20']\n","time: 4.8 ms (started: 2022-08-09 19:11:30 +00:00)\n"]}],"source":["from pprint import pprint as pp\n","pp(os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals\"))"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuVdGyPaAzo6","executionInfo":{"status":"ok","timestamp":1660072295947,"user_tz":-60,"elapsed":211,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"d89d3fbc-d56a-4b7c-fb54-9d290860d989"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 6.07 ms (started: 2022-08-09 19:11:35 +00:00)\n"]}],"source":["def get_df_fits(dir_name, title, sampled_internal_data, exact_results, model = None):\n","  if model is None:\n","    model = EuropeanOptionNet(NL = 5, NN = 100)\n","  model.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{dir_name}\"))\n","  model_fits = model.forward(sampled_internal_data)\n","  df_model = create_surface_result_df(sampled_internal_data, exact_results, model_fits, title)\n","  df_model = df_model.sort_values(by=['Time','S1'], ascending=[True, True]).reset_index()\n","  df_model.drop(['index'], axis=1, inplace=True)\n","  df_model[\"diff\"] = df_model.Estimate - df_model.Exact\n","  df_model['diff_abs'] = np.abs(df_model[\"diff\"].tolist())\n","  df_model['perc_abs'] = np.abs(1.0 - df_model.Estimate/df_model.Exact)\n","  return df_model"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpYyOsg1Azo7","executionInfo":{"status":"ok","timestamp":1660072310997,"user_tz":-60,"elapsed":3700,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"75f0e043-d234-4833-f790-2c14d73ba3d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.31 s (started: 2022-08-09 19:11:47 +00:00)\n"]}],"source":["EuCallModel_SL_U = EuropeanOptionNet(NL = 5, NN = 100)\n","bsSampler = EuropeanBlackScholesSingleStock(EuCallModel_SL_U) # we just use this for sampling\n","sampled_internal_data = bsSampler.sample(\"U\", 2**14, to_cpu=True)[0]\n","exact_results = bs_price(\"C\", torch.Tensor([ bsSampler.K  ]), sampled_internal_data[:,1], sampled_internal_data[:,0], torch.Tensor([bsSampler.SIGMA]), bsSampler.R )\n","\n","df_EuCallModel_SL_U = get_df_fits(\"EuCallSs_20220508213150_calculateLoss_U_39999_0p0001_5_100\", \"SL_U_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_SL_LN = get_df_fits(\"EuCallSs_20220508223001_calculateLoss_LN_39999_0p0001_5_100\", \"SL_LN_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KL_U = get_df_fits(\"EuCallSs_20220508230212_calculateLossUsingKLMinMax_U_39999_0p0001_5_100\", \"KL_U_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KL_LN = get_df_fits(\"EuCallSs_20220508232650_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100\", \"KL_LN_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","\n","df_EuCallModel_HL_U = get_df_fits(\"EuCallSs_Huber0p1_20220509080833_calculateLoss_U_39999_0p0001_5_100\", \"HL_U_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_HL_LN = get_df_fits(\"EuCallSs_Huber0p1_20220509083018_calculateLoss_LN_39999_0p0001_5_100\", \"HL_LN_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KLH_U = get_df_fits(\"EuCallSs_Huber0p1_20220509085253_calculateLossUsingKLMinMax_U_39999_0p0001_5_100\", \"KLH_U_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KLH_LN = get_df_fits(\"EuCallSs_Huber0p1_20220509091521_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100\", \"KLH_LN_NST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","\n","df_EuCallModel_SL_U_ST = get_df_fits(\"EuCallSs_20220513164145_calculateLoss_U_39999_0p0001_5_100_gamma0p0001_stSaXbrks5_StSaTbrks5\", \"SL_U_ST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KL_U_ST = get_df_fits(\"EuCallSs_20220513204142_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5\", \"KL_U_ST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KL_U_ST2 = get_df_fits(\"EuCallSs_20220514042446_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5\", \"KL_U_ST2\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","\n","df_EuCallModel_SLH_U_ST = get_df_fits(\"EuCallSs_Huber0p1_20220514210611_calculateLoss_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5\", \"SLH_U_ST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","df_EuCallModel_KLH_U_ST = get_df_fits(\"EuCallSs_Huber0p1_20220515005345_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5\", \"KLH_U_ST\", sampled_internal_data, exact_results, model=EuropeanOptionNet(NL = 5, NN = 100))\n","\n","\n","# 'EuCallSs_20220508213150_calculateLoss_U_39999_0p0001_5_100',\n","#  'EuCallSs_20220508223001_calculateLoss_LN_39999_0p0001_5_100',\n","#  'EuCallSs_20220508230212_calculateLossUsingKLMinMax_U_39999_0p0001_5_100',\n","#  'EuCallSs_20220508232650_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100',\n","#  'EuCallSs_Huber0p1_20220509080833_calculateLoss_U_39999_0p0001_5_100',\n","#  'EuCallSs_Huber0p1_20220509083018_calculateLoss_LN_39999_0p0001_5_100',\n","#  'EuCallSs_Huber0p1_20220509085253_calculateLossUsingKLMinMax_U_39999_0p0001_5_100',\n","#  'EuCallSs_Huber0p1_20220509091521_calculateLossUsingKLMinMax_LN_39999_0p0001_5_100',\n","#  'EuCallSs_20220510143114_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001',\n","#  'EuCallSs_20220510145422_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001',\n","#  'EuCallSs_20220510151626_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07',\n","#  'EuCallSs_20220510160809_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p0001',\n","#  'EuCallSs_20220510163042_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07',\n","#  'EuCallSs_20220510165222_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p1',\n","#  'EuCallSs_20220510180749_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p001',\n","#  'EuCallSs_20220513204142_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_20220514003309_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_20220514042446_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_20220514073022_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_20220514104452_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma1p0000000000000001e-07_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_20220513164145_calculateLoss_U_39999_0p0001_5_100_gamma0p0001_stSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_20220514172657_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta0p0001_gamma0p001_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_Huber0p1_20220514210611_calculateLoss_U_39999_0p0001_5_100_beta0p0001_gamma0p0001_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_Huber0p1_20220515005345_calculateLossUsingKLMinMax_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n","#  'EuCallSs_Huber0p1_20220515042748_calculateLossKLMinMaxGamma_U_39999_0p0001_5_100_beta1p0000000000000001e-07_gamma0p0001_StSaXbrks5_StSaTbrks5',\n","#  'AmCallSs_20220524114755_calculateLoss_U_5388_0p01_3_30'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-RBr03JAzo7"},"outputs":[],"source":["# print(df_EuCallModel_SL_U[df_EuCallModel_SL_U.Exact>0.1].sort_values(by='perc_abs', ascending=False).head(10))\n","# print(df_EuCallModel_KL_U[df_EuCallModel_KL_U.Exact>0.1].sort_values(by='perc_abs', ascending=False).head(10))\n","# print(df_EuCallModel_SL_LN[df_EuCallModel_SL_LN.Exact>0.1].sort_values(by='perc_abs', ascending=False).head(10))\n","print(df_EuCallModel_KL_LN[df_EuCallModel_KL_LN.Exact>0.1].sort_values(by='perc_abs', ascending=False).head(10))"]},{"cell_type":"code","source":["pp(df_EuCallModel_SL_U.head(4))\n","df_EuCallModel_SL_U.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6W1nERPXBWU6","executionInfo":{"status":"ok","timestamp":1660072342355,"user_tz":-60,"elapsed":288,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"0b96cd7f-6d7e-4666-bbe6-a64f0ebf4bab"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["       Time          S1    Sample   Estimate      Exact      diff  diff_abs  perc_abs\n","0  0.000088   29.862587  SL_U_NST   0.156650   0.000000  0.156650  0.156650       inf\n","1  0.000113  137.734177  SL_U_NST  82.658432  87.734459 -5.076027  5.076027  0.057857\n","2  0.000170  131.268539  SL_U_NST  78.300072  81.268967 -2.968895  2.968895  0.036532\n","3  0.000208   89.999611  SL_U_NST  42.338440  40.000130  2.338310  2.338310  0.058458\n"]},{"output_type":"execute_result","data":{"text/plain":["(16384, 8)"]},"metadata":{},"execution_count":75},{"output_type":"stream","name":"stdout","text":["time: 8.95 ms (started: 2022-08-09 19:12:21 +00:00)\n"]}]},{"cell_type":"code","execution_count":76,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kt3Q-HmCAzo7","executionInfo":{"status":"ok","timestamp":1660072374082,"user_tz":-60,"elapsed":234,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"7629f1b4-2c8c-4148-eabb-e90bcdd0c22e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 13.8 ms (started: 2022-08-09 19:12:53 +00:00)\n"]}],"source":["surface_df = pd.concat([df_EuCallModel_SL_U, \n","                        df_EuCallModel_SL_LN, \n","                        df_EuCallModel_KL_U, \n","                        df_EuCallModel_KL_LN, \n","                        df_EuCallModel_HL_U, \n","                        df_EuCallModel_HL_LN, \n","                        df_EuCallModel_KLH_U, \n","                        df_EuCallModel_KLH_LN,\n","                        df_EuCallModel_SL_U_ST,\n","                        df_EuCallModel_KL_U_ST,\n","                        df_EuCallModel_SLH_U_ST,\n","                        df_EuCallModel_KLH_U_ST], axis=0)\n"]},{"cell_type":"code","execution_count":77,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZbSVcKSAzo8","executionInfo":{"status":"ok","timestamp":1660072379523,"user_tz":-60,"elapsed":1673,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"}},"outputId":"d292b1c9-ddbf-4895-ce74-2c50cd3cf3f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.46 s (started: 2022-08-09 19:12:57 +00:00)\n"]}],"source":["surface_df.to_csv(f\"{paper_name}_EuCallss.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"V2-EYJ7vAzo8"},"source":["##### R implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC6Kq4GXAzo8"},"outputs":[],"source":["# require(ggplot2)\n","# require(ggthemes)\n","# require(data.table)\n","# require(dplyr)\n","# require(interp)\n","# require(gridExtra)\n","# require(ggpubr)\n","\n","\n","# setwd(\"D:/Code/dgm_lossfunc\")\n","\n","# eucall_data = fread(\"dgm_lossfunctional_EuCallss.csv\")\n","\n","\n","# get_diff_abs_display <- function(data, interval=1.0) {\n","  \n","#   #establish the min and max of scale \n","#   grandmin <- 0.0\n","#   grandmax <- ceiling(max(data$diff_abs))\n","#   #define the number of breaks.  In this case 8 +1 \n","#   mybreaks <- seq(grandmin, grandmax, length.out = grandmax/interval)\n","#   #Function to return the desired number of colors\n","  \n","#   mycolors<- function(x) {\n","#     colors<-colorRampPalette(c(\"darkblue\",\"dodgerblue\",\"green\",\"yellow\",\"orange\",\"darkred\" ))( grandmax/interval )\n","#     colors[1:x]\n","#   }\n","  \n","#   #Function to create labels for legend\n","#   breaklabel <- function(x, breaks = mybreaks){\n","#     labels<- paste0(sapply( breaks[1:(length(breaks)-1)], function(x) {  as.character(round(x,1))   }), \"-\", \n","#                     sapply( breaks[2:length(breaks)], function(x) {  as.character(round(x,1))   }) )\n","#     labels[1:x]\n","#   }\n","  \n","#   return(list(grandmin, grandmax, mybreaks, mycolors, breaklabel))\n","# }\n","\n","# diff_abs_list = get_diff_abs_display(eucall_data)\n","# diff_abs_breaks = diff_abs_list[[3]]\n","# diff_abs_colorf = diff_abs_list[[4]]\n","# diff_abs_lblsf = diff_abs_list[[5]]\n","\n","# # ggplot(sample_n(eucall_data[(Exact>0.0) & (Sample==\"SL_U\")],1000), aes(x=Time, y=S1, z=perc_abs)) + geom_contour()\n","# # ggplot(data, aes(x, y, z = z1)) +\n","# #   geom_contour_filled(breaks= mybreaks, show.legend = TRUE) +\n","# #   scale_fill_manual(palette=mycolors, values=breaklabel(8), name=\"Value\", drop=FALSE) +\n","# #   theme(legend.position = \"right\")\n","\n","# contour_log10_perc_abs <- function(d1, title_str, palette = \"Spectral\") {\n","#   d1$perc_abs = log10(d1$perc_abs)\n","#   grid <- with(d1, interp::interp(Time, S1, perc_abs))\n","#   griddf <- subset(data.frame(Time = rep(grid$x, nrow(grid$z)),\n","#                               S1 = rep(grid$y, each = ncol(grid$z)),\n","#                               perc_abs = as.numeric(grid$z)),\n","#                    !is.na(perc_abs))\n","#   p<- ggplot(griddf, aes(Time, S1, z = perc_abs)) +\n","#     geom_contour_filled(colour = \"white\", show.legend = T) + \n","#     scale_fill_brewer(palette = palette, direction = -1) + \n","#     theme_fivethirtyeight() + ggtitle(title_str)\n","  \n","#   return(p)\n","# }\n","\n","# # d1 = eucall_data[(Exact>0.0) & (Sample==\"SL_U\")]\n","# # title_str = \"European Call, ABS difference - Adam on L2 Loss, Uniform Sampling\"\n","# # palette=diff_abs_colorf(length(diff_abs_breaks))\n","# # breaks=diff_abs_breaks\n","# # breaklabels =  diff_abs_lblsf(length(diff_abs_breaks)-1)\n","\n","# get_legend<-function(myggplot){\n","#   tmp <- ggplot_gtable(ggplot_build(myggplot))\n","#   leg <- which(sapply(tmp$grobs, function(x) x$name) == \"guide-box\")\n","#   legend <- tmp$grobs[[leg]]\n","#   return(legend)\n","# }\n","\n","# contour_diff_abs <- function(d1, title_str, breaks, palette, breaklabels, show.legend = FALSE) {\n","  \n","#   grid <- with(d1, interp::interp(Time, S1, diff_abs))\n","#   griddf <- subset(data.frame(Time = rep(grid$x, nrow(grid$z)),\n","#                               S1 = rep(grid$y, each = ncol(grid$z)),\n","#                               diff_abs = as.numeric(grid$z)),\n","#                    !is.na(diff_abs))\n","  \n","#   p<- ggplot(griddf, aes(Time, S1, z = diff_abs)) +\n","#     geom_contour_filled(colour = \"white\", breaks=breaks, show.legend = show.legend) + \n","#     scale_fill_manual(palette=palette, values=breaklabels, name=\"diff_abs\", drop=FALSE) + \n","#     # scale_fill_brewer(palette = palette, direction = -1) + \n","#     theme_fivethirtyeight() + ggtitle(title_str)\n","#   return(p)\n","# }\n","\n","  \n","# SL_U_Acc_perc_abs = contour_log10_perc_abs(eucall_data[(Exact>0.0) & (Sample==\"SL_U\")], \"European Call, log(10) percentage difference - Adam on L2 Loss, Uniform Sampling\")\n","# SL_LN_Acc_perc_abs = contour_log10_perc_abs(eucall_data[(Exact>0.0) & (Sample==\"SL_LN\")], \"European Call, log(10) percentage difference - Adam on L2 Loss, LN Sampling\")\n","# KL_U_Acc_perc_abs = contour_log10_perc_abs(eucall_data[(Exact>0.0) & (Sample==\"KL_U\")], \"European Call, log(10) percentage difference - Adam on KL(L2) Loss, Uniform Sampling\")\n","# KL_LN_Acc_perc_abs = contour_log10_perc_abs(eucall_data[(Exact>0.0) & (Sample==\"KL_LN\")], \"European Call, log(10) percentage difference - Adam on KL(L2) Loss, LN Sampling\")\n","\n","\n","# SL_U_Acc_diff_abs = contour_diff_abs(eucall_data[(Exact>0.0) & (Sample==\"SL_U\")], \n","#                                      \"European Call, ABS difference - Adam on L2 Loss, Uniform Sampling\", \n","#                                      palette=diff_abs_colorf,\n","#                                      breaks=diff_abs_breaks, breaklabels = diff_abs_lblsf(length(diff_abs_breaks)-1))\n","# SL_LN_Acc_diff_abs = contour_diff_abs(eucall_data[(Exact>0.0) & (Sample==\"SL_LN\")], \"European Call, ABS difference - Adam on L2 Loss, LN Sampling\",\n","#                                       palette=diff_abs_colorf,\n","#                                       breaks=diff_abs_breaks, breaklabels = diff_abs_lblsf(length(diff_abs_breaks)-1))\n","# KL_U_Acc_diff_abs = contour_diff_abs(eucall_data[(Exact>0.0) & (Sample==\"KL_U\")], \"European Call, ABS difference - Adam on KL(L2) Loss, Uniform Sampling\",\n","#                                      palette=diff_abs_colorf,\n","#                                      breaks=diff_abs_breaks, breaklabels = diff_abs_lblsf(length(diff_abs_breaks)-1))\n","# KL_LN_Acc_diff_abs = contour_diff_abs(eucall_data[(Exact>0.0) & (Sample==\"KL_LN\")], \"European Call, ABS difference - Adam on KL(L2) Loss, LN Sampling\",\n","#                                       palette=diff_abs_colorf,\n","#                                       breaks=diff_abs_breaks, breaklabels = diff_abs_lblsf(length(diff_abs_breaks)-1))\n","\n","# diff_abs_legend = get_legend(contour_diff_abs(eucall_data[(Exact>0.0) & (Sample==\"SL_U\")], \n","#                             \"European Call, ABS difference - Adam on L2 Loss, Uniform Sampling\", \n","#                             palette=diff_abs_colorf,\n","#                             breaks=diff_abs_breaks, breaklabels = diff_abs_lblsf(length(diff_abs_breaks)-1), show.legend=T))\n","# blankPlot <- ggplot()+geom_blank(aes(1,1)) + cowplot::theme_nothing()\n","\n","\n","# ggarrange(\n","#   SL_U_Acc_diff_abs, SL_LN_Acc_diff_abs, \n","#   common.legend = TRUE, legend = \"bottom\"\n","# )\n","\n","# ggarrange(\n","#   SL_U_Acc_diff_abs, KL_U_Acc_diff_abs, \n","#   common.legend = TRUE, legend = \"bottom\"\n","# )\n","\n","# ggarrange(\n","#   ggarrange(SL_U_Acc_diff_abs, SL_LN_Acc_diff_abs, ncol = 2),                # First row with line plot\n","#   # Second row with box and dot plots\n","#   ggarrange(KL_U_Acc_diff_abs, KL_LN_Acc_diff_abs, ncol = 2), \n","#   nrow = 2, \n","#   common.legend = TRUE\n","# ) \n","\n","\n","\n","# grid.arrange(diff_abs_legend, blankPlot,  SL_U_Acc_diff_abs, SL_LN_Acc_diff_abs,\n","#              ncol=2, nrow = 2, \n","#              widths = c(2.7, 2.7), heights = c(0.5, 2.5))\n","\n","\n","# grid.arrange(SL_U_Acc_diff_abs, SL_LN_Acc_diff_abs, diff_abs_legend, ncol=3, widths=c(2.3, 2.3, 0.4))\n","\n","# grid.arrange(SL_U_Acc_diff_abs,SL_LN_Acc_diff_abs,KL_U_Acc_diff_abs,KL_LN_Acc_diff_abs, nrow=1, ncol=4)\n","\n","# grid.arrange(SL_U_Acc_diff_abs,SL_LN_Acc_diff_abs,KL_U_Acc_diff_abs, nrow=1, ncol=3)\n","# grid.arrange(SL_U_Acc_diff_abs,SL_LN_Acc_diff_abs,nrow=1,ncol=2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGr9nCCbAzo8"},"outputs":[],"source":["import matplotlib\n","matplotlib.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3hGX14IAzo9"},"outputs":[],"source":["# from matplotlib.mlab import griddata\n","# import matplotlib.pyplot as plt\n","\n","# # First we'll make a regular grid to interpolate onto\n","# data = df_EuCallModel_SL_U.copy()\n","\n","# numcols, numrows = 30, 30\n","# xi = np.linspace(data.Time.min(), data.Time.max(), numcols)\n","# yi = np.linspace(data.S1.min(), data.S1.max(), numrows)\n","# xi, yi = np.meshgrid(xi, yi)\n","\n","# #-- Interpolate at the points in xi, yi\n","# # \"griddata\" expects \"raw\" numpy arrays, so we'll pass in\n","# # data.x.values instead of just the pandas series data.x\n","# x, y, z = data.Time.values, data.S1.values, data.delta.values\n","# zi = griddata(x, y, z, xi, yi)\n","\n","# #-- Display the results\n","# fig, ax = plt.subplots()\n","# im = ax.contourf(xi, yi, zi)\n","# ax.scatter(data.Time, data.S1, c=data.delta, s=100,\n","#            vmin=zi.min(), vmax=zi.max())\n","# fig.colorbar(im)\n","\n","# #  ggplot(df_EuCallModel_SL_U, aes('Time', 'S1', z = 'delta')) + geom_contour()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBCWU5p_AzpA"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"gw9vGp0ZAzpA"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"ynDTNXtzZKU0"},"source":["## Adapted setup using multi-stock Basket European Black-Scholes Option\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lAgwEsfACPRQ"},"source":["### Black-Scholes Closed Form"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1653640764778,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"puYFs0L2NTBs","outputId":"01113758-3708-4277-d927-fd16e038d897"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 7.93 ms (started: 2022-05-27 08:39:24 +00:00)\n"]}],"source":["import torch\n","from torch.distributions import Normal\n","\n","std_norm_cdf = Normal(0, 1).cdf\n","std_norm_pdf = lambda x: torch.exp(Normal(0, 1).log_prob(x))\n","\n","def bs_price(right, K, S, T, sigma, r):\n","    d_1 = (1 / (sigma * torch.sqrt(T))) * (torch.log(S / K) + (r + (torch.square(sigma) / 2)) * T)\n","    d_2 = d_1 - sigma * torch.sqrt(T)\n","    \n","    if right == \"C\":\n","        C = std_norm_cdf(d_1) * S - std_norm_cdf(d_2) * K * torch.exp(-r * T)\n","        return C\n","        \n","    elif right == \"P\":\n","        P = std_norm_cdf(-d_2) * K * torch.exp(-r * T) - std_norm_cdf(-d_1) * S\n","        return P"]},{"cell_type":"markdown","metadata":{"id":"fPi9t3UzCW5Y"},"source":["### Levy Pricing of European Basket Option"]},{"cell_type":"markdown","metadata":{"id":"MY8ZK710qfcc"},"source":["[Levy pricing european basket options](https://github.com/nluciw/pricing_basket_options/blob/main/playground.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1653640766013,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"WYtZp58uqed5","outputId":"9b0042f7-6890-4c08-d1b5-c390649d34a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 26.9 ms (started: 2022-05-27 08:39:25 +00:00)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import itertools\n","from scipy import stats\n","\n","class BasketOption:\n","    '''Functions for a European basket call option.'''\n","    \n","    def __init__(self, weights, prices, vol, corr, strike, time, rate):\n","        '''           \n","        Parameters\n","        ----------\n","        weights : ndarray\n","            Floats representing weights of the underlying assets in the basket. \n","            Should sum to 1, be 1-D, and be of length equal to the length of prices.\n","        prices : ndarray\n","            Floats representing the asset prices at time zero. Should be 1-D, and same\n","            length as prices.\n","        vol : float\n","            The volatility of the assets. N.B. the Levy formula assumes homogeneous asset\n","            volatility.\n","        corr : ndarray\n","            Correlation matrix of the assets. Should be of shape (n,n), where n is the\n","            number of assets.\n","        strike : float\n","            Strike price.\n","        time : float\n","            Time to maturity.\n","        rate : float\n","            Riskless interest rate.\n","        '''\n","        self.weights = weights\n","        self.prices = prices\n","        self.vol = vol\n","        self.corr = corr\n","        self.strike = strike\n","        self.time = time\n","        self.rate = rate\n","    \n","        if not len(weights) == len(prices) == len(corr):\n","            raise ValueError('Number of weights, prices, corr rows should be equal')\n","            \n","        if abs(1-sum(weights))>0.01:\n","            raise ValueError('The weights must cumulatively sum to 1.0')\n","            \n","    def get_levy_price(self):\n","        \"\"\"\n","        Use the Levy formula to approximate the price of a European basket call option.\n","        \"\"\"\n","    \n","        discount = np.exp(-self.rate*self.time)\n","    \n","        # First moment of T-forward prices (also the basket T-forward price)\n","        m1 = np.sum(self.weights * self.prices * discount)\n","\n","        # Second moment of T-forward prices\n","        w_ij, f_ij = [list(map(lambda x: np.product(x), list(itertools.product(q, q)))) \n","                          for q in [self.weights, self.prices * discount]]\n","        m2 = np.sum(np.array(w_ij) * np.array(f_ij)\n","                    * np.exp(self.corr.flatten() * self.vol**2 * self.time))\n","    \n","        vol_basket = ( self.time**(-1) * np.log(m2 / m1**2) )**(0.5)\n","    \n","        # Parameters of the price formula\n","        d1 = np.log(m1 / self.strike)/(vol_basket * self.time**(0.5))\\\n","                + (vol_basket * self.time**(0.5))/2\n","        d2 = d1 - vol_basket * self.time**(0.5)\n","\n","        # Levy formula for basket call option price\n","        self.levy_price = discount * (m1 * stats.norm.cdf(d1) - self.strike * stats.norm.cdf(d2))\n","\n","        return self.levy_price"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1653640766013,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"75zv4y7grGaM","outputId":"6bb08ed9-18fd-47e2-866c-0a48ee9512bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 988 Âµs (started: 2022-05-27 08:39:25 +00:00)\n"]}],"source":["# weights = np.array([0.25,0.25,0.25,0.25])\n","# prices = np.array([100.,100.,100.,100.])\n","# vol = 0.4\n","# corr = np.array([[1.,0.5,0.5,0.5],[0.5,1.,0.5,0.5],[0.5,0.5,1.,0.5],[0.5,0.5,0.5,1.]])\n","# strike = 100.\n","# time = 5\n","# rate = 0.\n","# # levy_formula(weights, prices, vol, corr, strike, time, rate)\n","# basket_option = BasketOption(weights, prices, vol, corr, strike, time, rate)\n","# basket_option.get_levy_price()"]},{"cell_type":"markdown","metadata":{"id":"8IZo1GkJC8Ik"},"source":["#### MutiStockEuropeanOptionNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1653640766013,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"vKPmKPd0C8Im","outputId":"b5be2979-44da-4474-c68a-72037f2cbc99"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 11.6 ms (started: 2022-05-27 08:39:25 +00:00)\n"]}],"source":["class MultiStockEuropeanOptionNet(nn.Module):\n","    def __init__(self , nstocks, NL  , NN, activation = torch.tanh  ):\n","        super(MultiStockEuropeanOptionNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        #  time + Number of stocks\n","        # ( t , xi)\n","        self.Input = 1 + nstocks\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        # pdb.set_trace()\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out \n","    "]},{"cell_type":"markdown","metadata":{"id":"LzFe9rKsCeNT"},"source":["### MultiStockEuropeanBlackScholesCall"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":781,"status":"ok","timestamp":1653640771498,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"F-mOyeQQZKU3","outputId":"fe5e3471-c9f4-4028-e769-94864b7213de"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 543 ms (started: 2022-05-27 08:39:30 +00:00)\n"]}],"source":["from torch.distributions.multivariate_normal import MultivariateNormal\n","\n","class MultiStockEuropeanBlackScholesCall():\n","    \n","    def __init__(self , net, K = 1.0, RHO = 0.5, nstocks = 3):\n","        \n","        self.C = 0.0\n","        self.R = 0.05             # Interest Rate (Yearly)\n","        self.SIGMA = 0.25         # Volatility (Yearly)\n","        self.RHO = RHO            # Stock correlation\n","        self.K = K                # Strike Price & S_zeros\n","        self.T = 2.0              # Maturation time (in YEAR)\n","        self.MAX_X = self.K*3.0   # MAX price\n","        self.prices = torch.tensor([K]).repeat(nstocks)\n","        self.nstocks = nstocks\n","        \n","        ## for accept reject purpose!\n","        ## free boundry problems\n","        self.net = net\n","        # these are the weights on the prices\n","        # self.weights = None\n","        # the settings for KL approx\n","        self.gamma = 1E-4\n","        self.beta = 1E-4\n","\n","        self.cov_matrix = self._get_cov_matrix(nstocks,RHO)  \n","        self.corr_mvn = MultivariateNormal( loc=torch.tensor([0.0]).repeat(nstocks), \n","                                            covariance_matrix=self.cov_matrix )  # assuming std_devs of 1\n","\n","        self.norm_dist = torch.distributions.normal.Normal(loc = torch.tensor([0.0]), scale=torch.tensor([1.0]))\n","\n","    def _get_cov_matrix(self,nstocks,RHO):\n","        cov_matrix = np.ones((nstocks, nstocks))*RHO\n","        np.fill_diagonal(cov_matrix, 1.0)\n","        return torch.tensor(cov_matrix, dtype=torch.float)\n","\n","    def get_levy_price(self, supplied = None):\n","        \"\"\"\n","        Use the Levy formula to approximate the price of a European basket call option.\n","        \"\"\"    \n","\n","        weights_used = torch.tensor([1.0/self.nstocks]).repeat(self.nstocks).numpy()\n","\n","        if supplied is None:\n","          prices_used = self.prices.numpy()\n","          vol_used = self.SIGMA\n","          corr = self.cov_matrix.numpy()\n","          strike = self.K\n","          rate = self.R\n","          time = self.T\n","          # pdb.set_trace()\n","          basket_option = BasketOption(weights_used, prices_used, vol_used, corr, strike, time, rate)\n","          return basket_option.get_levy_price()\n","        else:\n","          basket_option_prices = []\n","          # pdb.set_trace()\n","          for i in range(supplied.shape[0]):\n","            prices_used = supplied[i,1:(1+self.nstocks)].numpy()\n","            vol_used = self.SIGMA\n","            corr = self.cov_matrix.numpy()\n","            strike = self.K\n","            rate = self.R\n","            time = supplied[i,0].item()\n","            # pdb.set_trace()\n","            basket_option = BasketOption(weights_used, prices_used, vol_used, corr, strike, time, rate)\n","            basket_option_prices.append(basket_option.get_levy_price())\n","          return basket_option_prices\n","\n","    def g(self , x):\n","        # pay off function\n","        # return torch.max( (x[:,1].reshape(-1,1)*x[:,2].reshape(-1,1)*x[:,3].reshape(-1,1))**(1/3) - \n","        #                    self.K , \n","        #                    torch.zeros([len(x),1]).cuda() ) #  geometric\n","        return torch.max( torch.sum(x[:,1:(self.nstocks+1)],dim=1)/self.nstocks  - self.K , \n","                          torch.zeros([x.shape[0],]).cuda() ).reshape(-1,1) \n","\n","    def mu(self, x):\n","        ## should test it! output dimension is important !\n","        return (self.R-self.C)*x.reshape(-1,1)\n","\n","    def sigma(self , x):\n","        return self.SIGMA*x.reshape(-1,1)\n","\n","    @staticmethod\n","    def to_device(x, to_cpu):\n","      if to_cpu:\n","        return x.cpu()\n","      else:\n","        return x.cuda()\n","\n","    def sample_correlated_uniform(self, sample_shape):\n","        # corrMvn.rsample(sample_shape=(10,))\n","        correlated_mvn_sample = self.corr_mvn.sample(sample_shape)\n","        return self.norm_dist.cdf(correlated_mvn_sample)\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8, to_cpu = False ):\n","        '''\n","        Sampling function\n","        '''\n","        # 4 samples returned:\n","        # internal, boundary, initial, terminal\n","        if sample_method_X in [\"U\",\"UE3\"]:\n","            # pdb.set_trace()\n","            range_multiplier = 3.0 if sample_method_X == \"UE3\" else 1.0\n","            # internal samples\n","            x_time_sample = torch.rand((int(size),int(1)))*self.T\n","            x = self.to_device(torch.cat(( x_time_sample, \n","                                          -self.MAX_X*range_multiplier*self.sample_correlated_uniform([size,])+self.MAX_X*range_multiplier), dim=1),to_cpu)\n","            ### Terminal time samples\n","            x_terminal = self.to_device(torch.cat( ( torch.zeros(size, 1) + self.T , \n","                                                    -self.MAX_X*range_multiplier*self.sample_correlated_uniform([size, ])+self.MAX_X*range_multiplier ) , dim = 1 ),to_cpu)\n","            ### initial time samples\n","            x_initial = self.to_device(torch.cat( ( torch.zeros(size, 1), self.K*torch.ones( size, self.nstocks)) , dim = 1 ),to_cpu)\n","            ### non-zero\n","            stock_space = self.K*0.8*self.sample_correlated_uniform([size, ]) # if self.is_call else self.K*(1.2 + torch.rand([size, self.nstocks]))\n","            # pdb.set_trace()\n","            x_nonzero = self.to_device(torch.cat( ( torch.rand((size,1))*self.T, stock_space ) , dim = 1 ),to_cpu)\n","            compare = self.net(x_nonzero) \n","            mask = compare < 0\n","            x_nonzero = x_nonzero[mask.reshape(-1),:]\n","            # x_initial = torch.cat( ( torch.zeros(size, 1), -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            return x , x_terminal , x_initial, x_nonzero\n","    \n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","\n","    def criterion(self , x , x_terminal , x_boundary, loss_transform = None):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''        \n","        d = torch.autograd.grad(\n","            self.net(x),\n","            x, \n","            grad_outputs = torch.ones_like(self.net(x)) ,\n","            create_graph = True)\n","        \n","        dt  = d[0][:,0].reshape(-1,1)\n","\n","        # dx1  = d[0][:,1].reshape(-1,1)\n","        # dx2  = d[0][:,2].reshape(-1,1)\n","        # dx3  = d[0][:,3].reshape(-1,1)\n","        \n","        first_derivs_wrt_underlying  = [ d[0][:,i+1].reshape(-1,1) for i in range(self.nstocks) ]\n","        cross_derivatives_wrt_underlying = [ [ torch.autograd.grad(first_derivs_wrt_underlying[i], \n","                                                                   x , \n","                                                                   grad_outputs=torch.ones_like(first_derivs_wrt_underlying[i]) ,\n","                                                                   create_graph = True)[0][:,z+1].reshape(-1,1)\n","                                               for z in range(len(first_derivs_wrt_underlying)) ]  for i in range(len(first_derivs_wrt_underlying)) ]\n","\n","        # second_derivs_wrt_underlying = [ torch.autograd.grad(first_derivs_wrt_underlying[i], \n","        #                                                      x, \n","        #                                                      grad_outputs=torch.ones_like(first_derivs_wrt_underlying[i]) ,\n","        #                                                      create_graph = True)[0][:,i+1].reshape(-1,1)\n","        #                                  for i in range(len(first_derivs_wrt_underlying)) ]\n","\n","        # cross_derivatives_wrt_underlying = [ [ torch.autograd.grad(first_derivs_wrt_underlying[i], \n","        #                                                            x , \n","        #                                                            grad_outputs=torch.ones_like(first_derivs_wrt_underlying[i]) ,\n","        #                                                            create_graph = True)[0][:,z+1].reshape(-1,1)\n","        #                                        for z in [ k for k in range(len(first_derivs_wrt_underlying)) if k!=i  ] ]  for i in range(len(first_derivs_wrt_underlying)) ]\n","        # du/dxdx\n","        # dx1x1 = torch.autograd.grad(dx1, \n","        #                             x , \n","        #                             grad_outputs=torch.ones_like(dx1),\n","        #                             create_graph = True)[0][:,1].reshape(-1,1)\n","        # dx1x2 = torch.autograd.grad(dx1, x , \n","        #                             grad_outputs=torch.ones_like(dx1) ,\n","        #                           create_graph = True)[0][:,2].reshape(-1,1)\n","        # dx1x3 = torch.autograd.grad(dx1, x , \n","        #                             grad_outputs=torch.ones_like(dx1) ,\n","        #                           create_graph = True)[0][:,3].reshape(-1,1)            \n","        # dx2x1 = torch.autograd.grad(dx2, x , grad_outputs=torch.ones_like(dx2) ,\\\n","        #                           create_graph = True)[0][:,1].reshape(-1,1)\n","        # dx2x2 = torch.autograd.grad(dx2, x , grad_outputs=torch.ones_like(dx2) ,\\\n","        #                           create_graph = True)[0][:,2].reshape(-1,1)\n","        # dx2x3 = torch.autograd.grad(dx2, x , grad_outputs=torch.ones_like(dx2) ,\\\n","        #                           create_graph = True)[0][:,3].reshape(-1,1)\n","        # dx3x1 = torch.autograd.grad(dx3, x , grad_outputs=torch.ones_like(dx3) ,\\\n","        #                           create_graph = True)[0][:,1].reshape(-1,1)\n","        # dx3x2 = torch.autograd.grad(dx3, x , grad_outputs=torch.ones_like(dx3) ,\\\n","        #                           create_graph = True)[0][:,2].reshape(-1,1)\n","        # dx3x3 = torch.autograd.grad(dx3, x , grad_outputs=torch.ones_like(dx3) ,\\\n","        #                           create_graph = True)[0][:,3].reshape(-1,1)\n","        \n","        if loss_transform is None:\n","          loss_transform = torch.square\n","\n","        DO = None\n","        TC = None\n","        BC = None\n","\n","        if len(x) == 0:\n","          # print('zero batch size for domain!')\n","          DO = torch.tensor(0).cuda().float()\n","        else:\n","          # x is above the free boundary ( so immediate pay-off is positive )\n","          # above_boundary = dt + self.mu(x[:,1])*( dx1 ) + self.mu(x[:,2])*( dx2 ) + self.mu(x[:,3])*( dx3 ) + \\\n","          #                         0.5*(  (self.sigma(x[:,1])*self.sigma(x[:,1]))*dx1x1  \\\n","          #                     + self.RU*(self.sigma(x[:,1])*self.sigma(x[:,2]))*dx1x2  \\\n","          #                     + self.RU*(self.sigma(x[:,1])*self.sigma(x[:,3]))*dx1x3  \\\n","          #                     + self.RU*(self.sigma(x[:,2])*self.sigma(x[:,1]))*dx2x1  \\\n","          #                     + (self.sigma(x[:,2])*self.sigma(x[:,2]))*dx2x2  \\\n","          #                     + self.RU*(self.sigma(x[:,2])*self.sigma(x[:,3]))*dx2x3  \\\n","          #                     + self.RU*(self.sigma(x[:,3])*self.sigma(x[:,1]))*dx3x1  \\\n","          #                     + self.RU*(self.sigma(x[:,3])*self.sigma(x[:,2]))*dx3x2  \\\n","          #                     + (self.sigma(x[:,3])*self.sigma(x[:,3]))*dx3x3 ) - self.R*self.net(x)\n","\n","          above_xs_first_derivs = torch.zeros([x.shape[0],self.nstocks]).cuda().float()          \n","          for i in range(len(first_derivs_wrt_underlying)):\n","            above_xs_first_derivs[:,i] =  (self.mu(x[:,i+1])*first_derivs_wrt_underlying[i]).reshape(1,-1)\n","          above_xs_first_derivs = torch.sum(above_xs_first_derivs, dim=1)\n","          \n","          above_xs_second_derivs = torch.zeros((x.shape[0],self.nstocks,self.nstocks)).cuda().float()\n","          for i in range(len(first_derivs_wrt_underlying)):\n","            for j in range(len(first_derivs_wrt_underlying)):\n","                if i<=j:\n","                  above_xs_second_derivs[:,i,j] = (0.5*self.cov_matrix[i,j]*self.sigma(x[:,i+1])*self.sigma(x[:,j+1])*cross_derivatives_wrt_underlying[i][j]).reshape(1,-1)          \n","          above_xs_second_derivs = torch.einsum(\"ijk->i\", above_xs_second_derivs)\n","\n","          above_boundary = dt.reshape(-1) + above_xs_first_derivs + above_xs_second_derivs - self.R*self.net(x).reshape(-1)\n","          DO = loss_transform(above_boundary).reshape(-1,1)\n","          # 4.1 in paper - note self.net(x) for u(x,t), difference supposed to be zero\n","          # Domain \n","          #DO = (dt + 0.5*self.RU*(self.SIGMA**2)*(x[:,1].reshape(-1,1)**2)*dxx - self.R*self.net(x) + (self.R-self.C)*x[:,1].reshape(-1,1)*dx)**2\n","\n","        # pdb.set_trace()\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        TC = loss_transform( self.g(x_terminal) - self.net(x_terminal) )\n","\n","        # Boundry Condition - below the payoff value, it should still be worth something for optionality\n","        # len() is safe here , because it just shows batch number \n","        if( len(x_boundary) != 0):\n","            BC = loss_transform( torch.max( self.g(x_boundary) - self.net(x_boundary), torch.zeros([len(x_boundary),1]).cuda() ) )\n","        else:\n","            # print('zero batch size for outside domain!')\n","            BC = torch.tensor(0).cuda().float()\n","        \n","        return  DO , TC , BC\n","\n","    def calculateLoss(self , batch_x, train = True, loss_transform = None, keep_batch = False):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        x , x_terminal , x_boundary, x_nonzero = batch_x  #ignore nonzero for now\n","        x = Variable( x , requires_grad=True )\n","        Ls = self.criterion( x , x_terminal , x_boundary, loss_transform = loss_transform )\n","        DO , TC , BC = Ls\n","\n","        numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","        # DOm = torch.mean(DO).detach().cpu().float().item()\n","        # TCm = torch.mean(TC).detach().cpu().float().item()\n","        # BCm = torch.mean(BC).detach().cpu().float().item()\n","\n","        if not keep_batch:\n","          loss_equalWeightedByType = (1./numActive*torch.mean(DO) + 1./numActive*torch.mean(TC) + 1./numActive*torch.mean(BC))\n","          return  loss_equalWeightedByType , 1./numActive*torch.mean(DO) , 1./numActive*torch.mean(TC) , 1./numActive*torch.mean(BC) , loss_equalWeightedByType             \n","        else:\n","          return DO.numpy(), TC.numpy(), BC.numpy(), DO.numpy(), TC.numpy(), BC.numpy()\n","\n","    def calculateLossUsingKLMinMax(self , batch_x , train = True, loss_transform = None):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        x , x_terminal , x_boundary = batch_x\n","        x = Variable( x, requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , x_boundary, loss_transform = loss_transform)\n","        DO , TC , BC = Ls\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(DO.device)/len(Ls)\n","        \n","        DOt = self.weights[0,0] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * DO)), self.gamma/self.beta) \n","        TCt = self.weights[0,1] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * TC)), self.gamma/self.beta) \n","        BCt = self.weights[0,2] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * BC)), self.gamma/self.beta) \n","\n","        # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n","\n","        numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","        transformed_loss = 1.0/self.gamma * torch.log(DOt + TCt + BCt)\n","        loss_equalWeightedByType = (1./numActive*torch.mean(DO) + 1./numActive*torch.mean(TC) + 1./numActive*torch.mean(BC))\n","        return   transformed_loss, 1./numActive*torch.mean(DO) , 1./numActive*torch.mean(TC) , 1./numActive*torch.mean(BC) , loss_equalWeightedByType\n","\n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = [\"Time\", \"S1\"])\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1653640771499,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"2sHM7qo4Jz6F","outputId":"24385840-e960-4886-9261-50ef138ae95f"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 696 Âµs (started: 2022-05-27 08:39:31 +00:00)\n"]}],"source":["# loss_terms = [ 34.25, 100.12, 23.45]\n","# target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*loss_terms"]},{"cell_type":"markdown","metadata":{"id":"pO55QGFSCkhN"},"source":["### TrainMultiStockEuropeanBlackScholesCall"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":829,"status":"ok","timestamp":1653640772661,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"7nvjizAOZKU5","outputId":"8a829190-f9c3-4a69-d532-ce8d5a0e9f0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 573 ms (started: 2022-05-27 08:39:31 +00:00)\n"]}],"source":["class TrainMultiStockEuropeanBlackScholesCall():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_internal = []\n","        self.history_terminal = []\n","        self.history_initial = []              \n","        self.history_nonzero = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.optimizer_used = optim.Adam\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.monitored_loss_type = \"Train_L2\"\n","        self.early_stop_counter = 0\n","\n","        self.stop_epoch = 0\n","\n","        self.validation_sample = None\n","        self.validation_losses = None\n","        self.train_losses = None        \n","        \n","\n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\", key_loss_func = torch.square, huber_delta = 0.5):\n","        \n","        self.validation_losses = np.ones((epoch, 4), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 4 ), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            # pdb.set_trace()\n","            sample_batch = self.model.sample(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","\n","            losses_L2  = loss_calc_method( sample_batch, loss_transform = torch.square, keep_batch = False )\n","            # pdb.set_trace()\n","            loss , internal , terminal , initial, losses_equalWeightedByType = losses_L2\n","            # loss_abs , internal_abs , terminal_abs , initial_abs, nonzero_abs, losses_equalWeightedByType_abs = losses_ABS\n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal , initial ]))\n","\n","            self.train_losses[e,:] = [ to_cpu_detach(loss) , to_cpu_detach(internal) , to_cpu_detach(terminal) , to_cpu_detach(initial) ]\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation = loss_calc_method( self.validation_sample,\n","                                                      loss_transform = torch.square,\n","                                                      keep_batch = False )\n","              # pdb.set_trace()\n","              # , torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","              validation_loss_list = [x.item() for x in losses_L2_validation]\n","              validation_loss_list = validation_loss_list[0:(len(validation_loss_list)-1)]\n","              self.validation_losses[e,:] = validation_loss_list\n","            \n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","            \n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","\n","                loss_avg = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, max_loss_L2 ))\n","\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                self.history_initial.append( initial )\n","                \n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","                    xinitial_res = self.model.net(xinitial).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    df_initial = self.create_result_df(e, xinitial, xinitial_res, \"INITIAL\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal, df_initial],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal, df_initial],axis=0) ], axis=0)\n","                    # pdb.set_trace()\n","\n","        self.stop_epoch = e\n","\n","    def train_stratified(self , epoch , lr, \n","                         eqLossFn = 'calculateLoss', \n","                         sample_method_X = \"U\", \n","                         key_loss_func = torch.square, \n","                         huber_delta = 0.5\n","                         ):\n","        \n","        self.validation_losses = np.ones((epoch, 4), dtype='float32') * np.nan\n","        self.train_losses = np.ones((epoch, 4), dtype='float32') * np.nan\n","\n","        optimizer = self.optimizer_used(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0.0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            internal_xts_bts, terminal_xts_bts, initial_xts_bts, nonzero_xts_bts = self.model.sample_stratified(sample_method_X = sample_method_X, size=self.BATCH_SIZE)\n","            validation_stratum_losses = None #np.array([])#.reshape(1,self.validation_losses.shape[1])\n","            training_stratum_losses = None # np.array([])#.reshape(1,self.train_losses.shape[1])  \n","            training_value_to_optimize = torch.tensor(0.0,requires_grad=True)           \n","            \n","            # pdb.set_trace()\n","            for stratum_count in range(len(internal_xts_bts)):              \n","              sample_batch = (internal_xts_bts[stratum_count], \n","                              terminal_xts_bts[stratum_count], \n","                              initial_xts_bts[stratum_count], \n","                              nonzero_xts_bts[stratum_count])  \n","\n","              stratum_losses_L2, stratum_losses_ABS = loss_calc_method( sample_batch, loss_transforms = [ key_loss_func, torch.abs ], keep_batch = False )\n","              if np.isnan(stratum_losses_L2[0].detach().cpu().item()):\n","                # pdb.set_trace()\n","                pass\n","            \n","              if training_stratum_losses is not None:\n","                training_stratum_losses = torch.vstack([training_stratum_losses, torch.tensor([*to_cpu_detach(stratum_losses_L2), *to_cpu_detach(stratum_losses_ABS)]) ]) \n","              else:\n","                training_stratum_losses = torch.tensor([*stratum_losses_L2, *stratum_losses_ABS], requires_grad=False) \n","\n","              # pdb.set_trace()  \n","              training_value_to_optimize = training_value_to_optimize + stratum_losses_L2[0]\n","\n","            # pdb.set_trace()              \n","            training_loss_for_epoch = torch.sum(training_stratum_losses,0)\n","            loss = training_value_to_optimize\n","\n","            loss_optimized , internal , terminal , initial, nonzero, losses_equalWeightedByType, \\\n","            loss_abs , internal_abs , terminal_abs , initial_abs, nonzero_abs, losses_equalWeightedByType_abs = training_loss_for_epoch            \n","            max_loss_L2 = torch.max(torch.tensor([internal , terminal , initial, nonzero]))\n","\n","            self.train_losses[e,:] = training_loss_for_epoch.detach().numpy()\n","\n","            if self.debug == True and (self.validation_sample is not None):\n","              losses_L2_validation, losses_ABS_validation, losses_Huber_valiation = \\\n","                loss_calc_method( self.validation_sample, \n","                                  loss_transforms = [ torch.square, torch.abs, partial(huber_loss_zero_target, delta=huber_delta) ], \n","                                  keep_batch = False )\n","              validation_loss = [*to_cpu_detach(losses_L2_validation),\n","                                              *to_cpu_detach(losses_ABS_validation),\n","                                              *to_cpu_detach(losses_Huber_valiation)]\n","              validation_loss = validation_loss.pop(5) # the L2 loss is duplicated at index 1                \n","              self.validation_losses[e,:] = validation_loss\n","\n","            if self.use_early_stop:\n","              loss_to_check = losses_equalWeightedByType\n","              if self.monitored_loss_type == \"Train_L2\":\n","                pass\n","              elif self.monitored_loss_type == \"Train_L1\":             \n","                loss_to_check = losses_equalWeightedByType_abs\n","              elif self.monitored_loss_type == \"Train_MAXL2\":             \n","                loss_to_check = max_loss_L2\n","              if loss_to_check < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = loss_to_check\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, {self.monitored_loss_type}: {loss_to_check} with patience {self.early_stop_patience}\")\n","                break\n","\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss_avg = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  key loss: {} - eqWeighted loss: {} - L1 loss {} - Max Loss {}\".format(e , lr , loss, losses_equalWeightedByType, loss_abs, max_loss_L2 ))\n","                # loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss_avg )\n","                self.history_internal.append( internal )\n","                self.history_terminal.append( terminal )\n","                self.history_initial.append( initial )\n","                self.history_nonzero.append( nonzero )\n","                if self.debug == True and (self.validation_sample is not None):\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xinternal, xterminal, xinitial, xnonzero = self.validation_sample\n","                    xinternal_res = self.model.net(xinternal).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","                    xinitial_res = self.model.net(xinitial).detach()\n","                    xnonzero_res = self.model.net(xnonzero).detach()\n","\n","                    # pdb.set_trace()\n","                    df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    df_initial = self.create_result_df(e, xinitial, xinitial_res, \"INITIAL\")\n","                    df_nonzero = self.create_result_df(e, xnonzero, xnonzero_res, \"NONZERO\")\n","                    \n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_internal, df_terminal, df_initial, df_nonzero],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_internal, df_terminal, df_initial,df_nonzero],axis=0) ], axis=0)\n","\n","        self.stop_epoch = e\n","\n","\n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, xsample, xsample_res, sample_type):\n","      # df_internal = self.create_result_df(e, xinternal, xinternal_res, \"INTERNAL\")\n","      colnames =[\"Time\"]\n","      colnames.extend([f\"S{x+1}\" for x in range(self.model.nstocks)])\n","      df_xsample = pd.DataFrame(xsample.cpu().numpy(), columns = colnames)\n","      df_xsample[\"Epoch\"] = e\n","      df_xsample[\"Sample\"] = sample_type\n","      df_xsample[\"Result\"] = xsample_res.cpu().numpy()\n","      return df_xsample\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CxfUPGzaCnvp"},"source":["### Test Case MultiStockEuropeanBlackScholesCall"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1653573898810,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"LuAtKO80rRMV","outputId":"03770169-f25b-4ccf-8fb2-295fe710043f"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 3, 3])\n"]},{"data":{"text/plain":["tensor([18, 18])"]},"execution_count":90,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["time: 7.94 ms (started: 2022-05-26 14:04:58 +00:00)\n"]}],"source":["# from pprint import pprint as pp\n","# a=torch.tensor([[[1,2,3],[1,2,3],[1,2,3]],[[1,2,3],[1,2,3],[1,2,3]]])\n","# pp(a.shape)\n","# torch.einsum(\"ijk->i\", a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXal0LQdZKU2","outputId":"144f9a72-dbae-484c-9ba8-6a018563f732"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - lr 0.0001 -  key loss: 0.3360136151313782 - eqWeighted loss: 0.3360136151313782 - Max Loss 0.3044297397136688\n","Epoch 499 - lr 0.0001 -  key loss: 0.0004887289251200855 - eqWeighted loss: 0.0004887289251200855 - Max Loss 0.0003886431222781539\n","Epoch 999 - lr 0.0001 -  key loss: 0.0002410904853604734 - eqWeighted loss: 0.0002410904853604734 - Max Loss 0.00021128315711393952\n","Epoch 1499 - lr 0.0001 -  key loss: 0.00014421410742215812 - eqWeighted loss: 0.00014421410742215812 - Max Loss 0.00013139531074557453\n","Epoch 1999 - lr 0.0001 -  key loss: 7.038324838504195e-05 - eqWeighted loss: 7.038324838504195e-05 - Max Loss 5.20106841577217e-05\n","Epoch 2499 - lr 0.0001 -  key loss: 8.544824959244579e-05 - eqWeighted loss: 8.544824959244579e-05 - Max Loss 6.455813127104193e-05\n","Epoch 2999 - lr 0.0001 -  key loss: 5.67001334275119e-05 - eqWeighted loss: 5.67001334275119e-05 - Max Loss 4.263901792000979e-05\n","Epoch 3499 - lr 0.0001 -  key loss: 4.5666194637306035e-05 - eqWeighted loss: 4.5666194637306035e-05 - Max Loss 3.7189776776358485e-05\n","Epoch 3999 - lr 0.0001 -  key loss: 4.256184183759615e-05 - eqWeighted loss: 4.256184183759615e-05 - Max Loss 3.6142209864920005e-05\n","Epoch 4499 - lr 0.0001 -  key loss: 0.00011722256749635562 - eqWeighted loss: 0.00011722256749635562 - Max Loss 0.00010928778647212312\n","Epoch 4999 - lr 0.0001 -  key loss: 4.4097403588239104e-05 - eqWeighted loss: 4.4097403588239104e-05 - Max Loss 3.632840162026696e-05\n","Epoch 5499 - lr 0.0001 -  key loss: 2.889666211558506e-05 - eqWeighted loss: 2.889666211558506e-05 - Max Loss 2.267956551804673e-05\n","Epoch 5999 - lr 0.0001 -  key loss: 1.8568280211184174e-05 - eqWeighted loss: 1.8568280211184174e-05 - Max Loss 1.4321044545795303e-05\n","Epoch 6499 - lr 0.0001 -  key loss: 3.2650172215653583e-05 - eqWeighted loss: 3.2650172215653583e-05 - Max Loss 3.0135450288071297e-05\n","Epoch 6999 - lr 0.0001 -  key loss: 3.067839134018868e-05 - eqWeighted loss: 3.067839134018868e-05 - Max Loss 2.7116650016978383e-05\n","Epoch 7499 - lr 0.0001 -  key loss: 2.9438357159961015e-05 - eqWeighted loss: 2.9438357159961015e-05 - Max Loss 2.626557761686854e-05\n","Epoch 7999 - lr 0.0001 -  key loss: 1.5158533642534167e-05 - eqWeighted loss: 1.5158533642534167e-05 - Max Loss 1.1746529708034359e-05\n","Epoch 8499 - lr 0.0001 -  key loss: 2.6954838176607154e-05 - eqWeighted loss: 2.6954838176607154e-05 - Max Loss 2.3593631340190768e-05\n","Epoch 8999 - lr 0.0001 -  key loss: 2.8933314752066508e-05 - eqWeighted loss: 2.8933314752066508e-05 - Max Loss 2.340258470212575e-05\n","Epoch 9499 - lr 0.0001 -  key loss: 8.401766535826027e-06 - eqWeighted loss: 8.401766535826027e-06 - Max Loss 6.436678631871473e-06\n","Epoch 9999 - lr 0.0001 -  key loss: 8.645177331345621e-06 - eqWeighted loss: 8.645177331345621e-06 - Max Loss 5.755125584983034e-06\n","Epoch 10499 - lr 0.0001 -  key loss: 8.056136721279472e-06 - eqWeighted loss: 8.056136721279472e-06 - Max Loss 6.320773536572233e-06\n","Epoch 10999 - lr 0.0001 -  key loss: 2.9596336389658973e-05 - eqWeighted loss: 2.9596336389658973e-05 - Max Loss 2.3251952370628715e-05\n","Epoch 11499 - lr 0.0001 -  key loss: 2.284271795360837e-05 - eqWeighted loss: 2.284271795360837e-05 - Max Loss 2.0311175831011496e-05\n","Epoch 11999 - lr 0.0001 -  key loss: 9.826087989495136e-06 - eqWeighted loss: 9.826087989495136e-06 - Max Loss 7.876616109570023e-06\n","Epoch 12499 - lr 0.0001 -  key loss: 1.4620612091675866e-05 - eqWeighted loss: 1.4620612091675866e-05 - Max Loss 1.3296885299496353e-05\n","Epoch 12999 - lr 0.0001 -  key loss: 1.4001207091496326e-05 - eqWeighted loss: 1.4001207091496326e-05 - Max Loss 1.0398408448963892e-05\n","Epoch 13499 - lr 0.0001 -  key loss: 2.8526630558189936e-05 - eqWeighted loss: 2.8526630558189936e-05 - Max Loss 2.6917199647868983e-05\n","Epoch 13999 - lr 0.0001 -  key loss: 5.282008714857511e-05 - eqWeighted loss: 5.282008714857511e-05 - Max Loss 4.9882968596648425e-05\n","Epoch 14499 - lr 0.0001 -  key loss: 3.6885328427160857e-06 - eqWeighted loss: 3.6885328427160857e-06 - Max Loss 3.0521900953317527e-06\n","Epoch 14999 - lr 0.0001 -  key loss: 9.357783710584044e-06 - eqWeighted loss: 9.357783710584044e-06 - Max Loss 7.210750027297763e-06\n","Epoch 15499 - lr 0.0001 -  key loss: 5.473482815432362e-06 - eqWeighted loss: 5.473482815432362e-06 - Max Loss 3.769909653783543e-06\n","Epoch 15999 - lr 0.0001 -  key loss: 5.657684596371837e-06 - eqWeighted loss: 5.657684596371837e-06 - Max Loss 4.287654519430362e-06\n","Epoch 16499 - lr 0.0001 -  key loss: 7.852226190152578e-06 - eqWeighted loss: 7.852226190152578e-06 - Max Loss 4.967931999999564e-06\n","Epoch 16999 - lr 0.0001 -  key loss: 8.149097084242385e-06 - eqWeighted loss: 8.149097084242385e-06 - Max Loss 6.181870958243962e-06\n","Epoch 17499 - lr 0.0001 -  key loss: 6.863538146717474e-06 - eqWeighted loss: 6.863538146717474e-06 - Max Loss 5.0128219299949706e-06\n","Epoch 17999 - lr 0.0001 -  key loss: 1.0627471056068316e-05 - eqWeighted loss: 1.0627471056068316e-05 - Max Loss 8.896564395399764e-06\n","Epoch 18499 - lr 0.0001 -  key loss: 7.361086318269372e-06 - eqWeighted loss: 7.361086318269372e-06 - Max Loss 5.933116881351452e-06\n","Epoch 18999 - lr 0.0001 -  key loss: 2.389722976658959e-05 - eqWeighted loss: 2.389722976658959e-05 - Max Loss 2.2256737793213688e-05\n","Epoch 19499 - lr 0.0001 -  key loss: 2.9364962756517343e-05 - eqWeighted loss: 2.9364962756517343e-05 - Max Loss 2.7550944651011378e-05\n","Epoch 19999 - lr 0.0001 -  key loss: 5.125392817717511e-06 - eqWeighted loss: 5.125392817717511e-06 - Max Loss 4.0338263715966605e-06\n","Epoch 20499 - lr 0.0001 -  key loss: 5.433962542156223e-06 - eqWeighted loss: 5.433962542156223e-06 - Max Loss 4.268592419975903e-06\n","Epoch 20999 - lr 0.0001 -  key loss: 8.576504114898853e-06 - eqWeighted loss: 8.576504114898853e-06 - Max Loss 7.135604391805828e-06\n","Epoch 21499 - lr 0.0001 -  key loss: 4.983489816368092e-06 - eqWeighted loss: 4.983489816368092e-06 - Max Loss 3.6216413263900904e-06\n","Epoch 21999 - lr 0.0001 -  key loss: 3.0926767067285255e-06 - eqWeighted loss: 3.0926767067285255e-06 - Max Loss 1.7447431446271366e-06\n","Epoch 22499 - lr 0.0001 -  key loss: 5.2086579671595246e-06 - eqWeighted loss: 5.2086579671595246e-06 - Max Loss 4.0476807043887675e-06\n","Epoch 22999 - lr 0.0001 -  key loss: 3.2802286114019807e-06 - eqWeighted loss: 3.2802286114019807e-06 - Max Loss 2.5294586976087885e-06\n","Epoch 23499 - lr 0.0001 -  key loss: 3.7836009596503573e-06 - eqWeighted loss: 3.7836009596503573e-06 - Max Loss 2.659687652339926e-06\n","Epoch 23999 - lr 0.0001 -  key loss: 7.330128937610425e-06 - eqWeighted loss: 7.330128937610425e-06 - Max Loss 6.1795153669663705e-06\n"]}],"source":["seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","net = MultiStockEuropeanOptionNet( nstocks = 2, NL = 8 , NN = 100 )\n","net.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","msequation = MultiStockEuropeanBlackScholesCall(net, nstocks = 2)\n","train = TrainMultiStockEuropeanBlackScholesCall(net, msequation, BATCH_SIZE = 2**7, debug = True)\n","\n","train.hook_interval = 500\n","train.use_early_stop = True\n","train.early_stop_patience = 2500\n","train.validation_sample = msequation.sample(sample_method_X=\"U\", size=2**7)\n","\n","train.train( epoch =30000 , lr = 0.0001, eqLossFn= 'calculateLoss')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1653586944061,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"28OhoLyhCCtC","outputId":"ec398ebb-057f-4ff8-c867-3167606fbe61"},"outputs":[{"data":{"text/plain":["torch.Size([64, 3])"]},"execution_count":109,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["time: 4.23 ms (started: 2022-05-26 17:42:23 +00:00)\n"]}],"source":["train.validation_sample[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1653643559856,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"Q8tNd8EiHX_t","outputId":"befe27fc-e3ab-4f47-90ef-d295d36dfa68"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.06755492366870504\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<ggplot: (8740211256081)>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["time: 326 ms (started: 2022-05-27 09:25:59 +00:00)\n"]}],"source":["from pprint import pprint as pp\n","\n","pp(train.model.get_levy_price())\n","\n","validation_levy_prices = train.model.get_levy_price(train.validation_sample[0].cpu().detach()) # because only the interior\n","validation_net_prices = train.model.net(train.validation_sample[0]).reshape(-1).cpu().detach().numpy().tolist()\n","\n","# u_internal_sample = torch.cat((internal_sample, mequation.pi_net(internal_sample).reshape(-1,1)), dim=1)\n","# u_net_results = u_net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n","# htx_results = Htx(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist()\n","dataf2 = pd.DataFrame( { 'u_net': validation_net_prices, \n","                         'levy': validation_levy_prices } )\n","ggplot(dataf2, aes(x='u_net', y='levy')) + geom_point()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VOrBJjtZ45Ua"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2836378,"status":"ok","timestamp":1653647308783,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"91rkV6sH4sLr","outputId":"2b5255c8-28ab-4243-940d-d8a2d1c6f428"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - lr 0.0001 -  key loss: 0.2606489956378937 - eqWeighted loss: 0.2606489956378937 - Max Loss 0.25983452796936035\n","Epoch 499 - lr 0.0001 -  key loss: 0.0008281435584649444 - eqWeighted loss: 0.0008281435584649444 - Max Loss 0.0006048369687050581\n","Epoch 999 - lr 0.0001 -  key loss: 0.0003517964214552194 - eqWeighted loss: 0.0003517964214552194 - Max Loss 0.0002896429505199194\n","Epoch 1499 - lr 0.0001 -  key loss: 0.00029672030359506607 - eqWeighted loss: 0.00029672030359506607 - Max Loss 0.0002629828522913158\n","Epoch 1999 - lr 0.0001 -  key loss: 0.0002516436798032373 - eqWeighted loss: 0.0002516436798032373 - Max Loss 0.00022733605874236673\n","Epoch 2499 - lr 0.0001 -  key loss: 0.00018396624363958836 - eqWeighted loss: 0.00018396624363958836 - Max Loss 0.00015689077554270625\n","Epoch 2999 - lr 0.0001 -  key loss: 0.00021041040599811822 - eqWeighted loss: 0.00021041040599811822 - Max Loss 0.00018479421851225197\n","Epoch 3499 - lr 0.0001 -  key loss: 0.00012753262126352638 - eqWeighted loss: 0.00012753262126352638 - Max Loss 0.00010594121704343706\n","Epoch 3999 - lr 0.0001 -  key loss: 9.073666296899319e-05 - eqWeighted loss: 9.073666296899319e-05 - Max Loss 7.205935980891809e-05\n","Epoch 4499 - lr 0.0001 -  key loss: 7.353285764111206e-05 - eqWeighted loss: 7.353285764111206e-05 - Max Loss 5.886673170607537e-05\n","Epoch 4999 - lr 0.0001 -  key loss: 7.492736040148884e-05 - eqWeighted loss: 7.492736040148884e-05 - Max Loss 5.9157824580324814e-05\n","Epoch 5499 - lr 0.0001 -  key loss: 5.000639794161543e-05 - eqWeighted loss: 5.000639794161543e-05 - Max Loss 3.7908357626292855e-05\n","Epoch 5999 - lr 0.0001 -  key loss: 3.911297972081229e-05 - eqWeighted loss: 3.911297972081229e-05 - Max Loss 2.8669601306319237e-05\n","Epoch 6499 - lr 0.0001 -  key loss: 6.0833815950900316e-05 - eqWeighted loss: 6.0833815950900316e-05 - Max Loss 4.7243793233064935e-05\n","Epoch 6999 - lr 0.0001 -  key loss: 3.6305325920693576e-05 - eqWeighted loss: 3.6305325920693576e-05 - Max Loss 2.8377980925142765e-05\n","Epoch 7499 - lr 0.0001 -  key loss: 4.168184023001231e-05 - eqWeighted loss: 4.168184023001231e-05 - Max Loss 3.368382385815494e-05\n","Epoch 7999 - lr 0.0001 -  key loss: 3.244520121370442e-05 - eqWeighted loss: 3.244520121370442e-05 - Max Loss 2.3973621864570305e-05\n","Epoch 8499 - lr 0.0001 -  key loss: 4.473425724427216e-05 - eqWeighted loss: 4.473425724427216e-05 - Max Loss 3.5318240406922996e-05\n","Epoch 8999 - lr 0.0001 -  key loss: 2.2325784811982885e-05 - eqWeighted loss: 2.2325784811982885e-05 - Max Loss 1.752884236339014e-05\n","Epoch 9499 - lr 0.0001 -  key loss: 2.8081514756195247e-05 - eqWeighted loss: 2.8081514756195247e-05 - Max Loss 2.266722367494367e-05\n","Epoch 9999 - lr 0.0001 -  key loss: 3.359544280101545e-05 - eqWeighted loss: 3.359544280101545e-05 - Max Loss 2.82174842141103e-05\n","Epoch 10499 - lr 0.0001 -  key loss: 1.55865382112097e-05 - eqWeighted loss: 1.55865382112097e-05 - Max Loss 1.0895637387875468e-05\n","Epoch 10999 - lr 0.0001 -  key loss: 2.2730415366822854e-05 - eqWeighted loss: 2.2730415366822854e-05 - Max Loss 1.906248326122295e-05\n","Epoch 11499 - lr 0.0001 -  key loss: 1.5978213923517615e-05 - eqWeighted loss: 1.5978213923517615e-05 - Max Loss 1.2493185749917757e-05\n","Epoch 11999 - lr 0.0001 -  key loss: 1.3717932233703323e-05 - eqWeighted loss: 1.3717932233703323e-05 - Max Loss 9.414906344318297e-06\n","Epoch 12499 - lr 0.0001 -  key loss: 9.368905921292026e-06 - eqWeighted loss: 9.368905921292026e-06 - Max Loss 6.631860742345452e-06\n","Epoch 12999 - lr 0.0001 -  key loss: 1.0124939763045404e-05 - eqWeighted loss: 1.0124939763045404e-05 - Max Loss 6.608167950616917e-06\n","Epoch 13499 - lr 0.0001 -  key loss: 8.59122519614175e-06 - eqWeighted loss: 8.59122519614175e-06 - Max Loss 4.916531906928867e-06\n","Epoch 13999 - lr 0.0001 -  key loss: 1.0665718036761973e-05 - eqWeighted loss: 1.0665718036761973e-05 - Max Loss 8.53389337862609e-06\n","Epoch 14499 - lr 0.0001 -  key loss: 1.2469024113670457e-05 - eqWeighted loss: 1.2469024113670457e-05 - Max Loss 9.636398317525163e-06\n","Epoch 14999 - lr 0.0001 -  key loss: 2.3297305233427323e-05 - eqWeighted loss: 2.3297305233427323e-05 - Max Loss 2.0317991584306583e-05\n","Epoch 15499 - lr 0.0001 -  key loss: 8.406002962146886e-06 - eqWeighted loss: 8.406002962146886e-06 - Max Loss 5.291624802339356e-06\n","Epoch 15999 - lr 0.0001 -  key loss: 3.2344723877031356e-05 - eqWeighted loss: 3.2344723877031356e-05 - Max Loss 2.9073466066620313e-05\n","Epoch 16499 - lr 0.0001 -  key loss: 8.158465789165348e-06 - eqWeighted loss: 8.158465789165348e-06 - Max Loss 5.49695596419042e-06\n","Epoch 16999 - lr 0.0001 -  key loss: 2.06073273147922e-05 - eqWeighted loss: 2.06073273147922e-05 - Max Loss 1.8200400518253446e-05\n","Epoch 17499 - lr 0.0001 -  key loss: 6.142749953141902e-06 - eqWeighted loss: 6.142749953141902e-06 - Max Loss 4.017738319816999e-06\n","Epoch 17999 - lr 0.0001 -  key loss: 6.551572369062342e-06 - eqWeighted loss: 6.551572369062342e-06 - Max Loss 4.566179086396005e-06\n","Epoch 18499 - lr 0.0001 -  key loss: 8.805709512671456e-06 - eqWeighted loss: 8.805709512671456e-06 - Max Loss 7.271017238963395e-06\n","Epoch 18999 - lr 0.0001 -  key loss: 4.6870818550814874e-06 - eqWeighted loss: 4.6870818550814874e-06 - Max Loss 3.605385018090601e-06\n","Epoch 19499 - lr 0.0001 -  key loss: 1.4502423255180474e-05 - eqWeighted loss: 1.4502423255180474e-05 - Max Loss 1.0681666026357561e-05\n","Epoch 19999 - lr 0.0001 -  key loss: 6.044173460395541e-06 - eqWeighted loss: 6.044173460395541e-06 - Max Loss 5.00310579809593e-06\n","Epoch 20499 - lr 0.0001 -  key loss: 1.2017240806017071e-05 - eqWeighted loss: 1.2017240806017071e-05 - Max Loss 1.0628458767314442e-05\n","Epoch 20999 - lr 0.0001 -  key loss: 4.493209871725412e-06 - eqWeighted loss: 4.493209871725412e-06 - Max Loss 3.5086477510049008e-06\n","Epoch 21499 - lr 0.0001 -  key loss: 6.127349479356781e-06 - eqWeighted loss: 6.127349479356781e-06 - Max Loss 4.712613645097008e-06\n","Epoch 21999 - lr 0.0001 -  key loss: 6.307925104920287e-06 - eqWeighted loss: 6.307925104920287e-06 - Max Loss 5.620393494609743e-06\n","Epoch 22499 - lr 0.0001 -  key loss: 7.403311883535935e-06 - eqWeighted loss: 7.403311883535935e-06 - Max Loss 6.327004484774079e-06\n","Epoch 22999 - lr 0.0001 -  key loss: 5.803673957416322e-06 - eqWeighted loss: 5.803673957416322e-06 - Max Loss 3.4396061892039143e-06\n","Epoch 23499 - lr 0.0001 -  key loss: 4.067876034241635e-06 - eqWeighted loss: 4.067876034241635e-06 - Max Loss 2.359148311370518e-06\n","Epoch 23999 - lr 0.0001 -  key loss: 5.452584446175024e-06 - eqWeighted loss: 5.452584446175024e-06 - Max Loss 3.8028642848075833e-06\n","Epoch 24499 - lr 0.0001 -  key loss: 7.4521931310300715e-06 - eqWeighted loss: 7.4521931310300715e-06 - Max Loss 6.513844709843397e-06\n","Epoch 24999 - lr 0.0001 -  key loss: 6.321994078462012e-06 - eqWeighted loss: 6.321994078462012e-06 - Max Loss 5.268773293209961e-06\n","Epoch 25499 - lr 0.0001 -  key loss: 4.7253042794181965e-06 - eqWeighted loss: 4.7253042794181965e-06 - Max Loss 3.0237595183280064e-06\n","Epoch 25999 - lr 0.0001 -  key loss: 1.2267571037227754e-05 - eqWeighted loss: 1.2267571037227754e-05 - Max Loss 1.0934691090369597e-05\n","Epoch 26499 - lr 0.0001 -  key loss: 5.737113042414421e-06 - eqWeighted loss: 5.737113042414421e-06 - Max Loss 5.071244686405407e-06\n","Early Stop at epoch 26732, Train_L2: 1.5210073797788937e-05 with patience 2500\n","time: 47min 15s (started: 2022-05-27 09:41:12 +00:00)\n"]}],"source":["seed = 123\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","\n","nunderlying = 5\n","net = MultiStockEuropeanOptionNet( nstocks = nunderlying, NL = 5 , NN = 100 )\n","net.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","msequation = MultiStockEuropeanBlackScholesCall(net, nstocks = nunderlying)\n","train = TrainMultiStockEuropeanBlackScholesCall(net, msequation, BATCH_SIZE = 2**7, debug = True)\n","\n","train.hook_interval = 500\n","train.use_early_stop = True\n","train.early_stop_patience = 2500\n","train.validation_sample = msequation.sample(sample_method_X=\"U\", size=2**7)\n","\n","train.train( epoch =30000 , lr = 0.0001, eqLossFn= 'calculateLoss')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1653643559856,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"0jCQKmQt4sLt","outputId":"befe27fc-e3ab-4f47-90ef-d295d36dfa68"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.06755492366870504\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<ggplot: (8740211256081)>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["time: 326 ms (started: 2022-05-27 09:25:59 +00:00)\n"]}],"source":["from pprint import pprint as pp\n","\n","pp(train.model.get_levy_price())\n","\n","validation_levy_prices = train.model.get_levy_price(train.validation_sample[0].cpu().detach()) # because only the interior\n","validation_net_prices = train.model.net(train.validation_sample[0]).reshape(-1).cpu().detach().numpy().tolist()\n","\n","# u_internal_sample = torch.cat((internal_sample, mequation.pi_net(internal_sample).reshape(-1,1)), dim=1)\n","# u_net_results = u_net(u_internal_sample).detach().cpu().numpy().reshape(-1).tolist()\n","# htx_results = Htx(u_internal_sample).cpu().detach().numpy().reshape(-1).tolist()\n","dataf2 = pd.DataFrame( { 'u_net': validation_net_prices, \n","                         'levy': validation_levy_prices } )\n","ggplot(dataf2, aes(x='u_net', y='levy')) + geom_point()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajgVUw3l9h30"},"outputs":[],"source":["# train.history_surfaces_hooks.Epoch.unique()\n","# # train.history_surfaces_hooks.head(5)\n","# # train.history_surfaces_hooks[train.history_surfaces_hooks.Epoch == 299]\n","# train.history_surfaces_hooks[train.history_surfaces_hooks.Epoch == 299].S1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8rePR_fkCJeZ"},"outputs":[],"source":["# size=10\n","# ndim=3\n","# rho=0.9\n","\n","# from torch.distributions.multivariate_normal import MultivariateNormal\n","# # cov_matrix = np.ones((ndim, ndim))*rho\n","# # np.fill_diagonal(cov_matrix, 1.0)\n","# # corrMvn = MultivariateNormal(loc=torch.tensor([0.0]).repeat(ndim), covariance_matrix=torch.tensor(cov_matrix, dtype=torch.float) )  # assuming std_devs of 1\n","# # # corrMvn.rsample(sample_shape=(10,))\n","# # corrMvn.sample_n(10)\n","\n","# norm_dist = torch.distributions.normal.Normal(loc = torch.tensor([0.0]), scale=torch.tensor([1.0]))\n","# norm_dist.cdf(value= torch.tensor([[0.0, -1.0, 1.0,2.0],[0.0, -1.0, 1.0,2.0]]))\n","# norm_dist.sample((2,10))\n","\n","# [i+j for i in range(3) for j in range(3) if i==j]"]},{"cell_type":"markdown","metadata":{"id":"HwHZ8dR9SCbc"},"source":["## Adapted Heat equation with a control\n","\n","[github.com/pooyasf/DGM](https://github.com/pooyasf/DGM/blob/main/Heat/heat.py)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1653562793359,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"gw6KOqh8H5fv","outputId":"04d95fff-df0c-4b29-d24c-898589d557f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 80.6 ms (started: 2022-05-26 10:59:52 +00:00)\n"]}],"source":["class Heat():\n","    \n","    def __init__(self , net):\n","        \n","        ## for accept reject purpose!\n","        ## free boundry problems\n","        self.net = net\n","        self.weights = None\n","        self.gamma = 0.01\n","        self.beta = 5.0\n","\n","    def sample(self , ts = 0 , te = np.pi , xs = 0 , xe = np.pi , size = 2**8 ):\n","         x = torch.cat(( torch.rand( [size,1] )*te  , torch.rand( [size,1] )*xe  ) , dim = 1 ).cuda()\n","         x_initial = torch.cat(( torch.zeros(size, 1)  , torch.rand( [size,1] )*xe  ) , dim = 1 ).cuda()\n","         x_boundary_start = torch.cat(( torch.rand( [size,1] )*te , torch.zeros(size, 1)  ) , dim = 1 ).cuda()\n","         x_boundary_end = torch.cat(( torch.rand( [size,1] )*te  , torch.zeros(size, 1) + xe  ) , dim = 1 ).cuda()\n","         return x , x_initial , x_boundary_start , x_boundary_end\n","    \n","    def criterion(self , x , x_initial , x_boundary_0 , x_boundary_pi, loss_transforms = None):\n","        \n","        d = torch.autograd.grad(self.net(x), x , grad_outputs=torch.ones_like(self.net(x)) ,\\\n","                                create_graph=True )\n","        dt = d[0][:,0].reshape(-1,1)\n","        dx = d[0][:,1].reshape(-1,1)\n","        # du/dxdx\n","        dxx = torch.autograd.grad(dx, x , grad_outputs=torch.ones_like(dx) ,\\\n","                                  create_graph = True)[0][:,1].reshape(-1,1)\n","        if loss_transforms is None:\n","          loss_transforms = [torch.square]*4\n","\n","        # Domain \n","        DO = loss_transforms[0]( dt - dxx )\n","        # Terminal Condition\n","        IC = loss_transforms[1]( (torch.sin(x_initial[:,1].reshape(-1,1) )) - self.net(x_initial) )\n","        BD_0  = loss_transforms[2]( self.net(x_boundary_0)  - torch.zeros(len(x_boundary_0), 1).cuda() )\n","        BD_pi = loss_transforms[3]( self.net(x_boundary_pi) - torch.zeros(len(x_boundary_pi), 1).cuda() )\n","        return  DO , IC , BD_0 , BD_pi\n","\n","    def calculateLoss(self , size = 2**8 , train = True, loss_transforms = None):\n","        x , x_initial , x_boundary_0 , x_boundary_pi = self.sample(size)\n","        x = Variable( x , requires_grad=True)\n","        DO , IC , BD_0  , BD_pi = self.criterion( x , x_initial , x_boundary_0 , x_boundary_pi , loss_transforms = loss_transforms)\n","        if train == True:\n","            return  torch.mean(DO + IC + BD_0 + BD_pi) , torch.mean( DO ) , torch.mean( IC ) , torch.mean( BD_0 + BD_pi ), torch.mean(DO + IC + BD_0 + BD_pi)  \n","        else:\n","            return  DO , IC , BD_0 , BD_pi\n","    \n","    def exact_solution(self , t , x ):\n","        return np.sin(x)*np.exp(-1*t)\n","\n","    # def calculateLossUsingKLMinMax(self , size = 2**8 , train = True):\n","    #     '''\n","    #     Helper function that Samples and Calculate loss,\n","    #     This is adapted in that it changes the weights on the losses\n","    #     and the distribution of sampling to maximize the loss provided \n","    #     the KL distance of the loss is within positive constraints\n","    #     beta represents the constraints on the weights\n","    #     gamma represents the constraints on the sampling distribution\n","    #     (each representing an upper bound the KL distribution)\n","    #     '''        \n","    #     x , x_initial , x_boundary_0 , x_boundary_pi = self.sample(size)\n","    #     x = Variable( x , requires_grad=True)\n","    #     Ls = self.criterion( x , x_initial , x_boundary_0 , x_boundary_pi )\n","    #     DO , IC , BD_0  , BD_pi = Ls\n","\n","    #     if self.weights is None:\n","    #       self.weights = torch.ones(1,len(Ls)).to(DO.device)/len(Ls)\n","        \n","    #     DOt = self.weights[0,0] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * DO)), self.gamma/self.beta) \n","    #     ICt = self.weights[0,1] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * IC)), self.gamma/self.beta) \n","    #     BD_0t = self.weights[0,2] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * BD_0)), self.gamma/self.beta) \n","    #     BD_pit = self.weights[0,2] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * BD_pi)), self.gamma/self.beta) \n","    #     # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n","    #     # numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","    #     if train == True:\n","    #         return  1.0/self.gamma * torch.log(DOt + ICt + BD_0t + BD_pit) , \\\n","    #                  torch.mean(DO) , \\\n","    #                  torch.mean(IC) , \\\n","    #                  torch.mean(BD_0 + BD_pi) , \\\n","    #                  torch.mean(DO + IC + BD_0 + BD_pi)             \n","    #     else:\n","    #         return  DO , IC , BD_0  , BD_pi, torch.mean(DO + IC + BD_0 + BD_pi)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1653562798988,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"st4YE6eaIKH1","outputId":"b9a2e5a8-fe4b-4b60-d053-0ad7b1a81ba4"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 12.6 ms (started: 2022-05-26 10:59:58 +00:00)\n"]}],"source":["class HeatNet(nn.Module):\n","    \n","    def __init__(self , NL = 2 , NN = 10 ):\n","        super(HeatNet, self).__init__()\n","        \n","        self.NL = NL\n","        self.NN = NN\n","        \n","        self.fc_input = nn.Linear(2,self.NN)\n","        #torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        \n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        #for i, l in enumerate(self.linears):    \n","        #    torch.nn.init.xavier_uniform_(l.weight)\n","        \n","        self.fc_output = nn.Linear(self.NN,1)\n","        #torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = torch.tanh\n","        \n","    def forward(self, x):\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        \n","        return out "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1653562808694,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ChUA9F6iIMX6","outputId":"426999d8-7374-48ae-dfde-5c01f38425fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 4.32 ms (started: 2022-05-26 11:00:08 +00:00)\n"]}],"source":["activation_heat = {}\n","def get_activation_heat(name):\n","    def hook(model, input, output):\n","        activation_heat = output.detach()\n","    return hook\n","\n","#hooks = {}\n","#for name, module in net.named_modules():\n","#    hooks[name] = module.register_forward_hook(get_activation(name))\n","def register_heat_hook(net):\n","    net.fc_input.register_forward_hook(get_activation_heat('fc_input'))\n","    net.linears[0].register_forward_hook(get_activation_heat('linears[0]'))\n","    net.linears[1].register_forward_hook(get_activation_heat('linears[1]'))\n","    net.fc_output.register_forward_hook(get_activation_heat('fc_output'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1653562809544,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"4cQKHGdqFva9","outputId":"0aaaad71-f927-43e8-94a1-709dbbb59775"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 154 ms (started: 2022-05-26 11:00:08 +00:00)\n"]}],"source":["class TrainHeat():\n","\n","    def __init__(self , net , heatequation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = []\n","        self.history_surfaces_hooks = []\n","        self.history_tl = []\n","        self.history_dl = []\n","        self.history_il = []\n","        self.history_bl = []\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = heatequation\n","        self.debug = debug\n","        if self.debug == True:\n","            self.hooks = {}\n","            self.get_all_layers_heat(self.net)\n","    \n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', save_pics = False):\n","        optimizer = optim.Adam(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0\n","\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","\n","        # pdb.set_trace()\n","\n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            loss , Ls1 , Ls2 , Ls3 , losses_equalWeightedByType= loss_calc_method( self.BATCH_SIZE )\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","            if e % 50 == 49:\n","                loss = loss_avg/50\n","                # max_loss = torch.max(Ls1, Ls2, Ls3)\n","                l1_loss, _, _, _, _ = self.model.calculateLoss(self.BATCH_SIZE, loss_transforms = [torch.abs]*4)\n","                print(\"Epoch {} - lr {} - rebalanced loss: {} - original loss: {} - L1 loss {}\".format(e , lr , loss, losses_equalWeightedByType, l1_loss ))\n","                loss_avg = 0\n","                if save_pics:\n","                  #history_validation_de.append( validate_DE()[0] )\n","                  plt.ioff()\n","                  y_range = np.linspace(0, np.pi, 40, dtype=np.float)\n","                  x_range = np.linspace(0, np.pi, 40, dtype=np.float)\n","                  data = np.empty((2,1))\n","                  Z = []\n","                  for _x in x_range:\n","                      data[0] = _x\n","                      for _y in y_range:\n","                          data[1] = _y\n","                          indata = torch.Tensor(data.reshape(1,-1)).cuda()\n","                          Zdata = self.net(indata).detach().cpu().float().item()\n","                          Z.append(Zdata)\n","                  _X, _Y = np.meshgrid(x_range, y_range, indexing='ij')\n","                  Z_surface = np.reshape(Z, (x_range.shape[0], y_range.shape[0]))\n","                  # plot\n","                  fig = plt.figure()\n","                  ax = fig.gca(projection='3d')\n","                  ax.set_zlim([-0.2,1.2])\n","                  # pdb.set_trace()\n","                  ax.plot_surface( _X, _Y, Z_surface,  cmap=cm.YlOrBr_r, edgecolor='black', linewidth=0.0004, antialiased=True)\n","                  ax.set_xlabel(' T ')\n","                  ax.set_ylabel(' X ')\n","                  ax.set_zlabel(' H ')\n","                  #ax.legend(fontsize=8)\n","                  path = f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/{e}.png\"\n","                  plt.savefig(path)\n","                  plt.close(fig)\n","                \n","                ## report detailed loss ##\n","                tl , dl , il , bl, old_l = self.model.calculateLoss( 2**8 )\n","                self.history_tl.append( tl )\n","                self.history_dl.append( dl )\n","                self.history_il.append( il )\n","                self.history_bl.append( bl )\n","                \n","                if self.debug == True:\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","            \n","    def plot_report_heat(self):\n","\n","        fig, ax = plt.subplots(4, 1 ,constrained_layout=True)\n","        ax[0].plot( np.log(self.history_tl) , '-b', label='total')\n","        ax[0].set_title('total')\n","        fig.suptitle('Training Loss', fontsize=10)\n","        ax[1].plot( np.log(self.history_dl) )\n","        ax[1].set_title('diff operator')\n","        ax[2].plot( np.log(self.history_il) )\n","        ax[2].set_title('initial condition')\n","        ax[3].plot( np.log(self.history_bl) )\n","        ax[3].set_title('boundry condition')\n","    \n","    def hook_fn_heat(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers_heat(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn_heat)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn_heat)\n","    \n","    def plot_activation_mean_heat(self):\n","        \n","        if self.debug == False:\n","            print( 'error: debug is off , turn it on and train again ' )\n","        else:\n","            history = np.array(self.history_mean_hooks)\n","            jet= plt.get_cmap('jet')\n","            colors = iter(jet(np.linspace(0,1,10)))\n","            fig, ax = plt.subplots()\n","            for i in range(history.shape[1]):\n","                ax.plot(history[:,i], '--r', label= i , color=next(colors) )\n","            \n","            fig.suptitle('Layers activation mean value', fontsize=10)\n","            leg = ax.legend();\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24132,"status":"ok","timestamp":1651775129889,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"zwVlf21HOlHd","outputId":"489f4297-3702-4c44-8643-d15f099074a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 49 - lr 0.0001 - rebalanced loss: 0.5056189090013504 - original loss: 0.44934314489364624 - L1 loss 0.7790582776069641\n","Epoch 99 - lr 0.0001 - rebalanced loss: 0.4039174920320511 - original loss: 0.3661675453186035 - L1 loss 0.7696151733398438\n","Epoch 149 - lr 0.0001 - rebalanced loss: 0.34158959686756135 - original loss: 0.3103456199169159 - L1 loss 0.7901034951210022\n","Epoch 199 - lr 0.0001 - rebalanced loss: 0.30575873613357546 - original loss: 0.2964978516101837 - L1 loss 0.7794448137283325\n","Epoch 249 - lr 0.0001 - rebalanced loss: 0.2770907700061798 - original loss: 0.2552706003189087 - L1 loss 0.7207360863685608\n","Epoch 299 - lr 0.0001 - rebalanced loss: 0.2581018394231796 - original loss: 0.2408275455236435 - L1 loss 0.7673238515853882\n","Epoch 349 - lr 0.0001 - rebalanced loss: 0.24002969413995742 - original loss: 0.2213743031024933 - L1 loss 0.7521560788154602\n","Epoch 399 - lr 0.0001 - rebalanced loss: 0.2293036785721779 - original loss: 0.22227051854133606 - L1 loss 0.726797342300415\n","Epoch 449 - lr 0.0001 - rebalanced loss: 0.2179550638794899 - original loss: 0.21378183364868164 - L1 loss 0.7336210012435913\n","Epoch 499 - lr 0.0001 - rebalanced loss: 0.2069568467140198 - original loss: 0.2022951990365982 - L1 loss 0.6946772336959839\n","Epoch 549 - lr 0.0001 - rebalanced loss: 0.19811911195516585 - original loss: 0.19458156824111938 - L1 loss 0.6780902147293091\n","Epoch 599 - lr 0.0001 - rebalanced loss: 0.18792833358049393 - original loss: 0.17770826816558838 - L1 loss 0.6778504848480225\n","Epoch 649 - lr 0.0001 - rebalanced loss: 0.17730100572109223 - original loss: 0.17770394682884216 - L1 loss 0.6146889925003052\n","Epoch 699 - lr 0.0001 - rebalanced loss: 0.16577075392007828 - original loss: 0.15746092796325684 - L1 loss 0.5880146026611328\n","Epoch 749 - lr 0.0001 - rebalanced loss: 0.15446688145399093 - original loss: 0.14827537536621094 - L1 loss 0.5801248550415039\n","Epoch 799 - lr 0.0001 - rebalanced loss: 0.14095731407403947 - original loss: 0.13594689965248108 - L1 loss 0.5571008324623108\n","Epoch 849 - lr 0.0001 - rebalanced loss: 0.12995473861694337 - original loss: 0.1355268508195877 - L1 loss 0.5098850131034851\n","Epoch 899 - lr 0.0001 - rebalanced loss: 0.12139612823724746 - original loss: 0.11025641858577728 - L1 loss 0.5151947140693665\n","Epoch 949 - lr 0.0001 - rebalanced loss: 0.11289114311337471 - original loss: 0.11017961800098419 - L1 loss 0.4838600158691406\n","Epoch 999 - lr 0.0001 - rebalanced loss: 0.1048876702785492 - original loss: 0.09702077507972717 - L1 loss 0.4568440914154053\n","Epoch 1049 - lr 0.0001 - rebalanced loss: 0.09646135106682778 - original loss: 0.09240543842315674 - L1 loss 0.4589630365371704\n","Epoch 1099 - lr 0.0001 - rebalanced loss: 0.08945369824767113 - original loss: 0.07998652011156082 - L1 loss 0.43105751276016235\n","Epoch 1149 - lr 0.0001 - rebalanced loss: 0.08325923934578895 - original loss: 0.07751496881246567 - L1 loss 0.4013597369194031\n","Epoch 1199 - lr 0.0001 - rebalanced loss: 0.0766975723206997 - original loss: 0.07664882391691208 - L1 loss 0.3980799913406372\n","Epoch 1249 - lr 0.0001 - rebalanced loss: 0.07146993845701217 - original loss: 0.07220987975597382 - L1 loss 0.3641870319843292\n","Epoch 1299 - lr 0.0001 - rebalanced loss: 0.06499741405248642 - original loss: 0.06318987905979156 - L1 loss 0.354993999004364\n","Epoch 1349 - lr 0.0001 - rebalanced loss: 0.05979657001793384 - original loss: 0.05936694145202637 - L1 loss 0.3393183946609497\n","Epoch 1399 - lr 0.0001 - rebalanced loss: 0.05504580788314342 - original loss: 0.052543289959430695 - L1 loss 0.326452374458313\n","Epoch 1449 - lr 0.0001 - rebalanced loss: 0.050701668858528136 - original loss: 0.04760825261473656 - L1 loss 0.3097505271434784\n","Epoch 1499 - lr 0.0001 - rebalanced loss: 0.04597946345806122 - original loss: 0.04206101596355438 - L1 loss 0.30415356159210205\n","Epoch 1549 - lr 0.0001 - rebalanced loss: 0.04211191646754742 - original loss: 0.04225540906190872 - L1 loss 0.2569305896759033\n","Epoch 1599 - lr 0.0001 - rebalanced loss: 0.038543165028095246 - original loss: 0.033550430089235306 - L1 loss 0.26064372062683105\n","Epoch 1649 - lr 0.0001 - rebalanced loss: 0.035749311819672586 - original loss: 0.03390846028923988 - L1 loss 0.24731232225894928\n","Epoch 1699 - lr 0.0001 - rebalanced loss: 0.03253033298999071 - original loss: 0.03153460845351219 - L1 loss 0.23622708022594452\n","Epoch 1749 - lr 0.0001 - rebalanced loss: 0.02960076056420803 - original loss: 0.02899532951414585 - L1 loss 0.2332746982574463\n","Epoch 1799 - lr 0.0001 - rebalanced loss: 0.026512223929166794 - original loss: 0.02356754243373871 - L1 loss 0.21485458314418793\n","Epoch 1849 - lr 0.0001 - rebalanced loss: 0.024757616743445395 - original loss: 0.022526811808347702 - L1 loss 0.2042771577835083\n","Epoch 1899 - lr 0.0001 - rebalanced loss: 0.022216340973973275 - original loss: 0.02036849409341812 - L1 loss 0.20294281840324402\n","Epoch 1949 - lr 0.0001 - rebalanced loss: 0.020841603316366673 - original loss: 0.017431385815143585 - L1 loss 0.18841111660003662\n","Epoch 1999 - lr 0.0001 - rebalanced loss: 0.018872732128947974 - original loss: 0.0170920230448246 - L1 loss 0.17534053325653076\n","Epoch 2049 - lr 0.0001 - rebalanced loss: 0.018170839343219995 - original loss: 0.015587491914629936 - L1 loss 0.16589736938476562\n","Epoch 2099 - lr 0.0001 - rebalanced loss: 0.016360274311155082 - original loss: 0.015137333422899246 - L1 loss 0.17152263224124908\n","Epoch 2149 - lr 0.0001 - rebalanced loss: 0.015039109475910663 - original loss: 0.014964494854211807 - L1 loss 0.14797046780586243\n","Epoch 2199 - lr 0.0001 - rebalanced loss: 0.013780735936015844 - original loss: 0.013782672584056854 - L1 loss 0.15072330832481384\n","Epoch 2249 - lr 0.0001 - rebalanced loss: 0.013152150586247444 - original loss: 0.012246306985616684 - L1 loss 0.13496437668800354\n","Epoch 2299 - lr 0.0001 - rebalanced loss: 0.012072953842580319 - original loss: 0.010627130046486855 - L1 loss 0.13381680846214294\n","Epoch 2349 - lr 0.0001 - rebalanced loss: 0.011298009175807238 - original loss: 0.010998595505952835 - L1 loss 0.1260148137807846\n","Epoch 2399 - lr 0.0001 - rebalanced loss: 0.01047584915533662 - original loss: 0.011533660814166069 - L1 loss 0.12668725848197937\n","Epoch 2449 - lr 0.0001 - rebalanced loss: 0.009807201614603401 - original loss: 0.00885680690407753 - L1 loss 0.12173746526241302\n","Epoch 2499 - lr 0.0001 - rebalanced loss: 0.009289109157398343 - original loss: 0.010586087591946125 - L1 loss 0.12568499147891998\n","Epoch 2549 - lr 0.0001 - rebalanced loss: 0.008603813862428068 - original loss: 0.00913875363767147 - L1 loss 0.11593471467494965\n","Epoch 2599 - lr 0.0001 - rebalanced loss: 0.00809745696373284 - original loss: 0.006066798232495785 - L1 loss 0.10612640529870987\n","Epoch 2649 - lr 0.0001 - rebalanced loss: 0.007913695005699993 - original loss: 0.007288001012057066 - L1 loss 0.1061723381280899\n","Epoch 2699 - lr 0.0001 - rebalanced loss: 0.007351122135296464 - original loss: 0.00778017146512866 - L1 loss 0.10786367952823639\n","Epoch 2749 - lr 0.0001 - rebalanced loss: 0.006989304265007376 - original loss: 0.005854465998709202 - L1 loss 0.0984637513756752\n","Epoch 2799 - lr 0.0001 - rebalanced loss: 0.006637746803462506 - original loss: 0.006656978279352188 - L1 loss 0.09510477632284164\n","Epoch 2849 - lr 0.0001 - rebalanced loss: 0.006326173301786185 - original loss: 0.006582880392670631 - L1 loss 0.09804211556911469\n","Epoch 2899 - lr 0.0001 - rebalanced loss: 0.006036975001916289 - original loss: 0.00576774450019002 - L1 loss 0.08226225525140762\n","Epoch 2949 - lr 0.0001 - rebalanced loss: 0.0056859796773642305 - original loss: 0.00598550122231245 - L1 loss 0.08950145542621613\n","Epoch 2999 - lr 0.0001 - rebalanced loss: 0.005433527380228042 - original loss: 0.005045317113399506 - L1 loss 0.09079554677009583\n"]}],"source":["heat_net = HeatNet( NL = 2 , NN = 20 )\n","heat_net.to(torch.device(\"cuda:0\"))  \n","\n","## providing sampler with net so it can accept/reject based on net and other criterions\n","heatequation = Heat(heat_net)\n","#register_hook(net)\n","    \n","train = TrainHeat( heat_net , heatequation , BATCH_SIZE = 2**8 , debug = True )\n","#%%\n","train.train( epoch = 3000 , lr = 0.0001, eqLossFn = 'calculateLoss')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26142,"status":"ok","timestamp":1651775156025,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"ur8LGQzbYHpU","outputId":"5feb171c-dde9-4127-aa54-bde735ade141"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 49 - lr 0.0001 - rebalanced loss: 0.10814771756529808 - original loss: 0.30061230063438416 - L1 loss 0.8462585210800171\n","Epoch 99 - lr 0.0001 - rebalanced loss: 0.08895048454403877 - original loss: 0.26801806688308716 - L1 loss 0.8047064542770386\n","Epoch 149 - lr 0.0001 - rebalanced loss: 0.07520867869257927 - original loss: 0.22924689948558807 - L1 loss 0.788983941078186\n","Epoch 199 - lr 0.0001 - rebalanced loss: 0.06680136889219285 - original loss: 0.21298441290855408 - L1 loss 0.7401078939437866\n","Epoch 249 - lr 0.0001 - rebalanced loss: 0.06160075820982456 - original loss: 0.20415103435516357 - L1 loss 0.7423343658447266\n","Epoch 299 - lr 0.0001 - rebalanced loss: 0.05805354759097099 - original loss: 0.2011408656835556 - L1 loss 0.7170853614807129\n","Epoch 349 - lr 0.0001 - rebalanced loss: 0.054812652543187144 - original loss: 0.183389812707901 - L1 loss 0.7001839876174927\n","Epoch 399 - lr 0.0001 - rebalanced loss: 0.0524990826100111 - original loss: 0.18329104781150818 - L1 loss 0.6509301662445068\n","Epoch 449 - lr 0.0001 - rebalanced loss: 0.04876566678285599 - original loss: 0.16935119032859802 - L1 loss 0.6371749639511108\n","Epoch 499 - lr 0.0001 - rebalanced loss: 0.04515889868140221 - original loss: 0.15517184138298035 - L1 loss 0.5972634553909302\n","Epoch 549 - lr 0.0001 - rebalanced loss: 0.04122669167816639 - original loss: 0.13313265144824982 - L1 loss 0.5874568819999695\n","Epoch 599 - lr 0.0001 - rebalanced loss: 0.037281224876642226 - original loss: 0.12965081632137299 - L1 loss 0.5225815176963806\n","Epoch 649 - lr 0.0001 - rebalanced loss: 0.033480986915528775 - original loss: 0.1142527163028717 - L1 loss 0.4734196364879608\n","Epoch 699 - lr 0.0001 - rebalanced loss: 0.02981360152363777 - original loss: 0.09718694537878036 - L1 loss 0.45330992341041565\n","Epoch 749 - lr 0.0001 - rebalanced loss: 0.026731959730386733 - original loss: 0.09840395301580429 - L1 loss 0.46385958790779114\n","Epoch 799 - lr 0.0001 - rebalanced loss: 0.023786567486822605 - original loss: 0.08996600657701492 - L1 loss 0.433682382106781\n","Epoch 849 - lr 0.0001 - rebalanced loss: 0.02142080184072256 - original loss: 0.07746131718158722 - L1 loss 0.4043319821357727\n","Epoch 899 - lr 0.0001 - rebalanced loss: 0.019679036661982535 - original loss: 0.07122773677110672 - L1 loss 0.39871224761009216\n","Epoch 949 - lr 0.0001 - rebalanced loss: 0.018222098238766193 - original loss: 0.06603499501943588 - L1 loss 0.38394904136657715\n","Epoch 999 - lr 0.0001 - rebalanced loss: 0.017020680755376816 - original loss: 0.06311274319887161 - L1 loss 0.36630868911743164\n","Epoch 1049 - lr 0.0001 - rebalanced loss: 0.015836175177246333 - original loss: 0.05839775502681732 - L1 loss 0.3420947194099426\n","Epoch 1099 - lr 0.0001 - rebalanced loss: 0.01447167295962572 - original loss: 0.05452530086040497 - L1 loss 0.34723007678985596\n","Epoch 1149 - lr 0.0001 - rebalanced loss: 0.013299293871968985 - original loss: 0.044376958161592484 - L1 loss 0.3334694504737854\n","Epoch 1199 - lr 0.0001 - rebalanced loss: 0.012440620437264443 - original loss: 0.045125383883714676 - L1 loss 0.30564814805984497\n","Epoch 1249 - lr 0.0001 - rebalanced loss: 0.011596004702150822 - original loss: 0.04022609815001488 - L1 loss 0.29721134901046753\n","Epoch 1299 - lr 0.0001 - rebalanced loss: 0.01047389291226864 - original loss: 0.03876371681690216 - L1 loss 0.28881949186325073\n","Epoch 1349 - lr 0.0001 - rebalanced loss: 0.009706262014806271 - original loss: 0.03241535648703575 - L1 loss 0.2699776887893677\n","Epoch 1399 - lr 0.0001 - rebalanced loss: 0.008919792491942644 - original loss: 0.0323004275560379 - L1 loss 0.2558187246322632\n","Epoch 1449 - lr 0.0001 - rebalanced loss: 0.008212942499667406 - original loss: 0.028983473777770996 - L1 loss 0.24935314059257507\n","Epoch 1499 - lr 0.0001 - rebalanced loss: 0.007401190781965852 - original loss: 0.02304942160844803 - L1 loss 0.21641814708709717\n","Epoch 1549 - lr 0.0001 - rebalanced loss: 0.006749162403866649 - original loss: 0.023271311074495316 - L1 loss 0.22247382998466492\n","Epoch 1599 - lr 0.0001 - rebalanced loss: 0.006123353587463498 - original loss: 0.021930117160081863 - L1 loss 0.2054690420627594\n","Epoch 1649 - lr 0.0001 - rebalanced loss: 0.005595287457108498 - original loss: 0.02091747708618641 - L1 loss 0.19327983260154724\n","Epoch 1699 - lr 0.0001 - rebalanced loss: 0.005023113619536161 - original loss: 0.01626649498939514 - L1 loss 0.1850455105304718\n","Epoch 1749 - lr 0.0001 - rebalanced loss: 0.004523412398993969 - original loss: 0.017217492684721947 - L1 loss 0.17711646854877472\n","Epoch 1799 - lr 0.0001 - rebalanced loss: 0.004155548480339348 - original loss: 0.014014956541359425 - L1 loss 0.16818790137767792\n","Epoch 1849 - lr 0.0001 - rebalanced loss: 0.0037626502104103564 - original loss: 0.017206307500600815 - L1 loss 0.15697240829467773\n","Epoch 1899 - lr 0.0001 - rebalanced loss: 0.0033881082898005845 - original loss: 0.011868152767419815 - L1 loss 0.14540192484855652\n","Epoch 1949 - lr 0.0001 - rebalanced loss: 0.0030998697830364108 - original loss: 0.010472684167325497 - L1 loss 0.1453448385000229\n","Epoch 1999 - lr 0.0001 - rebalanced loss: 0.0027703850250691176 - original loss: 0.009935731068253517 - L1 loss 0.12647408246994019\n","Epoch 2049 - lr 0.0001 - rebalanced loss: 0.002468078045640141 - original loss: 0.008488485589623451 - L1 loss 0.12130673229694366\n","Epoch 2099 - lr 0.0001 - rebalanced loss: 0.002275202313903719 - original loss: 0.00885841529816389 - L1 loss 0.11123862862586975\n","Epoch 2149 - lr 0.0001 - rebalanced loss: 0.0019807617622427643 - original loss: 0.0062600672245025635 - L1 loss 0.10980360954999924\n","Epoch 2199 - lr 0.0001 - rebalanced loss: 0.001861793037969619 - original loss: 0.005916112568229437 - L1 loss 0.11032399535179138\n","Epoch 2249 - lr 0.0001 - rebalanced loss: 0.001706108630169183 - original loss: 0.005664770025759935 - L1 loss 0.09972051531076431\n","Epoch 2299 - lr 0.0001 - rebalanced loss: 0.0015749805234372615 - original loss: 0.005510237067937851 - L1 loss 0.09630168974399567\n","Epoch 2349 - lr 0.0001 - rebalanced loss: 0.001446474848780781 - original loss: 0.004297347739338875 - L1 loss 0.08720967173576355\n","Epoch 2399 - lr 0.0001 - rebalanced loss: 0.0013122471235692502 - original loss: 0.004819908179342747 - L1 loss 0.08727578818798065\n","Epoch 2449 - lr 0.0001 - rebalanced loss: 0.0012199803069233893 - original loss: 0.004933031275868416 - L1 loss 0.07277269661426544\n","Epoch 2499 - lr 0.0001 - rebalanced loss: 0.0011649063136428594 - original loss: 0.005382643546909094 - L1 loss 0.07926073670387268\n","Epoch 2549 - lr 0.0001 - rebalanced loss: 0.0010802686237730086 - original loss: 0.0042724451050162315 - L1 loss 0.07267040759325027\n","Epoch 2599 - lr 0.0001 - rebalanced loss: 0.0009696435963269323 - original loss: 0.0035537690855562687 - L1 loss 0.06802944839000702\n","Epoch 2649 - lr 0.0001 - rebalanced loss: 0.0008988339477218688 - original loss: 0.0031835553236305714 - L1 loss 0.07346281409263611\n","Epoch 2699 - lr 0.0001 - rebalanced loss: 0.0008470975735690445 - original loss: 0.003031018190085888 - L1 loss 0.06407620012760162\n","Epoch 2749 - lr 0.0001 - rebalanced loss: 0.000787970257224515 - original loss: 0.0032533640041947365 - L1 loss 0.06710341572761536\n","Epoch 2799 - lr 0.0001 - rebalanced loss: 0.0007748573378194123 - original loss: 0.0030983942560851574 - L1 loss 0.06276990473270416\n","Epoch 2849 - lr 0.0001 - rebalanced loss: 0.0007126305613201112 - original loss: 0.002338071819394827 - L1 loss 0.06466719508171082\n","Epoch 2899 - lr 0.0001 - rebalanced loss: 0.0006837821239605546 - original loss: 0.002544730668887496 - L1 loss 0.05819818750023842\n","Epoch 2949 - lr 0.0001 - rebalanced loss: 0.0006575562502257526 - original loss: 0.0025645759887993336 - L1 loss 0.057425618171691895\n","Epoch 2999 - lr 0.0001 - rebalanced loss: 0.0006153564481064677 - original loss: 0.0025562536902725697 - L1 loss 0.055918797850608826\n"]}],"source":["heat_net2 = HeatNet( NL = 2 , NN = 20 )\n","heat_net2.to(torch.device(\"cuda:0\"))  \n","\n","## providing sampler with net so it can accept/reject based on net and other criterions\n","heatequation2 = Heat(heat_net2)\n","#register_hook(net)\n","    \n","train2 = TrainHeat( heat_net2 , heatequation2 , BATCH_SIZE = 2**8 , debug = True )\n","#%%\n","train2.train( epoch = 3000 , lr = 0.0001, eqLossFn = 'calculateLossUsingKLMinMax')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21708,"status":"ok","timestamp":1651574805335,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"iaQQ8y5MZpFG","outputId":"1aace60a-8a42-4552-8ed0-14f0279eda01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 49 - lr 0.0001 - rebalanced loss: 0.20206966787576675 - original loss: 0.4127390384674072\n","Epoch 99 - lr 0.0001 - rebalanced loss: 0.15180848747491837 - original loss: 0.39203816652297974\n","Epoch 149 - lr 0.0001 - rebalanced loss: 0.14186561584472657 - original loss: 0.36039239168167114\n","Epoch 199 - lr 0.0001 - rebalanced loss: 0.13009952276945114 - original loss: 0.3376663625240326\n","Epoch 249 - lr 0.0001 - rebalanced loss: 0.12217730835080147 - original loss: 0.3325193524360657\n","Epoch 299 - lr 0.0001 - rebalanced loss: 0.11404094442725182 - original loss: 0.29036784172058105\n","Epoch 349 - lr 0.0001 - rebalanced loss: 0.10581339374184609 - original loss: 0.2698376476764679\n","Epoch 399 - lr 0.0001 - rebalanced loss: 0.10039236679673195 - original loss: 0.26930856704711914\n","Epoch 449 - lr 0.0001 - rebalanced loss: 0.0958828592300415 - original loss: 0.25296616554260254\n","Epoch 499 - lr 0.0001 - rebalanced loss: 0.09149332016706467 - original loss: 0.21302446722984314\n","Epoch 549 - lr 0.0001 - rebalanced loss: 0.0859213899075985 - original loss: 0.2258697748184204\n","Epoch 599 - lr 0.0001 - rebalanced loss: 0.08314175099134445 - original loss: 0.2250451147556305\n","Epoch 649 - lr 0.0001 - rebalanced loss: 0.08046467423439026 - original loss: 0.22679656744003296\n","Epoch 699 - lr 0.0001 - rebalanced loss: 0.07692034214735032 - original loss: 0.22816205024719238\n","Epoch 749 - lr 0.0001 - rebalanced loss: 0.075544613301754 - original loss: 0.2152230441570282\n","Epoch 799 - lr 0.0001 - rebalanced loss: 0.07297830983996391 - original loss: 0.21845437586307526\n","Epoch 849 - lr 0.0001 - rebalanced loss: 0.07031019672751426 - original loss: 0.1986459493637085\n","Epoch 899 - lr 0.0001 - rebalanced loss: 0.06775181606411934 - original loss: 0.18811431527137756\n","Epoch 949 - lr 0.0001 - rebalanced loss: 0.06512237593531608 - original loss: 0.18129758536815643\n","Epoch 999 - lr 0.0001 - rebalanced loss: 0.06207448393106461 - original loss: 0.1883564293384552\n","Epoch 1049 - lr 0.0001 - rebalanced loss: 0.058965288400650025 - original loss: 0.17638248205184937\n","Epoch 1099 - lr 0.0001 - rebalanced loss: 0.056310470998287204 - original loss: 0.1704312562942505\n","Epoch 1149 - lr 0.0001 - rebalanced loss: 0.05342719562351703 - original loss: 0.15785589814186096\n","Epoch 1199 - lr 0.0001 - rebalanced loss: 0.050233957916498186 - original loss: 0.15628792345523834\n","Epoch 1249 - lr 0.0001 - rebalanced loss: 0.047175794765353204 - original loss: 0.14273883402347565\n","Epoch 1299 - lr 0.0001 - rebalanced loss: 0.04455537438392639 - original loss: 0.13323470950126648\n","Epoch 1349 - lr 0.0001 - rebalanced loss: 0.04299080483615399 - original loss: 0.12079225480556488\n","Epoch 1399 - lr 0.0001 - rebalanced loss: 0.04047057509422302 - original loss: 0.11933024227619171\n","Epoch 1449 - lr 0.0001 - rebalanced loss: 0.03831795297563076 - original loss: 0.12952660024166107\n","Epoch 1499 - lr 0.0001 - rebalanced loss: 0.03692907102406025 - original loss: 0.11228492856025696\n","Epoch 1549 - lr 0.0001 - rebalanced loss: 0.035360948964953426 - original loss: 0.1183505728840828\n","Epoch 1599 - lr 0.0001 - rebalanced loss: 0.03448030512779951 - original loss: 0.10274899005889893\n","Epoch 1649 - lr 0.0001 - rebalanced loss: 0.03289066195487976 - original loss: 0.10242880880832672\n","Epoch 1699 - lr 0.0001 - rebalanced loss: 0.03224850814789534 - original loss: 0.0971694067120552\n","Epoch 1749 - lr 0.0001 - rebalanced loss: 0.03099956028163433 - original loss: 0.09384853392839432\n","Epoch 1799 - lr 0.0001 - rebalanced loss: 0.029588787481188775 - original loss: 0.091018907725811\n","Epoch 1849 - lr 0.0001 - rebalanced loss: 0.02904393248260021 - original loss: 0.10475724935531616\n","Epoch 1899 - lr 0.0001 - rebalanced loss: 0.028096278682351112 - original loss: 0.09449243545532227\n","Epoch 1949 - lr 0.0001 - rebalanced loss: 0.02699567172676325 - original loss: 0.08842877298593521\n","Epoch 1999 - lr 0.0001 - rebalanced loss: 0.02624620582908392 - original loss: 0.09351842105388641\n","Epoch 2049 - lr 0.0001 - rebalanced loss: 0.02513965968042612 - original loss: 0.08364633470773697\n","Epoch 2099 - lr 0.0001 - rebalanced loss: 0.02457402363419533 - original loss: 0.0839538425207138\n","Epoch 2149 - lr 0.0001 - rebalanced loss: 0.023818908296525478 - original loss: 0.08924302458763123\n","Epoch 2199 - lr 0.0001 - rebalanced loss: 0.022981923669576645 - original loss: 0.07518559694290161\n","Epoch 2249 - lr 0.0001 - rebalanced loss: 0.021854364946484565 - original loss: 0.07917020469903946\n","Epoch 2299 - lr 0.0001 - rebalanced loss: 0.021163343749940396 - original loss: 0.06986261904239655\n","Epoch 2349 - lr 0.0001 - rebalanced loss: 0.020384734496474266 - original loss: 0.06868208199739456\n","Epoch 2399 - lr 0.0001 - rebalanced loss: 0.01999039847403765 - original loss: 0.07055466622114182\n","Epoch 2449 - lr 0.0001 - rebalanced loss: 0.01909452836960554 - original loss: 0.06390845775604248\n","Epoch 2499 - lr 0.0001 - rebalanced loss: 0.0184057442471385 - original loss: 0.06301697343587875\n","Epoch 2549 - lr 0.0001 - rebalanced loss: 0.017760439999401568 - original loss: 0.059466518461704254\n","Epoch 2599 - lr 0.0001 - rebalanced loss: 0.017057142443954943 - original loss: 0.05294445902109146\n","Epoch 2649 - lr 0.0001 - rebalanced loss: 0.016312105525285007 - original loss: 0.05496958643198013\n","Epoch 2699 - lr 0.0001 - rebalanced loss: 0.015724770091474057 - original loss: 0.053481325507164\n","Epoch 2749 - lr 0.0001 - rebalanced loss: 0.014961756579577924 - original loss: 0.05137065052986145\n","Epoch 2799 - lr 0.0001 - rebalanced loss: 0.014149309024214745 - original loss: 0.04468864947557449\n","Epoch 2849 - lr 0.0001 - rebalanced loss: 0.013232466205954552 - original loss: 0.045050181448459625\n","Epoch 2899 - lr 0.0001 - rebalanced loss: 0.012685660347342491 - original loss: 0.04003860801458359\n","Epoch 2949 - lr 0.0001 - rebalanced loss: 0.01197603030130267 - original loss: 0.039011839777231216\n","Epoch 2999 - lr 0.0001 - rebalanced loss: 0.011393368542194367 - original loss: 0.03389868885278702\n"]}],"source":["heat_net2 = HeatNet( NL = 2 , NN = 20 )\n","heat_net2.to(torch.device(\"cuda:0\"))  \n","\n","## providing sampler with net so it can accept/reject based on net and other criterions\n","heatequation2 = Heat(heat_net2)\n","\n","#register_hook(net)\n","    \n","train2 = TrainHeat( heat_net2 , heatequation2 , BATCH_SIZE = 2**8 , debug = True )\n","#%%\n","train2.train( epoch = 3000 , lr = 0.0001, eqLossFn = 'calculateLossUsingKLMinMax')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7kAyb9IgfgVm"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7VFywSUfniu"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"D7d5U4kTYsbp"},"source":["## Fast American Option Pricing - Andersen"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1653469988335,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"yisHV_dRYw41","outputId":"2d288aab-023f-4297-d621-afb19abdd6d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 792 Âµs (started: 2022-05-25 09:13:07 +00:00)\n"]}],"source":["sys.path.insert(0, '/content/drive/MyDrive/FastAmericanOptionPricing-master/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2520,"status":"ok","timestamp":1653469990851,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"CHGCM4--ahmu","outputId":"ecc843c8-9cb0-44f0-c739-f0c843efb264"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 2.45 s (started: 2022-05-25 09:13:07 +00:00)\n"]}],"source":["from FastAmericanOptionSolverA import *\n","from multiprocessing import Pool"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5255,"status":"ok","timestamp":1653469996103,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"jUH0NyEFceNK","outputId":"5a734cc3-cded-478f-8892-e48e3ba4c89e"},"outputs":[{"name":"stdout","output_type":"stream","text":["step 1. checking collocation points ...\n","\n","collocation point = [3.00000000e+00 2.89864827e+00 2.61153811e+00 2.18566017e+00\n"," 1.68750000e+00 1.18846904e+00 7.50000000e-01 4.12011906e-01\n"," 1.87500000e-01 6.43398282e-02 1.34618943e-02 8.70786986e-04\n"," 0.00000000e+00]\n","\n","step 3. checking numerical integration ...\n","\n","analytical sol = 8102.083927575384, numerical sol = 8102.083927575392, err = 8.185452315956354e-12\n","\n","step 4. checking QD+ alogrithm ...\n","\n","B guess = [72.02819841893911, 72.2261530824679, 72.83080955080466, 73.8730793788191, 75.39919417881895, 77.46136253500606, 80.10183400374555, 83.32751539766585, 87.07195514307985, 91.14275103011242, 95.15621577013005, 98.46286556928261, 100.0]\n","\n","tau  = [3.00000000e+00 2.89864827e+00 2.61153811e+00 2.18566017e+00\n"," 1.68750000e+00 1.18846904e+00 7.50000000e-01 4.12011906e-01\n"," 1.87500000e-01 6.43398282e-02 1.34618943e-02 8.70786986e-04\n"," 0.00000000e+00]\n","\n","step 5. starting iteration ...\n","\n","  iter = 1, err = 1.3186247834719795\n","\n","match condition err2 = 1.9903057466891725\n","\n","  iter = 2, err = 0.7062460809015453\n","\n","match condition err2 = 1.0262872984617784\n","\n","  iter = 3, err = 0.37089396042139516\n","\n","match condition err2 = 0.5258649106906795\n","\n","  iter = 4, err = 0.19211657386898132\n","\n","match condition err2 = 0.26826469932517066\n","\n","  iter = 5, err = 0.09861131864179534\n","\n","match condition err2 = 0.13643130963996175\n","\n","  iter = 6, err = 0.050319719026457356\n","\n","match condition err2 = 0.06922900985525705\n","\n","  iter = 7, err = 0.02557888030451589\n","\n","match condition err2 = 0.03506704155119937\n","\n","  iter = 8, err = 0.012968140282316866\n","\n","match condition err2 = 0.01773652887949202\n","\n","  iter = 9, err = 0.006561805130864844\n","\n","match condition err2 = 0.008959127502309882\n","\n","  iter = 10, err = 0.0033150121888113047\n","\n","match condition err2 = 0.00451992624245176\n","\n","  iter = 11, err = 0.001672469245050971\n","\n","match condition err2 = 0.0022776799606250503\n","\n","  iter = 12, err = 0.0008427517405476704\n","\n","match condition err2 = 0.0011464933882816282\n","\n","  iter = 13, err = 0.00042417841757181983\n","\n","match condition err2 = 0.000576486708550855\n","\n","  iter = 14, err = 0.00021327323809685038\n","\n","match condition err2 = 0.0002895810337491534\n","\n","  iter = 15, err = 0.00010712568036317391\n","\n","match condition err2 = 0.0001453259565909075\n","\n","  iter = 16, err = 5.3759328587319834e-05\n","\n","match condition err2 = 7.286927580936147e-05\n","\n","  iter = 17, err = 2.695605602371572e-05\n","\n","match condition err2 = 3.651052208747026e-05\n","\n","  iter = 18, err = 1.3506661508519939e-05\n","\n","match condition err2 = 1.828174590987541e-05\n","\n","  iter = 19, err = 6.763708523130526e-06\n","\n","match condition err2 = 9.149720642519391e-06\n","\n","step 6. checking exercise boundary ...\n","\n","exercise boundary = [71.22506018453315, 71.41101433903506, 71.98163548261508, 72.97416427589346, 74.44641096038245, 76.46821173125346, 79.10435155297465, 82.38602159206832, 86.26375977326944, 90.54646306103925, 94.8155355192325, 98.36075828886447, 100.0]\n","\n","match condition err = 9.149720642519391e-06\n","\n","time: 5.47 s (started: 2022-05-25 09:13:10 +00:00)\n"]}],"source":["# unit test one for valuing American option\n","r = 0.04     # risk free\n","q = 0.01      # dividend yield\n","K = 100.0       # strike\n","S = 80.0        # underlying spot\n","sigma = 0.2  # volatility\n","T = 3.0         # maturity\n","option_type = qd.OptionType.Put\n","\n","solver = FastAmericanOptionSolverA(r, q, sigma, K, T, option_type)\n","solver.use_derivative = False\n","solver.iter_tol = 1e-5\n","solver.max_iters = 20\n","price = solver.solve(0.0, S)   # t and S\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1653469996103,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"8reUojo0c5pr","outputId":"39edc33a-7a43-4038-b1fe-997aa61730ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["european price =  17.78865238332743\n","american price =  21.135486471314067\n","time: 1.57 ms (started: 2022-05-25 09:13:15 +00:00)\n"]}],"source":["print(\"european price = \", solver.european_price)\n","print(\"american price = \", price)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1653469997246,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"Yw0WRP2bc6y8","outputId":"695186ad-2a75-40ea-c94d-b7691cbc45b4"},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["time: 838 ms (started: 2022-05-25 09:13:15 +00:00)\n"]}],"source":["#make a plot for exercise boundary\n","plt.plot(solver.shared_tau, solver.shared_B, 'o-')\n","plt.plot(solver.shared_tau, solver.shared_B0, 'r--')\n","plt.legend([\"real exercise boundary\", \"initial guess\"])\n","plt.xlabel(\"Time to maturity tau\")\n","plt.ylabel(\"Exercise boundary [$]\")\n","plt.show()\n","\n","plt.figure(2)\n","iters = np.array([float(x[0]) for x in solver.iter_records])\n","errors = np.array([x[1] for x in solver.iter_records])\n","plt.loglog(iters, errors, 'o-')\n","plt.xlabel(\"Number of iterations\")\n","plt.ylabel(\"Match condition error\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1653469997247,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"MdzXgZNGdQfz","outputId":"34ca146e-6407-4029-8425-fcd8695e3745"},"outputs":[{"name":"stdout","output_type":"stream","text":["call =  22.014186418796882 , put =  22.014186418796903\n","time: 4.75 ms (started: 2022-05-25 09:13:16 +00:00)\n"]}],"source":["from EuropeanOptionSolver import EuropeanOption\n","\n","r = 0.04  # risk free\n","q = 0.04  # dividend yield\n","K = 100  # strike\n","S0 = 80  # underlying spot\n","sigma = 0.2  # volatility\n","T = 3.0  # maturity\n","put = EuropeanOption.european_put_value(T, S0, r, q, sigma, K)\n","call =  EuropeanOption.european_call_value(T, K, q, r, sigma , S0)\n","print(\"call = \", call, \", put = \", put)"]},{"cell_type":"markdown","metadata":{"id":"N8FXnhHAfgs6"},"source":["## Single Stock American Call option - sampling methodology\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1fqY3wltfgs8"},"source":["#### AmericanOptionNet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1653392412514,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"dKNWODm3fgs9","outputId":"060d9aad-cc40-499f-d99e-0a782baf763e"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 12.7 ms (started: 2022-05-24 11:40:11 +00:00)\n"]}],"source":["class AmericanOptionNet(nn.Module):\n","    def __init__(self , NL  , NN, activation = torch.tanh  ):\n","        super(AmericanOptionNet, self).__init__()\n","        self.NL = NL\n","        self.NN = NN\n","        ### Number of stocks + time\n","        ### ( t , xi)\n","        self.Input = 1 + 1\n","        self.fc_input = nn.Linear(self.Input,self.NN)\n","        torch.nn.init.xavier_uniform_(self.fc_input.weight)\n","        self.linears = nn.ModuleList([nn.Linear(self.NN, self.NN) for i in range(self.NL)])\n","        for i, l in enumerate(self.linears):    \n","            torch.nn.init.xavier_uniform_(l.weight)\n","        self.fc_output = nn.Linear(self.NN,1)\n","        torch.nn.init.xavier_uniform_(self.fc_output.weight)\n","        self.act = activation\n","        \n","    def forward(self, x):\n","        # pdb.set_trace()\n","        h = self.act( self.fc_input(x)  )\n","        for i, l in enumerate(self.linears):\n","            h = self.act( l(h) )\n","        out = self.fc_output(h)\n","        return out \n","    "]},{"cell_type":"markdown","metadata":{"id":"riU7CRY1fgs_"},"source":["#### AmericanBlackScholesSingleStock"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":536,"status":"ok","timestamp":1653392413572,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"LUiTvWcFfgtA","outputId":"7a423172-b58c-4efa-e56b-daad889e8a99"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 382 ms (started: 2022-05-24 11:40:12 +00:00)\n"]}],"source":["\n","class AmericanBlackScholesSingleStock():\n","    \n","    def __init__(self , net, is_call = True):\n","\n","        self.C = 0.0\n","        self.R = 0.05         # Interest Rate (Yearly)\n","\n","        self.SIGMA = 0.25  # Volatility (Yearly)\n","        self.RU = 1.0      # stock corrolation\n","        \n","        self.K = 50.0              # Strike Price \n","        self.T = 1.0               # Maturation time (in YEAR)\n","        self.MAX_X = self.K*3.0   # MAX price\n","        ## for accept reject purpose!\n","        ## free boundry problems\n","        self.net = net\n","        \n","        self.weights = None\n","        self.eps = 1E9\n","        self.weights_tbl = []\n","        \n","        self.gamma = 0.0001\n","        self.beta = 0.0001\n","        \n","        self.is_call = is_call\n","        self.log_normal_dist = torch.distributions.LogNormal(self.R-self.C, self.SIGMA)\n","        self.log_normal_dist_5 = torch.distributions.LogNormal(self.R-self.C, self.SIGMA*5.0)\n","\n","    def reset_weights(self):\n","        self.weights = None\n","        self.eps = 1E9\n","        self.weights_tbl = []\n","\n","    def g(self , x):\n","        # pay off function - 1 is the stock dimension, 0 is the time dimension\n","        if self.is_call:\n","          return torch.max( x[:,1].reshape(-1,1) - self.K , torch.zeros([len(x),1]).cuda() ) \n","        else:\n","          return torch.max( self.K - x[:,1].reshape(-1,1) , torch.zeros([len(x),1]).cuda() ) \n","\n","\n","    def mu(self, x):\n","        ## should test it! output dimension is important !\n","        return (self.R-self.C)*x.reshape(-1,1)\n","\n","    def sigma(self , x):\n","        return self.SIGMA*x.reshape(-1,1)\n","\n","    def sample(self , sample_method_X = \"U\", size = 2**8 ):\n","        '''\n","        Sampling function\n","        '''\n","        ### Domain (above boundry)\n","        # xsetup = [torch.rand( [size,1] )*self.T]\n","        # for xi in range(self.net.Input-1):\n","        #   xsetup.append(torch.rand( [size,1] )*self.MAX_X)\n","        # x = torch.cat(tuple(xsetup), dim = 1 ).cuda() \n","\n","        if sample_method_X in [\"U\",\"UE3\"] :\n","            \n","            range_multiplier = 3.0 if sample_method_X == \"UE3\" else 1.0\n","            # above current pay-off samples\n","            x = torch.cat(( torch.rand([size,1])*self.T , -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","\n","            # pdb.set_trace()\n","            compare = self.net(x) - self.g(x)\n","            mask = compare > 0\n","            # pdb.set_trace()\n","\n","            x = x[mask.reshape(-1),:]\n","            ## repeat for each d (dimension)\n","            # mask = mask.repeat(1,4)\n","            # x = x[mask].reshape(-1,4)\n","\n","            ### Terminal time samples\n","            x_terminal = torch.cat( ( torch.zeros(size, 1) + self.T , -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier ) , dim = 1 ).cuda()\n","            \n","            ### under current pay-off samples\n","            x_boundary = torch.cat(( torch.rand([size,1])*self.T , -self.MAX_X*range_multiplier*torch.rand([size, 1])+self.MAX_X*range_multiplier) , dim = 1 ).cuda()\n","            compare = self.net(x_boundary) - self.g(x_boundary)\n","            mask = compare < 0\n","            x_boundary = x_boundary[mask.reshape(-1),:]\n","            \n","            # ## repeat for each d (dimension)\n","            # mask = mask.repeat(1,4)\n","            # x_boundary = x_boundary[mask].reshape(-1,4)\n","            ###\n","            return x , x_terminal , x_boundary\n","\n","    \n","        if sample_method_X in [\"LN\", \"LN5\"]:\n","\n","            ln_dist = self.log_normal_dist_5 if sample_method_X == \"LN5\" else self.log_normal_dist\n","            # above current pay-off samples\n","            # pdb.set_trace()\n","            x = torch.cat(( torch.rand([size,1])*self.T , torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0]))   ) , dim = 1 ).cuda()\n","            compare = self.net(x) - self.g(x)\n","            mask = compare > 0\n","            x = x[mask.reshape(-1),:]\n","\n","            ### Terminal time samples\n","            x_terminal = torch.cat( ( torch.zeros(size, 1) + self.T , torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0])) ) , dim = 1 ).cuda()\n","            ### under current pay-off samples\n","            x_boundary = torch.cat(( torch.rand( [size,1] )*self.T , torch.maximum(ln_dist.sample((size,)).reshape(-1,1)*self.K,torch.Tensor([0.0])) ) , dim = 1 ).cuda()\n","\n","            compare = self.net(x_boundary) - self.g(x_boundary)\n","            mask = compare < 0\n","            x_boundary = x_boundary[mask.reshape(-1),:]\n","            \n","            return x , x_terminal , x_boundary\n","\n","        raise ValueError(f\"{sample_method_X} is not a supported sampling method\")\n","    \n","    def criterion(self , x , x_terminal , x_boundary, loss_transform = None):\n","        '''\n","        Loss function that helps network find solution to equation\n","        '''   \n","        # pdb.set_trace()     \n","        d = torch.autograd.grad(\n","            self.net(x), \n","            x , \n","            grad_outputs=torch.ones_like(self.net(x)) ,\n","            create_graph=True )\n","        \n","        dt  = d[0][:,0].reshape(-1,1)\n","        dx1 = d[0][:,1].reshape(-1,1)\n","        \n","        # du/dxdx\n","        dx1x1 = torch.autograd.grad(dx1, \n","                                    x , \n","                                    grad_outputs=torch.ones_like(dx1) ,\n","                                    create_graph = True)[0][:,1].reshape(-1,1)\n","        \n","        if loss_transform is None:\n","          loss_transform = torch.square\n","\n","        DO = None\n","        TC = None\n","        BC = None\n","\n","        if len(x) == 0:\n","          # print('zero batch size for domain!')\n","          DO = torch.tensor(0).cuda().float()\n","        else:\n","          # x is above the free boundary ( so immediate pay-off is positive )\n","          above_boundary = dt + self.mu(x[:,1])*( dx1 ) + 0.5*( (self.sigma(x[:,1])*self.sigma(x[:,1]))*dx1x1 ) - self.R*self.net(x)\n","          DO = loss_transform(above_boundary)\n","          # 4.1 in paper - note self.net(x) for u(x,t), difference supposed to be zero\n","          # Domain \n","          #DO = (dt + 0.5*self.RU*(self.SIGMA**2)*(x[:,1].reshape(-1,1)**2)*dxx - self.R*self.net(x) + (self.R-self.C)*x[:,1].reshape(-1,1)*dx)**2\n","\n","        # Terminal Condition - should be equal (both in- and out of the money)\n","        TC = loss_transform( self.g(x_terminal) - self.net(x_terminal) )\n","\n","        # Boundary Condition - below the payoff value, it should still be worth something for optionality\n","        # len() is safe here , because it just shows batch number \n","        if( len(x_boundary) != 0):\n","            BC = loss_transform( torch.max( self.g(x_boundary) - self.net(x_boundary) , torch.zeros([len(x_boundary),1]).cuda() ) )\n","        else:\n","            # print('zero batch size for outside domain!')\n","            BC = torch.tensor(0).cuda().float()\n","        return  DO , TC , BC\n","\n","    def calculateLoss(self , size = 2**8 , train = True, loss_transform = None, keep_batch = False, sample_method_X = \"U\"):\n","        '''\n","        Helper function that Sample and Calculate loss,\n","        '''        \n","        x , x_terminal , x_boundary = self.sample(sample_method_X, size)\n","        x = Variable( x , requires_grad=True)\n","        Ls = self.criterion( x , x_terminal , x_boundary, loss_transform = loss_transform )\n","        DO , TC , BC = Ls\n","\n","        numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","        # DOm = torch.mean(DO).detach().cpu().float().item()\n","        # TCm = torch.mean(TC).detach().cpu().float().item()\n","        # BCm = torch.mean(BC).detach().cpu().float().item()\n","\n","        if not keep_batch:\n","          loss_equalWeightedByType = (1./numActive*torch.mean(DO) + 1./numActive*torch.mean(TC) + 1./numActive*torch.mean(BC))\n","          return  loss_equalWeightedByType , 1./numActive*torch.mean(DO) , 1./numActive*torch.mean(TC) , 1./numActive*torch.mean(BC) , loss_equalWeightedByType             \n","        else:\n","          return DO.numpy(), TC.numpy(), BC.numpy(), DO.numpy(), TC.numpy(), BC.numpy()\n","\n","    def calculateLossUsingKLMinMax(self , size = 2**8 , train = True, loss_transform = None, sample_method_X = \"U\"):\n","        '''\n","        Helper function that Samples and Calculate loss,\n","        This is adapted in that it changes the weights on the losses\n","        and the distribution of sampling to maximize the loss provided \n","        the KL distance of the loss is within positive constraints\n","        beta represents the constraints on the weights\n","        gamma represents the constraints on the sampling distribution\n","        (each representing an upper bound the KL distribution)\n","        '''        \n","        x , x_terminal , x_boundary = self.sample(sample_method_X, size)\n","        x = Variable( x, requires_grad=True)\n","        # pdb.set_trace()\n","        Ls = self.criterion( x , x_terminal , x_boundary, loss_transform = loss_transform)\n","        DO , TC , BC = Ls\n","        if self.weights is None:\n","          self.weights = torch.ones(1,len(Ls)).to(DO.device)/len(Ls)\n","        \n","        DOt = self.weights[0,0] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * DO)), self.gamma/self.beta) \n","        TCt = self.weights[0,1] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * TC)), self.gamma/self.beta) \n","        BCt = self.weights[0,2] * torch.pow(1.0/size * torch.sum( torch.exp(self.beta * BC)), self.gamma/self.beta) \n","\n","        # print([DOt, TCt, BCt, torch.log(DOt + TCt + BCt), 1.0/self.gamma * torch.log(DOt + TCt + BCt)])\n","\n","        numActive = np.sum([1 if list(lv.size()) else 0 for lv in Ls ])\n","        transformed_loss = 1.0/self.gamma * torch.log(DOt + TCt + BCt)\n","        loss_equalWeightedByType = (1./numActive*torch.mean(DO) + 1./numActive*torch.mean(TC) + 1./numActive*torch.mean(BC))\n","        # print((DOt.detach().cpu().item(), TCt.detach().cpu().item(), BCt.detach().cpu().item(), transformed_loss.detach().cpu().item()))\n","        return   transformed_loss, 1./numActive*torch.mean(DO) , 1./numActive*torch.mean(TC) , 1./numActive*torch.mean(BC) , loss_equalWeightedByType\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1653392413572,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"Y_k0b_RzfgtD","outputId":"f3968916-81e2-4b8c-a42b-c656f9a592bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 692 Âµs (started: 2022-05-24 11:40:12 +00:00)\n"]}],"source":["# torch.maximum(torch.Tensor([1.2, -12.6, 56.33]),torch.Tensor([0.0]))\n","# loss_terms = [ 34.25, 100.12, 23.45]\n","# target = torch.tensor([1./len(loss_terms)]*len(loss_terms))*loss_terms"]},{"cell_type":"markdown","metadata":{"id":"Hsr6F15afgtE"},"source":["#### TrainAmericanBlackScholesSingleStock"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1653389273440,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"J52duco3fgtE","outputId":"a6e400ad-1f4b-464e-dae2-9a9600dc392f"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 109 ms (started: 2022-05-24 10:47:52 +00:00)\n"]}],"source":["class TrainAmericanBlackScholesSingleStock():\n","    \n","    def __init__(self , net , equation , BATCH_SIZE , debug = False):\n","        self.history_mean_hooks = [] \n","        self.history_surfaces_hooks = None       \n","        self.history_tl = []\n","        self.history_dl = []\n","        self.history_il = []\n","        self.history_bl = []              \n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.net = net\n","        self.model = equation        \n","        self.debug = debug  \n","        self.hook_interval = 20      \n","        if self.debug == True:\n","            self.hooks = {}            \n","            self.get_all_layers(self.net)\n","\n","        self.use_early_stop = False\n","        self.early_stop_patience = 10\n","        self.early_stop_delta = 0.0        \n","        self.best_loss = np.Inf\n","        self.early_stop_counter = 0\n","        \n","        self.stop_epoch = 0\n","\n","    \n","    def train(self , epoch , lr, eqLossFn = 'calculateLoss', sample_method_X = \"U\"):\n","        optimizer = optim.Adam(self.net.parameters(), lr)\n","        # optimizer = optim.SGD(self.net.parameters(), lr)\n","        loss_avg = 0\n","        loss_calc_method = None\n","        try:\n","            loss_calc_method = getattr(self.model, eqLossFn)\n","        except AttributeError:\n","            raise NotImplementedError(\"Class `{}` does not implement `{}`\".format(self.model.__class__.__name__, eqLossFn))\n","        \n","        for e in range(epoch):\n","            optimizer.zero_grad()\n","            #pdb.set_trace()\n","            loss , dom , tcm , bcm, losses_equalWeightedByType = loss_calc_method( self.BATCH_SIZE, sample_method_X = sample_method_X )\n","            l1_loss, _, _, _, _ = self.model.calculateLoss( 2**7, loss_transform = torch.abs, sample_method_X = sample_method_X)\n","\n","            # if e % 50 == 0:\n","            #   print(f\"{loss}, {dom}, {tcm}, {bcm}, {losses_equalWeightedByType}\")\n","\n","            if self.use_early_stop:\n","              if losses_equalWeightedByType < (self.best_loss-self.early_stop_delta):\n","                self.best_loss = losses_equalWeightedByType\n","                self.early_stop_counter = 0\n","                # print(f\"New Loss to beat for early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","              else:\n","                self.early_stop_counter += 1\n","              if self.early_stop_counter>=self.early_stop_patience:\n","                print(f\"Early Stop at epoch {e}, original loss: {losses_equalWeightedByType} with patience {self.early_stop_patience}\")\n","                break\n","            loss_avg = loss_avg + float(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","            if (e % self.hook_interval == (self.hook_interval-1)) or e == 0:\n","                loss = loss_avg/self.hook_interval\n","                print(\"Epoch {} - lr {} -  rebalanced loss: {} - original loss: {} - L1 loss {}\".format(e , lr , loss, losses_equalWeightedByType, l1_loss))\n","                loss_avg = 0\n","                ## report detailed loss ## ## puting inside no grad??? for memory optimization!\n","                # tl , dl , il , bl, _ = self.model.calculateLoss( 2**6 )  # note that this is the standard loss!!\n","                self.history_tl.append( loss )\n","                self.history_dl.append( dom )\n","                self.history_il.append( tcm )\n","                self.history_bl.append( bcm )\n","                if self.debug == True:\n","                    mean = []\n","                    for l in self.hooks:\n","                        mean.append(torch.mean( self.hooks[l] ).item())\n","                    self.history_mean_hooks.append( mean )\n","                    xabove, xterminal, xbelow = self.model.sample(sample_method_X, 2**9)\n","                    xabove_res = self.model.net(xabove).detach()\n","                    xterminal_res = self.model.net(xterminal).detach()\n","                    xbelow_res = self.model.net(xbelow).detach()\n","                    # pdb.set_trace()\n","                    df_above = self.create_result_df(e, xabove, xabove_res, \"ABOVE\")\n","                    df_terminal = self.create_result_df(e, xterminal, xterminal_res, \"TERMINAL\")\n","                    df_below = self.create_result_df(e, xbelow, xbelow_res, \"BELOW\")\n","                    if self.history_surfaces_hooks is None:\n","                      self.history_surfaces_hooks = pd.concat([df_above, df_terminal, df_below],axis=0)\n","                    else:\n","                      self.history_surfaces_hooks = pd.concat([self.history_surfaces_hooks,pd.concat([df_above, df_terminal, df_below],axis=0) ], axis=0)\n","        self.stop_epoch = e\n","                    \n","    def hook_fn(self, m, i, o):\n","              self.hooks[m] = o.detach()\n","            \n","    def get_all_layers(self, net):\n","      for name, layer in net._modules.items():\n","          if isinstance(layer, nn.ModuleList):\n","              for n , l in layer.named_children():\n","                l.register_forward_hook(self.hook_fn)\n","          else:\n","              # it's a non sequential. Register a hook\n","              layer.register_forward_hook(self.hook_fn)\n","    \n","    def create_result_df(self, e, x, x_res, sample_type):\n","      df_above = pd.DataFrame(x.cpu().numpy(), columns = [\"Time\", \"S1\"])\n","      df_above[\"Epoch\"] = e\n","      df_above[\"Sample\"] = sample_type\n","      df_above[\"Result\"] = x_res.cpu().numpy()\n","      return df_above\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G4xcnKTkfgtG"},"source":["#### American Call - Standard Loss, Uniform sampling , NL=3, NN=30, lr = 0.01"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100803,"status":"ok","timestamp":1653392516347,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"7ElksDEZfgtG","outputId":"92395410-cfd3-4e5b-996a-b5b98b3a88db"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - lr 0.01 -  rebalanced loss: 2.227487548828125 - original loss: 2227.487548828125 - L1 loss 35.26634979248047\n","Epoch 999 - lr 0.01 -  rebalanced loss: 768.0924981613159 - original loss: 117.31851959228516 - L1 loss 7.09603214263916\n","Epoch 1999 - lr 0.01 -  rebalanced loss: 35.52297046303749 - original loss: 31.008892059326172 - L1 loss 4.148499488830566\n","Epoch 2999 - lr 0.01 -  rebalanced loss: 2.0480426743626596 - original loss: 0.13487450778484344 - L1 loss 0.19275487959384918\n","Epoch 3999 - lr 0.01 -  rebalanced loss: 10.896574376031756 - original loss: 36.51483154296875 - L1 loss 2.7180709838867188\n","Epoch 4999 - lr 0.01 -  rebalanced loss: 36.87523343847692 - original loss: 0.37389522790908813 - L1 loss 0.36691999435424805\n","Early Stop at epoch 5388, original loss: 0.4675107002258301 with patience 2000\n","time: 1min 40s (started: 2022-05-24 11:40:15 +00:00)\n"]}],"source":["eqLossFn= 'calculateLoss'\n","sample_method= \"U\"\n","lr = 0.01\n","net = AmericanOptionNet( NL = 3 , NN = 30 )\n","net.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","bsequation = AmericanBlackScholesSingleStock(net)\n","trainAmss = TrainAmericanBlackScholesSingleStock( net , bsequation , BATCH_SIZE = 2**9 , debug = True )\n","trainAmss.hook_interval = 1000\n","trainAmss.use_early_stop = True\n","trainAmss.early_stop_patience = 2000\n","trainAmss.train( epoch = 10000 , lr = 0.01, eqLossFn= eqLossFn , sample_method_X= sample_method)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"123y37YlfgtH"},"outputs":[],"source":["df = trainAmss.history_surfaces_hooks[trainAmss.history_surfaces_hooks.Epoch == max(trainAmss.history_surfaces_hooks.Epoch)]\n","fig = px.scatter_3d(df, x='Time', y='S1', z='Result',color='Sample', width=500, height=400)\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1653392875787,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"EfgvKad2fgtI","outputId":"0235db64-ab03-42e8-df32-c77dbfe7b159"},"outputs":[{"name":"stdout","output_type":"stream","text":["time: 15.2 ms (started: 2022-05-24 11:47:55 +00:00)\n"]}],"source":["torch.save(net.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainAmss.stop_epoch}_{str(lr).replace('.','p')}_{net.NL}_{net.NN}\")\n","# torch.save(model.state_dict(), PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386,"status":"ok","timestamp":1651679957434,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"VA8eJp8DfgtJ","outputId":"400f1fa7-ee52-4d7c-d738-51b5dd7e0cbd"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["# testmodel = AmericanOptionNet(NL = 3, NN = 30)\n","# print(os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals\"))\n","# testmodel.load_state_dict(torch.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_20220504153012_calculateLoss_U_5558_3_30\"))\n","# testmodel.forward(bsequation.sample(\"U\", 2**4)[0].cpu().detach())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1651680997023,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"WxogcmflfgtJ","outputId":"388ff521-86b6-4388-8c9f-a2281f2b61af"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'0p001'"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["# %pip install pytorchtools\n","# lr = 0.001\n","# str(lr).replace('.','p')"]},{"cell_type":"markdown","metadata":{"id":"iEwlH_u2fgtK"},"source":["#### American Call - Standard Loss, Uniform sampling , NL=5, NN=100, lr = 0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":3620,"status":"error","timestamp":1653393172302,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"Me2Bcn9yfgtK","outputId":"9df2ffd6-3cd0-4cff-f497-abacb1207baf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - lr 0.001 -  rebalanced loss: 1.446651123046875 - original loss: 1446.651123046875 - L1 loss 19.132484436035156\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-a05252970baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainAmss_net5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_early_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainAmss_net5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop_patience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainAmss_net5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqLossFn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0meqLossFn\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msample_method_X\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msample_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainAmss_net5.stop_epoch}_{str(lr).replace('.','p')}_{net5.NL}_{net5.NN}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_net5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainAmss_net5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_surfaces_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrainAmss_net5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_surfaces_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainAmss_net5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_surfaces_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-c367417673bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch, lr, eqLossFn, sample_method_X)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_avg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_interval\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"name":"stdout","output_type":"stream","text":["time: 3.29 s (started: 2022-05-24 11:52:48 +00:00)\n"]}],"source":["eqLossFn= 'calculateLoss'\n","sample_method= \"U\"\n","lr = 0.001\n","net5 = AmericanOptionNet( NL = 5 , NN = 100 )\n","net5.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","bsequation_net5 = AmericanBlackScholesSingleStock(net5)\n","trainAmss_net5 = TrainAmericanBlackScholesSingleStock( net5 , bsequation_net5 , BATCH_SIZE = 2**9 , debug = True )\n","trainAmss_net5.hook_interval = 1000\n","trainAmss_net5.use_early_stop = True\n","trainAmss_net5.early_stop_patience = 2000\n","trainAmss_net5.train( epoch = 10000 , lr = lr, eqLossFn= eqLossFn , sample_method_X= sample_method)\n","torch.save(net5.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainAmss_net5.stop_epoch}_{str(lr).replace('.','p')}_{net5.NL}_{net5.NN}\")\n","df_net5 = trainAmss_net5.history_surfaces_hooks[trainAmss_net5.history_surfaces_hooks.Epoch == max(trainAmss_net5.history_surfaces_hooks.Epoch)]\n","fig_net5 = px.scatter_3d(df_net5, x='Time', y='S1', z='Result',color='Sample', width=500, height=400)\n","fig_net5.show()"]},{"cell_type":"markdown","metadata":{"id":"yZacjndLfgtK"},"source":["#### American Call - Standard Loss, LN sampling , NL=5, NN=100, lr = 0.0001"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ol8LZje_fgtL"},"outputs":[],"source":["eqLossFn= 'calculateLoss'\n","sample_method= \"LN\"\n","lr = 0.0001\n","net5 = AmericanOptionNet( NL = 5 , NN = 100 )\n","net5.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","bsequation_net5 = AmericanBlackScholesSingleStock(net5)\n","trainAmss_net5 = TrainAmericanBlackScholesSingleStock( net5 , bsequation_net5 , BATCH_SIZE = 2**9 , debug = True )\n","trainAmss_net5.hook_interval = 1000\n","trainAmss_net5.use_early_stop = True\n","trainAmss_net5.early_stop_patience = 2000\n","trainAmss_net5.train( epoch = 10000 , lr = lr, eqLossFn= eqLossFn , sample_method_X= sample_method)\n","torch.save(net5.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainAmss_net5.stop_epoch}_{str(lr).replace('.','p')}_{net5.NL}_{net5.NN}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iP_AUsPWfgtL"},"outputs":[],"source":["df_net5 = trainAmss_net5.history_surfaces_hooks[trainAmss_net5.history_surfaces_hooks.Epoch == max(trainAmss_net5.history_surfaces_hooks.Epoch)]\n","fig_net5 = px.scatter_3d(df_net5, x='Time', y='S1', z='Result',color='Sample', width=500, height=400)\n","fig_net5.show()"]},{"cell_type":"markdown","metadata":{"id":"xex3RmRrfgtL"},"source":["#### American Call - KLMinMax, Uniform Sampling , NL=5, NN=100, lr = 0.001\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":563},"executionInfo":{"elapsed":164967,"status":"ok","timestamp":1651756459930,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"aWTXe907fgtM","outputId":"d030a6d5-9079-4007-8e2e-4491ced4eb66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - lr 0.001 -  rebalanced loss: -1.75352294921875 - original loss: 2301.139892578125 - L1 loss 34.0631103515625\n","Epoch 999 - lr 0.001 -  rebalanced loss: -10565.351641601563 - original loss: 300.3323974609375 - L1 loss 7.566646099090576\n","Epoch 1999 - lr 0.001 -  rebalanced loss: -10970.835181640625 - original loss: 318.5314636230469 - L1 loss 2.540367364883423\n","Epoch 2999 - lr 0.001 -  rebalanced loss: -10984.766489257812 - original loss: 116.28165435791016 - L1 loss 3.3452980518341064\n","Epoch 3999 - lr 0.001 -  rebalanced loss: -10986.014154296876 - original loss: 989.4080200195312 - L1 loss 3.6708362102508545\n","Epoch 4999 - lr 0.001 -  rebalanced loss: -10985.659685546874 - original loss: 414.9598693847656 - L1 loss 5.434750080108643\n","Epoch 5999 - lr 0.001 -  rebalanced loss: -10978.350711914063 - original loss: 167.88430786132812 - L1 loss 5.186882019042969\n","Early Stop at epoch 6386, original loss: 222.95346069335938 with patience 2000\n"]},{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"e01d82d6-7f25-4db9-adfd-a5bcfb949c4d\" class=\"plotly-graph-div\" style=\"height:400px; width:500px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e01d82d6-7f25-4db9-adfd-a5bcfb949c4d\")) {                    Plotly.newPlot(                        \"e01d82d6-7f25-4db9-adfd-a5bcfb949c4d\",                        [{\"hovertemplate\":\"Sample=ABOVE<br>Time=%{x}<br>S1=%{y}<br>Result=%{z}<extra></extra>\",\"legendgroup\":\"ABOVE\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"ABOVE\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.6130924224853516,0.97310870885849,0.7269733548164368,0.318462073802948,0.21931523084640503,0.8296488523483276,0.8999347686767578,0.10874873399734497,0.5016056895256042,0.11130112409591675,0.898302435874939,0.4699772000312805,0.38589566946029663,0.7135259509086609,0.6942085027694702,0.5523843765258789,0.5582502484321594,0.13242411613464355,0.8572223782539368,0.6361978650093079,0.0856066346168518,0.0787581205368042,0.638634979724884,0.8032670617103577,0.7584922909736633,0.7133171558380127,0.6608752608299255,0.33119040727615356,0.8019495606422424,0.41926872730255127,0.40443646907806396,0.1160581111907959,0.22270578145980835,0.3206230401992798,0.8870114684104919,0.5637974143028259,0.6438961029052734,0.07904654741287231,0.5005396604537964,0.4748155474662781,0.35973697900772095,0.3927552103996277,0.06771093606948853,0.9105400443077087,0.3806971311569214,0.24929696321487427,0.002290964126586914,0.30062371492385864,0.2649487853050232,0.7655763626098633,0.20778006315231323,0.5708714127540588,0.6008923649787903,0.7939520478248596,0.6447068452835083,0.5906996130943298,0.030972659587860107,0.33582907915115356,0.3487551212310791,0.4436380863189697,0.9173998832702637,0.729210615158081,0.3793179988861084,0.19508743286132812,0.620574951171875,0.9331913590431213,0.03700143098831177,0.15465688705444336,0.8393023014068604,0.08494865894317627,0.22759664058685303,0.3836897611618042,0.9335057735443115,0.11200881004333496,0.7918033599853516,0.17307496070861816,0.7265914082527161,0.5255704522132874,0.26336193084716797,0.7823198437690735,0.260526180267334,0.918766438961029,0.3328312039375305,0.7505171895027161,0.5975730419158936,0.23685842752456665,0.9012562036514282,0.08596605062484741,0.5750561952590942,0.1825917363166809,0.851189136505127,0.004203975200653076,0.18708795309066772,0.009024441242218018,0.7415652275085449,0.8881651163101196,0.45033228397369385,0.2309216856956482,0.49583685398101807,0.6953835487365723,0.3261697292327881,0.29992175102233887,0.832939088344574,0.5355404019355774,0.7402287721633911,0.7940819263458252,0.06407499313354492,0.2318882942199707,0.3121557831764221,0.15733933448791504,0.12487173080444336,0.7800740599632263,0.19976484775543213,0.4914688467979431,0.22878718376159668,0.6979004144668579,0.7123874425888062,0.2816247344017029,0.8993582725524902,0.8980366587638855,0.8606023192405701,0.7414575219154358,0.4323098659515381,0.6809982061386108,0.6855962872505188,0.5765411257743835,0.9141377210617065,0.921153724193573,0.18145328760147095,0.4288461208343506,0.8329805135726929,0.5559030771255493,0.42451196908950806,0.05409097671508789,0.9207938313484192,0.362018883228302,0.020320415496826172,0.3531326651573181,0.44370102882385254,0.3287947177886963,0.08429467678070068,0.2693955898284912,0.5971197485923767,0.05716514587402344,0.9231439232826233,0.0272100567817688,0.3909754157066345,0.8168545961380005,0.44507741928100586,0.3748418092727661,0.5854942202568054,0.10045140981674194,0.831988513469696,0.38552266359329224,0.3015022277832031,0.5331419706344604,0.8820633888244629,0.2900305986404419,0.48064982891082764,0.8962337374687195,0.4059929847717285,0.1593838334083557,0.45279407501220703,0.024550318717956543,0.3948920965194702,0.47145766019821167,0.6299556493759155,0.8598309755325317,0.02796459197998047,0.6040303111076355,0.20430415868759155,0.15563464164733887,0.091205894947052,0.686920166015625,0.6310914158821106,0.21762996912002563,0.0020594000816345215,0.40457600355148315,0.3247089982032776,0.613576352596283,0.015542566776275635,0.12357747554779053,0.7760109305381775,0.4956408143043518,0.12771105766296387,0.21354812383651733,0.10821801424026489,0.8925567269325256,0.12500399351119995,0.44892269372940063,0.7937183380126953,0.8151134848594666,0.5344281196594238,0.47442710399627686,0.16335463523864746,0.15359801054000854,0.7361070513725281,0.2770191431045532,0.8028501272201538,0.8766122460365295,0.29004740715026855,0.9309723377227783,0.6758455038070679,0.11134487390518188,0.6529590487480164,0.2381594181060791,0.686967670917511,0.5314226150512695,0.885228157043457,0.9071986675262451,0.24605637788772583,0.9365741610527039,0.4100586175918579,0.5109179615974426,0.2546693682670593,0.040136098861694336,0.0563884973526001,0.26204103231430054,0.09617441892623901,0.4878188371658325,0.5067281126976013,0.10116159915924072,0.2981623411178589,0.7971715331077576,0.3768983483314514,0.5101035833358765,0.25645583868026733,0.19336360692977905,0.6367615461349487,0.8430092334747314,0.20280128717422485,0.6877400875091553,0.1504954695701599,0.43269771337509155,0.6184029579162598,0.15643566846847534,0.16376042366027832,0.3008441925048828,0.07026845216751099,0.6085185408592224,0.8382865786552429,0.48303061723709106,0.8458821177482605,0.5229941606521606,0.2536447048187256,0.04032045602798462,0.2719331383705139,0.4652920961380005,0.07802450656890869,0.4076892137527466,0.09145253896713257,0.819556474685669,0.29595667123794556,0.28013139963150024,0.2698827385902405,0.2799205780029297,0.0015854239463806152,0.10319817066192627,0.4095296859741211,0.2561938166618347,0.3755037784576416,0.8530349135398865,0.8074982762336731,0.5159261226654053,0.2489866018295288,0.11103332042694092,0.44456279277801514,0.013983190059661865,0.8842658400535583,0.48836731910705566,0.07544213533401489,0.30696433782577515,0.7110617756843567,0.7922203540802002,0.211073637008667,0.494115948677063,0.8383544087409973,0.18745332956314087,0.7859393954277039,0.2730090022087097,0.44932156801223755,0.3423234224319458,0.8779516220092773,0.49823617935180664,0.2565000057220459,0.17753541469573975,0.1770327091217041,0.08389049768447876,0.8886402249336243,0.3231736421585083,0.5668486952781677,0.7687344551086426,0.923121452331543,0.12539762258529663,0.7966310977935791,0.5199396014213562,0.7180032730102539,0.32690757513046265,0.8034189939498901,0.6714759469032288,0.6917495131492615,0.9101904034614563,0.7298179864883423,0.2225344181060791,0.6173469424247742,0.6614033579826355,0.9220927357673645,0.06598591804504395,0.08900070190429688,0.5740869641304016,0.06744903326034546,0.16071772575378418,0.2560212016105652,0.01371544599533081,0.5055456161499023,0.1567564606666565,0.5135362148284912,0.7054151296615601,0.9893388748168945,0.6524501442909241,0.8412500619888306,0.7597552537918091,0.18360137939453125,0.520815372467041,0.6963780522346497,0.948905885219574,0.25021040439605713,0.6074060201644897,0.3294808268547058,0.6438500881195068,0.8447939157485962,0.6873177289962769,0.7407501935958862,0.653100848197937,0.36830657720565796,0.6443758606910706,0.45684802532196045,0.8722934722900391,0.4535350203514099,0.8977553844451904,0.20076686143875122,0.6919115781784058,0.8621885180473328,0.26731032133102417,0.8321763277053833,0.4780871272087097,0.18056178092956543,0.6786393523216248,0.26220405101776123,0.3786829113960266,0.9402961134910583,0.24239033460617065,0.007407426834106445,0.4494820833206177,0.5610542893409729,0.4620501399040222,0.1650763750076294,0.7649057507514954,0.4096522331237793,0.16030842065811157,0.22854143381118774,0.6188070774078369,0.4410860538482666,0.36975908279418945,0.7496181726455688,0.5673237442970276,0.4499577283859253,0.027998149394989014],\"y\":[114.68966674804688,52.39286804199219,73.35726928710938,126.51350402832031,143.93545532226562,58.759002685546875,142.55714416503906,83.3680648803711,52.865516662597656,119.46441650390625,115.5827865600586,126.15298461914062,117.67770385742188,93.16241455078125,110.82833099365234,65.09700775146484,33.53394317626953,120.55264282226562,49.787437438964844,43.44190979003906,111.40010070800781,111.82292175292969,143.2288055419922,100.25979614257812,80.73043823242188,125.5534439086914,115.04054260253906,94.02128601074219,119.37479400634766,122.69747161865234,76.22300720214844,51.78887939453125,54.638404846191406,146.1397705078125,41.92823028564453,52.36968231201172,45.40850830078125,125.53028106689453,73.35533905029297,35.879608154296875,96.08956909179688,42.775917053222656,33.24177551269531,75.30120849609375,78.78131866455078,111.8498306274414,33.86918640136719,122.59574890136719,93.93121337890625,115.42080688476562,141.9456024169922,56.94355773925781,103.68928527832031,50.64801788330078,75.6102294921875,68.07485961914062,120.5920181274414,124.7937240600586,56.438636779785156,134.08389282226562,70.78939056396484,37.17493438720703,133.49078369140625,44.44456481933594,64.89437866210938,39.08465576171875,84.06429290771484,37.77587127685547,40.62846374511719,79.86967468261719,105.65495300292969,37.96104431152344,84.81398010253906,66.62569427490234,57.77447509765625,145.8617401123047,145.80369567871094,79.57108306884766,109.00569152832031,85.33980560302734,144.89144897460938,106.50823211669922,143.13072204589844,84.66557312011719,146.30882263183594,142.3347625732422,144.84295654296875,78.65338134765625,79.21002960205078,47.20135498046875,75.1041030883789,112.3254165649414,121.17085266113281,118.11731719970703,57.46827697753906,128.4154052734375,145.06231689453125,68.2535629272461,122.34748840332031,101.90750885009766,145.33041381835938,115.48178100585938,145.47572326660156,115.47511291503906,110.06903076171875,143.68270874023438,37.924530029296875,59.822113037109375,142.439697265625,30.80841064453125,146.2432861328125,97.71737670898438,72.3373794555664,68.46849822998047,133.20176696777344,57.38630676269531,101.24705505371094,115.84706115722656,85.34901428222656,53.28480529785156,79.74015808105469,113.96003723144531,82.94852447509766,92.67274475097656,64.92913055419922,105.78924560546875,75.33679962158203,70.40074157714844,103.9832763671875,133.34495544433594,60.806739807128906,77.63072204589844,129.03695678710938,64.79069519042969,36.12109375,41.300537109375,83.73300170898438,67.17437744140625,77.53845977783203,95.42561340332031,94.02259826660156,60.017845153808594,59.49542236328125,39.809730529785156,84.87348175048828,60.986083984375,95.7911376953125,90.83019256591797,64.38623046875,98.26620483398438,50.78117370605469,52.65397644042969,75.99337768554688,100.95326232910156,65.82991027832031,96.85585021972656,91.91162109375,77.3607177734375,119.21714782714844,66.17183685302734,76.31079864501953,84.50758361816406,57.53767395019531,58.11859893798828,135.48585510253906,126.05250549316406,104.30740356445312,145.18910217285156,63.026466369628906,74.95993041992188,66.76929473876953,105.378173828125,56.604591369628906,109.95598602294922,70.78736877441406,72.27967834472656,60.478607177734375,133.31808471679688,139.086181640625,104.67869567871094,49.35980224609375,70.12339782714844,118.03947448730469,132.59716796875,92.50748443603516,130.8197784423828,87.53504943847656,117.2541275024414,79.99830627441406,136.08895874023438,55.780052185058594,42.35108184814453,73.28771209716797,65.34477996826172,73.46617126464844,145.01231384277344,114.41950988769531,107.26182556152344,142.99057006835938,141.54823303222656,81.9106216430664,133.17074584960938,139.88742065429688,60.27686309814453,88.1539306640625,145.27174377441406,117.46467590332031,35.56239318847656,101.59588623046875,141.7464141845703,85.09823608398438,122.66873931884766,123.58187103271484,52.50079345703125,107.48763275146484,57.74176788330078,146.7335662841797,107.68695068359375,94.03850555419922,69.9756088256836,76.57622528076172,51.55323791503906,89.2507095336914,124.88506317138672,140.41555786132812,98.03262329101562,132.31504821777344,47.47618103027344,59.31525421142578,102.01747131347656,74.40619659423828,129.7728271484375,36.68932342529297,96.90940856933594,92.33216857910156,129.1829071044922,77.98977661132812,117.90978240966797,43.61460876464844,71.76252746582031,94.0489501953125,75.26762390136719,131.51922607421875,48.328407287597656,58.85551452636719,146.0642547607422,95.94161987304688,141.2545623779297,127.01834869384766,115.88897705078125,89.20014190673828,52.18524932861328,73.24260711669922,66.9096908569336,101.2226791381836,52.896827697753906,113.78018188476562,41.43592834472656,59.900634765625,72.30915069580078,141.53529357910156,55.96038055419922,49.06770324707031,137.87278747558594,128.70431518554688,120.41148376464844,86.98592376708984,102.93108367919922,45.752342224121094,78.33672332763672,59.31195068359375,81.62950134277344,88.239013671875,42.021507263183594,49.37157440185547,92.70780181884766,112.28050231933594,56.906776428222656,36.67430114746094,93.19534301757812,86.53227233886719,57.869239807128906,111.53337097167969,111.53993225097656,32.16059112548828,126.39665985107422,45.160430908203125,65.64918518066406,81.42916870117188,130.50375366210938,106.98649597167969,41.06803894042969,137.47857666015625,128.3452911376953,53.224639892578125,107.95079040527344,70.10702514648438,72.53192138671875,50.60932922363281,132.8628387451172,36.151763916015625,36.002349853515625,119.12154388427734,120.46969604492188,131.5323944091797,78.13512420654297,144.63685607910156,122.95903015136719,45.545989990234375,36.039947509765625,36.34648895263672,115.90505981445312,145.45526123046875,122.55884552001953,101.70526123046875,67.4824447631836,121.25003051757812,87.13658142089844,47.654808044433594,121.52499389648438,104.04613494873047,144.7891845703125,53.416229248046875,60.251808166503906,80.93671417236328,43.964332580566406,69.54414367675781,109.07943725585938,47.026214599609375,97.26138305664062,55.78247833251953,106.21642303466797,121.83552551269531,120.85704803466797,72.70405578613281,143.9392852783203,61.694984436035156,70.96569061279297,35.111572265625,82.78433990478516,69.68779754638672,95.73664093017578,74.20125579833984,84.98014831542969,108.52272033691406,68.40103912353516,114.30726623535156,109.54139709472656,92.77230834960938,68.27165222167969,70.6143798828125,96.61677551269531,123.75057220458984,141.16473388671875,139.72731018066406,143.58950805664062,86.17858123779297,146.0622100830078,87.72322082519531,116.84561157226562,111.85812377929688,54.873939514160156,121.78282928466797,110.98786163330078,53.413352966308594,50.84459686279297,41.89060974121094,84.4827880859375],\"z\":[69.84371185302734,2.785097122192383,26.05780792236328,87.4924545288086,96.90643310546875,9.678899765014648,93.7259292602539,44.523189544677734,7.00618839263916,83.25543975830078,66.33592224121094,84.17281341552734,76.5089111328125,46.0755615234375,64.67992401123047,19.717187881469727,0.025705277919769287,84.10979461669922,2.047659158706665,0.8189701437950134,74.94139862060547,75.49925231933594,96.07540893554688,52.11082077026367,32.95282745361328,79.21455383300781,69.45182800292969,52.62424087524414,71.57610321044922,81.25861358642578,32.872833251953125,10.317413330078125,12.24932861328125,96.91921997070312,0.33336639404296875,5.878968715667725,1.2417727708816528,90.96806335449219,28.753923416137695,0.17029839754104614,54.40346145629883,1.1877384185791016,0.24938929080963135,25.86237144470215,35.81344223022461,72.72661590576172,0.3556603193283081,83.29414367675781,53.55509567260742,68.21134185791016,96.82890319824219,10.63793659210205,58.736732482910156,2.781770944595337,29.262235641479492,22.34211540222168,86.12007141113281,85.1469955444336,12.733078002929688,93.2652359008789,21.352195739746094,0.11550039052963257,93.66431427001953,2.6452434062957764,18.706340789794922,0.12025237083435059,46.3729248046875,0.6471129059791565,0.2645697593688965,41.10526657104492,66.66828155517578,0.39731326699256897,34.81790542602539,26.716352462768555,9.037919998168945,96.98377227783203,96.281982421875,34.72104263305664,69.57259368896484,37.22657775878906,96.91385650634766,56.8353271484375,96.76738739013672,36.96076583862305,96.64634704589844,96.82194519042969,95.05748748779297,39.784461975097656,33.72275924682617,4.736924648284912,26.344806671142578,77.29387664794922,83.78927612304688,83.63290405273438,9.26465129852295,79.22713470458984,96.75590515136719,26.89251708984375,79.54899597167969,55.429710388183594,96.88616943359375,75.6506118774414,95.75794219970703,71.85160064697266,63.19606399536133,95.37123107910156,0.8246637582778931,17.95316505432129,96.74559020996094,0.06240445375442505,97.00821685791016,49.821956634521484,31.535024642944336,23.91504669189453,95.08184051513672,9.665604591369629,54.484405517578125,76.3330307006836,35.76166915893555,3.8294105529785156,30.752071380615234,67.11972045898438,39.42765426635742,46.02961730957031,17.987768173217773,61.30815505981445,25.8563175201416,20.922340393066406,65.6558837890625,92.85224151611328,11.8660306930542,32.37360382080078,88.3746337890625,25.533489227294922,0.0051316022872924805,0.9133445024490356,46.26886749267578,24.256391525268555,33.70964813232422,54.17094802856445,56.52302551269531,17.700944900512695,13.15977954864502,1.2891266345977783,35.00286865234375,21.816726684570312,53.600948333740234,42.27779769897461,20.245210647583008,56.50030517578125,4.284451484680176,11.512600898742676,27.433887481689453,59.199859619140625,23.49519157409668,52.56710433959961,42.49220657348633,35.557884216308594,76.5376205444336,16.8851261138916,32.94253921508789,44.992462158203125,12.703912734985352,18.713993072509766,94.78697204589844,84.03170776367188,58.935997009277344,95.50299072265625,23.99791717529297,29.104820251464844,25.685829162597656,67.5337142944336,16.159454345703125,63.90183639526367,24.613718032836914,31.239585876464844,21.59364128112793,93.18329620361328,96.34873962402344,59.57624053955078,8.795608520507812,30.230758666992188,70.65594482421875,91.01956939697266,54.17285919189453,93.78258514404297,49.05466842651367,68.08285522460938,40.657325744628906,94.60303497314453,6.936680793762207,0.435355007648468,28.272659301757812,20.898340225219727,33.200889587402344,96.97267150878906,67.66360473632812,67.55164337158203,95.00047302246094,93.27106475830078,40.32235336303711,83.48372650146484,94.52960968017578,19.96800422668457,41.77532196044922,96.93892669677734,71.46996307373047,0.12418484687805176,52.28900146484375,93.00099182128906,44.33875274658203,72.72598266601562,82.39483642578125,6.537858963012695,68.14212036132812,18.09322738647461,97.03475189208984,68.2309799194336,56.34889221191406,25.492034912109375,31.927812576293945,10.233762741088867,48.00605010986328,77.1398696899414,96.41388702392578,54.16289138793945,94.31462860107422,4.880695343017578,12.494685173034668,53.34162902832031,33.652774810791016,84.1968765258789,0.5042111873626709,54.153743743896484,46.56629943847656,93.25457763671875,37.96243667602539,78.1760482788086,2.9149463176727295,25.856531143188477,45.2432746887207,30.90187644958496,83.2305679321289,2.9890506267547607,16.617860794067383,97.02971649169922,55.6163330078125,96.29611206054688,92.5083236694336,74.31233215332031,51.13446807861328,3.5955071449279785,31.214691162109375,24.88179588317871,61.317378997802734,9.56319808959961,78.90758514404297,1.674983263015747,15.853402137756348,30.766334533691406,96.56333923339844,6.522573947906494,1.9459688663482666,94.94235229492188,91.305908203125,84.35332489013672,43.46503448486328,67.25804138183594,0.8152354955673218,33.94648361206055,19.373123168945312,39.78852081298828,41.076351165771484,0.42303043603897095,6.528201580047607,48.76513671875,63.93892288208008,15.272398948669434,0.06951898336410522,52.63492202758789,42.92289733886719,14.421501159667969,62.578433990478516,68.45304107666016,0.08150404691696167,90.09258270263672,3.201138734817505,26.053348541259766,32.0633430480957,91.8674545288086,62.70351791381836,0.35240840911865234,88.41122436523438,92.98743438720703,4.556321620941162,64.43328094482422,22.924257278442383,30.08574867248535,2.7081313133239746,88.01134490966797,0.08317619562149048,0.004253983497619629,72.45045471191406,82.33645629882812,87.52857208251953,31.57010269165039,94.77434539794922,88.25472259521484,4.163676738739014,0.1288435459136963,0.5647590160369873,78.43669891357422,96.93470764160156,88.81434631347656,58.12303161621094,27.03629493713379,78.09681701660156,40.03421401977539,0.9956965446472168,76.09149932861328,55.46101379394531,95.93148803710938,11.339094161987305,14.899208068847656,33.92607116699219,0.4771796464920044,27.984333038330078,64.23413848876953,3.4101715087890625,51.33967590332031,6.429830551147461,60.04188919067383,74.98406982421875,75.40511322021484,29.73679542541504,96.20594787597656,17.237382888793945,22.032197952270508,0.1379234790802002,33.273799896240234,28.766674041748047,49.04606628417969,25.3367977142334,43.9010124206543,60.20659637451172,24.005935668945312,76.40657043457031,63.6052131652832,52.34587860107422,25.074216842651367,20.922786712646484,56.8117790222168,90.32500457763672,96.32904052734375,95.42103576660156,96.61808013916016,46.70969009399414,96.202392578125,44.73969650268555,79.45654296875,73.07125091552734,7.841665744781494,79.89060974121094,69.915283203125,5.11499547958374,4.484457015991211,0.8615388870239258,46.96926498413086],\"type\":\"scatter3d\"},{\"hovertemplate\":\"Sample=TERMINAL<br>Time=%{x}<br>S1=%{y}<br>Result=%{z}<extra></extra>\",\"legendgroup\":\"TERMINAL\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"TERMINAL\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"y\":[62.78864288330078,136.10023498535156,107.98115539550781,10.693023681640625,30.533424377441406,82.69495391845703,116.33819580078125,119.43897247314453,102.29539489746094,54.22565460205078,121.86676025390625,87.91322326660156,7.8387451171875,76.03575134277344,135.90415954589844,8.781600952148438,132.1521759033203,39.875213623046875,142.70578002929688,38.218414306640625,22.193466186523438,63.13740539550781,87.279541015625,69.89591979980469,62.45375061035156,126.81993103027344,78.97486114501953,67.38264465332031,103.92039489746094,1.8680572509765625,71.70072174072266,47.96003723144531,40.71704864501953,100.37020874023438,47.523651123046875,25.64984893798828,36.974388122558594,122.10787200927734,72.05828857421875,40.69457244873047,122.61212158203125,34.453529357910156,15.860610961914062,133.00843811035156,12.841690063476562,137.0314178466797,50.87030792236328,19.70684814453125,95.23709106445312,89.27155303955078,94.28834533691406,32.787147521972656,117.13074493408203,30.086135864257812,144.07907104492188,121.61712646484375,93.41462707519531,74.92169952392578,83.49369049072266,88.58454895019531,52.25226593017578,62.12730407714844,16.679595947265625,81.5558853149414,12.018966674804688,6.918365478515625,140.96481323242188,145.5299530029297,42.46287536621094,136.46780395507812,32.16674041748047,101.03414916992188,38.585044860839844,108.71495056152344,10.9097900390625,76.602783203125,101.59515380859375,148.76759338378906,149.0334930419922,75.65983581542969,93.71817016601562,0.2648162841796875,146.94427490234375,36.68090057373047,109.4696044921875,35.14636993408203,3.6924285888671875,62.639686584472656,127.91643524169922,35.23991394042969,92.35167694091797,76.12509155273438,1.4277496337890625,81.35454559326172,42.802337646484375,105.68069458007812,117.19556427001953,82.18351745605469,60.34662628173828,18.7763671875,136.5365753173828,133.96676635742188,44.33427429199219,61.548614501953125,65.9673843383789,111.21226501464844,42.973388671875,119.72547912597656,50.20500946044922,110.04084777832031,76.71755981445312,144.00479125976562,123.84253692626953,21.0142822265625,46.32875061035156,116.35446166992188,98.6756820678711,25.19995880126953,104.93606567382812,26.765357971191406,133.99900817871094,109.32611083984375,38.76020812988281,31.17742919921875,93.36824035644531,54.122344970703125,29.62786102294922,40.959075927734375,85.941162109375,98.89412689208984,143.52005004882812,66.96924591064453,80.64091491699219,68.9891128540039,80.6937255859375,2.8886871337890625,91.27445983886719,49.26014709472656,62.24180603027344,95.1307373046875,32.13715362548828,84.5877456665039,37.37132263183594,16.4063720703125,81.14218139648438,149.21495056152344,65.92977142333984,29.11231231689453,58.197410583496094,129.17608642578125,40.48777770996094,29.04625701904297,44.12073516845703,32.23103332519531,6.409881591796875,91.92060852050781,39.19818878173828,80.22296905517578,147.73814392089844,138.64938354492188,58.14076232910156,34.81060028076172,0.57666015625,67.54007720947266,65.58042907714844,84.65435791015625,117.714111328125,100.66903686523438,38.80077362060547,8.947860717773438,101.14729309082031,40.60894012451172,117.26133728027344,90.03409576416016,110.46509552001953,44.95526885986328,108.19679260253906,82.67826843261719,32.499969482421875,79.67688751220703,139.14883422851562,110.9969253540039,75.81228637695312,41.46265411376953,35.207550048828125,79.94042205810547,146.3368377685547,130.76307678222656,108.90638732910156,54.782264709472656,20.487228393554688,114.83557891845703,48.996368408203125,105.57821655273438,67.93331909179688,126.87018585205078,80.87638854980469,134.26138305664062,97.8520736694336,4.76373291015625,144.5290985107422,55.68257141113281,22.55120849609375,12.140594482421875,121.94041442871094,34.693870544433594,108.29663848876953,41.74897766113281,29.428054809570312,5.821685791015625,122.38358306884766,78.75601196289062,43.24336242675781,55.525001525878906,35.03369903564453,88.15745544433594,78.05857849121094,1.243499755859375,51.32324981689453,85.75183868408203,6.9403228759765625,91.00935363769531,65.13922882080078,76.5783920288086,58.03295135498047,22.316848754882812,16.767868041992188,95.83209991455078,84.77300262451172,40.883087158203125,113.60870361328125,120.91590881347656,59.05851745605469,90.20536804199219,8.65863037109375,135.3931427001953,92.06620788574219,103.29513549804688,70.30572509765625,76.45960998535156,104.97599792480469,73.42565155029297,149.03358459472656,65.47903442382812,28.17113494873047,34.67041778564453,68.00640106201172,66.43465423583984,67.26252746582031,24.240211486816406,43.03223419189453,20.659957885742188,42.75257110595703,59.511268615722656,36.030181884765625,15.186386108398438,48.235321044921875,46.84693145751953,119.40269470214844,147.51316833496094,136.23736572265625,75.90247344970703,64.70764923095703,83.64820861816406,11.582489013671875,27.262290954589844,80.37858581542969,124.6986312866211,36.22937774658203,136.29364013671875,140.15060424804688,42.327178955078125,138.72018432617188,51.403472900390625,13.430404663085938,22.259429931640625,50.63838195800781,35.09833526611328,92.85890197753906,0.453216552734375,26.716903686523438,41.39764404296875,41.88221740722656,141.21749877929688,23.678489685058594,86.83966064453125,94.78085327148438,68.48973083496094,133.10507202148438,106.06439208984375,85.44654083251953,104.13640594482422,105.66160583496094,56.1224365234375,56.261878967285156,141.73353576660156,110.83273315429688,83.14689636230469,134.16452026367188,143.507080078125,112.40038299560547,24.27692413330078,100.22808837890625,89.20539093017578,89.90394592285156,12.64898681640625,128.92166137695312,145.79025268554688,79.46809387207031,124.38258361816406,135.5030517578125,99.1302490234375,82.95468139648438,35.47920227050781,130.61041259765625,21.904129028320312,84.73809051513672,106.64065551757812,81.06141662597656,15.243698120117188,140.57260131835938,75.35870361328125,131.82794189453125,33.343238830566406,102.265625,120.37769317626953,74.96586608886719,5.169464111328125,46.664100646972656,148.885009765625,137.6399383544922,126.71568298339844,82.83655548095703,132.3056640625,49.2303466796875,131.2366485595703,92.23921203613281,87.5904769897461,118.95713806152344,49.513763427734375,4.526397705078125,143.1741180419922,126.06369018554688,122.35453796386719,86.86390686035156,8.715576171875,16.081954956054688,143.25059509277344,34.457275390625,147.73800659179688,109.66569519042969,5.5130615234375,10.277130126953125,105.07138061523438,99.08995819091797,63.84394836425781,32.71778106689453,51.461280822753906,1.3292236328125,53.07972717285156,36.41222381591797,9.383087158203125,8.56414794921875,133.68016052246094,13.551193237304688,70.90155029296875,115.81977844238281,1.9095306396484375,12.917633056640625,75.35313415527344,88.9380111694336,20.980316162109375,24.881454467773438,53.601783752441406,38.64910125732422,100.98751068115234,111.220947265625,31.541099548339844,11.534912109375,10.0474853515625,57.05326843261719,36.4798583984375,134.247314453125,27.49584197998047,9.560226440429688,130.16705322265625,125.94515228271484,79.61668395996094,13.516006469726562,50.98377990722656,133.09793090820312,126.54440307617188,90.68070220947266,0.7177276611328125,5.249755859375,39.84698486328125,2.37353515625,140.931396484375,115.02740478515625,56.3345947265625,98.10581970214844,37.91728973388672,117.95335388183594,69.6104507446289,28.138206481933594,103.03436279296875,102.14563751220703,66.67069244384766,127.28919982910156,113.2558822631836,126.54348754882812,83.44786071777344,56.25006103515625,59.83684539794922,72.92525482177734,26.145126342773438,95.50651550292969,55.39086151123047,107.69316101074219,131.59043884277344,14.419937133789062,71.18793487548828,53.881500244140625,121.81649017333984,37.44044494628906,137.46383666992188,104.87419128417969,80.46310424804688,11.6343994140625,110.8511734008789,130.57582092285156,97.72201538085938,51.684295654296875,137.24916076660156,149.69119262695312,120.59310913085938,109.89494323730469,129.2442626953125,8.499801635742188,88.52320861816406,79.65812683105469,138.97154235839844,130.7482147216797,10.614913940429688,2.048553466796875,87.35841369628906,92.80911254882812,131.05563354492188,92.44868469238281,13.14031982421875,124.92943572998047,134.46359252929688,71.95136260986328,36.38910675048828,140.25294494628906,128.8553009033203,149.62144470214844,149.7030487060547,105.82489013671875,14.512710571289062,76.90744018554688,124.39888000488281,86.67826843261719,93.7191162109375,10.355300903320312,145.93014526367188,43.880340576171875,42.96587371826172,127.69779968261719,55.17073059082031,71.1744384765625,90.59773254394531,73.88665008544922,91.66024780273438,18.936569213867188,113.1893310546875,68.67137908935547,50.18963623046875,115.01548767089844,57.15692901611328,7.20574951171875,108.92539978027344,30.327926635742188,26.081756591796875,99.48328399658203,116.69476318359375,21.52215576171875,28.9310302734375,132.6484375,119.0384750366211,133.7814178466797,76.8234634399414,22.913650512695312,112.43695068359375,80.94927215576172,101.11077880859375,25.428359985351562,15.69830322265625,39.559715270996094,5.049774169921875,20.654678344726562,149.97012329101562,8.408721923828125,97.81309509277344,146.60035705566406,73.49078369140625,12.634033203125,76.0012435913086,111.08860778808594,111.31498718261719,47.85444641113281,112.18927001953125],\"z\":[12.129050254821777,85.4178237915039,57.12794876098633,-0.2666425108909607,-0.134221613407135,31.96845245361328,65.5376968383789,68.59202575683594,51.34360122680664,3.7887589931488037,70.9629135131836,37.01905059814453,-0.27365297079086304,25.569320678710938,85.2018051147461,-0.27144724130630493,81.18347930908203,0.13035482168197632,92.43702697753906,0.055866360664367676,-0.21752125024795532,12.50329875946045,36.40205383300781,19.541690826416016,11.769316673278809,75.80236053466797,28.39588165283203,16.980937957763672,52.99481964111328,-0.29806751012802124,21.34136962890625,1.0406429767608643,0.1761576533317566,49.39314270019531,0.9475507736206055,-0.19096869230270386,0.01071619987487793,71.19783020019531,21.6945858001709,0.17485308647155762,71.68901824951172,-0.06020534038543701,-0.24979370832443237,82.08057403564453,-0.26041311025619507,86.44981384277344,1.9291174411773682,-0.23219329118728638,44.23764419555664,38.34549331665039,43.293182373046875,-0.09593242406845093,66.32191467285156,-0.14072078466415405,93.5583724975586,70.7196273803711,42.42587661743164,24.492103576660156,32.737579345703125,37.67394256591797,2.5679659843444824,11.418474197387695,-0.24647289514541626,30.873340606689453,-0.26291602849960327,-0.27579158544540405,90.7552490234375,94.5103988647461,0.2938138246536255,85.8240966796875,-0.10749232769012451,50.064945220947266,0.07079249620437622,57.8739128112793,-0.2660577893257141,26.11600685119629,50.633323669433594,95.84942626953125,95.92127990722656,25.20635223388672,42.726924896240234,-0.32160133123397827,95.21427154541016,0.0011698603630065918,58.640201568603516,-0.04300183057785034,-0.28580838441848755,11.969075202941895,76.8866958618164,-0.04056066274642944,41.373931884765625,25.655519485473633,-0.30271226167678833,30.679943084716797,0.3210335373878479,54.78641891479492,66.38592529296875,31.476531982421875,9.51121711730957,-0.2369517683982849,85.90028381347656,83.09847259521484,0.4666213393211365,10.796728134155273,15.508170127868652,60.405025482177734,0.335374116897583,68.87254333496094,1.6773419380187988,59.219512939453125,26.22655487060547,93.50309753417969,72.88771057128906,-0.22486287355422974,0.7311856150627136,65.55381774902344,47.683258056640625,-0.1949257254600525,54.02833938598633,-0.18038028478622437,83.13297271728516,58.494571685791016,0.07821589708328247,-0.12427443265914917,42.37989044189453,3.715442657470703,-0.14705711603164673,0.19050800800323486,35.10255432128906,47.90327835083008,93.12688446044922,16.553050994873047,29.99474334716797,18.625585556030273,30.04543685913086,-0.2901490330696106,40.31147003173828,1.3732469081878662,11.541536331176758,44.13164138793945,-0.10802292823791504,33.79308319091797,0.024264991283416748,-0.24760395288467407,30.47599220275879,95.9677505493164,15.468735694885254,-0.15381968021392822,7.277955055236816,78.14400482177734,0.1630704402923584,-0.15465962886810303,0.4438081979751587,-0.10633355379104614,-0.27700895071029663,40.94833755493164,0.09764915704727173,29.593608856201172,95.52405548095703,88.25289916992188,7.221101760864258,-0.051530539989471436,-0.31519633531570435,17.143375396728516,15.101675987243652,33.85743713378906,66.89749908447266,49.69538497924805,0.07996231317520142,-0.2710515856742859,50.17953109741211,0.16992688179016113,66.45086669921875,39.092533111572266,59.649269104003906,0.5383992791175842,57.34723663330078,31.95240020751953,-0.1013900637626648,29.069580078125,88.8060302734375,60.18737030029297,25.353605270385742,0.2222503423690796,-0.04140758514404297,29.322465896606445,94.93738555908203,79.75093078613281,58.068389892578125,4.2013020515441895,-0.22791403532028198,64.04377746582031,1.2983598709106445,54.68208694458008,17.54792022705078,75.85187530517578,30.220792770385742,83.41441345214844,46.85487365722656,-0.2816118597984314,93.87968444824219,4.928478240966797,-0.21514242887496948,-0.2625557780265808,71.03466796875,-0.05441051721572876,57.448753356933594,0.2415149211883545,-0.14972347021102905,-0.2785034775733948,71.4664077758789,28.18582534790039,0.35891011357307434,4.79612398147583,-0.04590499401092529,37.25714111328125,27.516159057617188,-0.30495887994766235,2.120412826538086,34.919105529785156,-0.27573972940444946,40.05054473876953,14.636301040649414,26.092510223388672,7.113287925720215,-0.2167101502418518,-0.24610179662704468,44.831382751464844,33.97207260131836,0.18594199419021606,62.81712341308594,70.03577423095703,8.157269477844238,39.26056671142578,-0.2717389464378357,84.6413345336914,41.09202194213867,52.35905456542969,19.952972412109375,25.97805404663086,54.0689811706543,23.03667449951172,95.92130279541016,14.994902610778809,-0.1652490496635437,-0.05498337745666504,17.622913360595703,15.996894836425781,16.8568058013916,-0.20282405614852905,0.34040766954421997,-0.22692769765853882,0.3169403076171875,8.628986358642578,-0.01867574453353882,-0.25237685441970825,1.1038172245025635,0.8186377286911011,68.55648803710938,95.44189453125,85.5692138671875,25.440690994262695,14.179224967956543,32.88650131225586,-0.2641832232475281,-0.17527669668197632,29.742958068847656,73.72282409667969,-0.012784481048583984,85.63138580322266,89.89766693115234,0.28337568044662476,88.33152770996094,2.1560699939727783,-0.25852447748184204,-0.21708756685256958,1.8375364542007446,-0.04424548149108887,41.875465393066406,-0.31757527589797974,-0.18086451292037964,0.21800261735916138,0.2507994771003723,91.0137710571289,-0.20713084936141968,35.974422454833984,43.78312301635742,18.117382049560547,82.18254089355469,55.17707824707031,34.623477935791016,53.21455383300781,54.7669792175293,5.30863094329834,5.432284355163574,91.52802276611328,60.021324157714844,32.40350341796875,83.31038665771484,93.11646270751953,61.60336685180664,-0.2025342583656311,49.24946594238281,38.28076171875,38.96489715576172,-0.2610127329826355,77.88890075683594,94.65584564208984,28.869226455688477,73.41435241699219,84.76156616210938,48.1412353515625,32.21843338012695,-0.034173548221588135,79.59505462646484,-0.21939271688461304,33.938331604003906,55.763797760009766,30.3984432220459,-0.25216180086135864,90.34635925292969,24.91522216796875,80.8466796875,-0.08481490612030029,51.313385009765625,69.5102767944336,24.5349063873291,-0.2803341746330261,0.786737859249115,95.88172149658203,87.12794494628906,75.69966888427734,32.104732513427734,81.34344482421875,1.3645787239074707,80.236328125,41.26284408569336,36.704654693603516,68.11969757080078,1.4491851329803467,-0.2824257016181946,92.84242248535156,75.05876159667969,71.43812561035156,35.997982025146484,-0.2716047167778015,-0.24891716241836548,92.90642547607422,-0.06011718511581421,95.52400207519531,58.839141845703125,-0.2793411612510681,-0.2677404284477234,54.166072845458984,48.10062026977539,13.259577751159668,-0.09726810455322266,2.1821048259735107,-0.30388909578323364,3.033801317214966,-0.007237255573272705,-0.269996702671051,-0.2719631791114807,82.79246520996094,-0.2581271529197693,20.548097610473633,65.02334594726562,-0.2976763844490051,-0.26017361879348755,24.909832000732422,38.01927947998047,-0.22506409883499146,-0.19762581586837769,3.3618128299713135,0.07348477840423584,50.01771926879883,60.413795471191406,-0.1183273196220398,-0.2643182873725891,-0.2683320641517639,6.160092830657959,-0.005149722099304199,83.39930725097656,-0.17278772592544556,-0.2695600390434265,79.14403533935547,74.94246673583984,29.011808395385742,-0.2582436203956604,1.9754705429077148,82.17501068115234,75.53108215332031,39.72738265991211,-0.31269389390945435,-0.28009599447250366,0.12892282009124756,-0.2937319874763489,90.72073364257812,64.23501586914062,5.497340202331543,47.109893798828125,0.04418802261352539,67.1331787109375,19.25420379638672,-0.16562968492507935,52.09403991699219,51.191627502441406,16.242841720581055,76.26542663574219,62.463279724121094,75.53018188476562,32.69342041015625,5.421748161315918,8.971256256103516,22.54701805114746,-0.18641036748886108,44.50636672973633,4.685100555419922,56.834999084472656,80.6009292602539,-0.25515860319137573,20.8330020904541,3.548543691635132,70.9139175415039,0.02670300006866455,86.9315414428711,53.96534729003906,29.824079513549805,-0.2640346884727478,60.03997802734375,79.55977630615234,46.7242317199707,2.2852489948272705,86.69224548339844,96.08040618896484,69.72068786621094,59.07162094116211,78.21248626708984,-0.27211421728134155,37.614051818847656,29.05157470703125,88.61014556884766,79.73573303222656,-0.2668505311012268,-0.2964112162590027,36.478790283203125,41.826194763183594,80.05045318603516,41.469791412353516,-0.25946468114852905,73.94827270507812,83.63211822509766,21.589067459106445,-0.007946312427520752,90.00721740722656,77.82247161865234,96.06471252441406,96.08306121826172,54.933231353759766,-0.25483185052871704,26.40937042236328,73.43025207519531,35.817657470703125,42.72785949707031,-0.2675362229347229,94.7309341430664,0.4191805124282837,0.33473414182662964,76.66981506347656,4.506288051605225,20.81958770751953,39.645851135253906,23.48643684387207,40.69156265258789,-0.236158549785614,62.39647674560547,18.302553176879883,1.6719121932983398,64.22314453125,6.258443832397461,-0.27511876821517944,58.087703704833984,-0.13724863529205322,-0.18700510263442993,48.49728012084961,65.89083862304688,-0.22179168462753296,-0.156110942363739,81.70201110839844,68.19947052001953,82.90042114257812,26.328535079956055,-0.21265357732772827,61.64018249511719,30.290771484375,50.14254379272461,-0.1929360032081604,-0.2504275441169739,0.11469024419784546,-0.2806984782218933,-0.22695881128311157,96.14061737060547,-0.27232807874679565,46.81571960449219,95.06200408935547,23.100299835205078,-0.26105910539627075,25.536022186279297,60.28005599975586,60.5088005065918,1.0173470973968506,61.390769958496094],\"type\":\"scatter3d\"},{\"hovertemplate\":\"Sample=BELOW<br>Time=%{x}<br>S1=%{y}<br>Result=%{z}<extra></extra>\",\"legendgroup\":\"BELOW\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"BELOW\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.8908596038818359,0.9740158319473267,0.9648112654685974,0.2487262487411499,0.13218683004379272,0.8745080828666687,0.7027513980865479,0.9616541862487793,0.6490280032157898,0.21838080883026123,0.8298468589782715,0.99267578125,0.06504589319229126,0.9339852333068848,0.5465219616889954,0.45712530612945557,0.9656481146812439,0.9629837870597839,0.9688939452171326,0.983020544052124,0.3098897337913513,0.5297678112983704,0.6851894855499268,0.5304864645004272,0.38654106855392456,0.013998806476593018,0.9615844488143921,0.7748717069625854,0.78672194480896,0.0989343523979187,0.9188843965530396,0.125873863697052,0.3181382417678833,0.32053089141845703,0.9019126296043396,0.22628170251846313,0.6195203065872192,0.9819456934928894,0.44971734285354614,0.9535903334617615,0.44360196590423584,0.969410240650177,0.3131052255630493,0.7614688277244568,0.681923508644104,0.9624822735786438,0.015619218349456787,0.7219532132148743,0.20890772342681885,0.10012197494506836,0.859979510307312,0.4456392526626587,0.463609516620636,0.5538904666900635,0.41692131757736206,0.09104043245315552,0.885899543762207,0.2582947015762329,0.6092672944068909,0.7056204676628113,0.7436184287071228,0.3580995798110962,0.7672039270401001,0.9578795433044434,0.2092447280883789,0.4466506242752075,0.5965800881385803,0.3303687572479248,0.6656239032745361,0.9738689661026001,0.13593530654907227,0.7072393298149109,0.7770960927009583,0.9015699625015259,0.9979097843170166,0.4014039635658264,0.15481001138687134,0.07334256172180176,0.8228603005409241,0.9672591090202332,0.3831489086151123,0.4337790012359619,0.9401710629463196,0.9844330549240112,0.9414133429527283,0.3103015422821045,0.8187078237533569,0.39865219593048096,0.5723189115524292,0.30532747507095337,0.2121342420578003,0.4470558762550354,0.386421263217926,0.8831468820571899,0.9956003427505493,0.11197167634963989,0.6855261325836182,0.6547995805740356,0.7043382525444031,0.17773550748825073,0.8024938106536865,0.1188850998878479,0.2473723292350769,0.4452900290489197,0.6134247779846191,0.6857349872589111,0.7647959589958191,0.3432936668395996,0.6353506445884705,0.9473426938056946,0.4615383744239807,0.5877166986465454,0.9724630117416382,0.9574559926986694,0.3879055380821228,0.946565568447113,0.9173676371574402,0.4605046510696411,0.9951164126396179,0.6573823094367981,0.5743154287338257,0.2461305856704712,0.24885714054107666,0.69364994764328,0.3082415461540222,0.4991818070411682,0.14429205656051636,0.29614317417144775,0.3476545214653015,0.8787304759025574,0.7557662725448608,0.0710451602935791,0.447831392288208,0.2846320867538452,0.6929354071617126,0.672362744808197,0.49807339906692505,0.1664465069770813,0.9640054702758789,0.43147361278533936,0.6751763820648193,0.9688141345977783,0.9330470561981201,0.6833409070968628],\"y\":[4.4329833984375,99.11749267578125,82.85765838623047,5.909271240234375,26.72748565673828,22.536903381347656,26.043563842773438,127.19442749023438,33.13916015625,5.304046630859375,30.667083740234375,107.99775695800781,10.075454711914062,101.69908142089844,22.13538360595703,26.28802490234375,33.51072692871094,101.64268493652344,25.648147583007812,146.216552734375,2.3141021728515625,31.779556274414062,33.90782165527344,23.14354705810547,23.125503540039062,17.798324584960938,105.39445495605469,9.330642700195312,13.03985595703125,20.769546508789062,34.52247619628906,23.16320037841797,28.045211791992188,3.94647216796875,0.1212921142578125,1.8629608154296875,15.366973876953125,91.69842529296875,26.380653381347656,105.03276062011719,24.28864288330078,60.49604797363281,30.24573516845703,15.347396850585938,15.843521118164062,105.21453094482422,18.46844482421875,2.2366485595703125,20.607894897460938,13.950042724609375,27.0572509765625,22.742942810058594,13.655593872070312,21.429397583007812,29.454116821289062,16.795074462890625,34.34851837158203,147.3782958984375,2.180267333984375,11.889556884765625,5.062408447265625,2.135101318359375,9.034149169921875,125.20709228515625,9.300048828125,28.69322967529297,28.131759643554688,3.49658203125,17.495758056640625,7.0231475830078125,22.645973205566406,15.742324829101562,12.884811401367188,13.61578369140625,105.74693298339844,20.912826538085938,22.92932891845703,7.77508544921875,25.56415557861328,97.93087768554688,5.30682373046875,10.61279296875,124.36540222167969,19.513809204101562,105.8958740234375,18.233428955078125,30.96832275390625,22.49169158935547,22.538040161132812,14.663116455078125,25.402381896972656,147.06109619140625,147.92311096191406,21.149139404296875,63.41735076904297,15.578567504882812,25.76178741455078,13.37847900390625,17.645767211914062,19.807052612304688,16.760269165039062,23.68962860107422,2.39312744140625,19.04058837890625,9.773513793945312,28.070945739746094,2.909912109375,1.90203857421875,17.568359375,83.3346176147461,4.5332794189453125,17.892669677734375,123.45321655273438,105.26663208007812,11.0654296875,27.59648895263672,35.5228271484375,21.220672607421875,19.920394897460938,1.179656982421875,22.29723358154297,11.274948120117188,7.650177001953125,148.7813720703125,13.498794555664062,6.3118438720703125,11.279693603515625,28.707603454589844,22.19269561767578,28.09368896484375,20.885040283203125,27.436080932617188,1.980743408203125,0.2301177978515625,30.98249053955078,14.129119873046875,22.74005126953125,2.7212371826171875,78.90618896484375,11.652984619140625,8.720443725585938,27.396896362304688,30.78929901123047,7.724151611328125],\"z\":[-0.28131693601608276,48.49191665649414,32.54149627685547,-0.26453763246536255,-0.06382405757904053,-0.20856887102127075,-0.16209059953689575,76.78773498535156,-0.0142403244972229,-0.2656216025352478,-0.10901814699172974,57.25383377075195,-0.24413472414016724,51.6868782043457,-0.19038993120193481,-0.1299130916595459,-0.07511866092681885,51.21144104003906,-0.1887931227684021,95.02365112304688,-0.2828114628791809,-0.02350407838821411,-5.739927291870117e-05,-0.17902439832687378,-0.16568678617477417,-0.18891006708145142,55.060028076171875,-0.2663455605506897,-0.2549908757209778,-0.16421467065811157,-0.04126399755477905,-0.13376152515411377,-0.07232218980789185,-0.27303165197372437,-0.32448214292526245,-0.285403311252594,-0.2402762770652771,40.96346664428711,-0.12738782167434692,54.80830383300781,-0.15713149309158325,10.004295349121094,-0.011182963848114014,-0.24510544538497925,-0.24018746614456177,54.86298751831055,-0.18220490217208862,-0.29103630781173706,-0.17732173204421997,-0.2252456545829773,-0.16519838571548462,-0.1758001446723938,-0.24283021688461304,-0.19732850790023804,-0.05946904420852661,-0.20412272214889526,-0.03839319944381714,96.98284912109375,-0.28973037004470825,-0.2570568919181824,-0.2770325541496277,-0.2854716181755066,-0.26700323820114136,74.88257598876953,-0.25247591733932495,-0.08323413133621216,-0.11903613805770874,-0.27536338567733765,-0.2309820055961609,-0.27519625425338745,-0.14309394359588623,-0.24154561758041382,-0.25530916452407837,-0.2557075619697571,54.884578704833984,-0.19075196981430054,-0.14135116338729858,-0.2534788250923157,-0.178386390209198,47.387420654296875,-0.26955610513687134,-0.25445109605789185,74.33404541015625,-0.2326299548149109,55.87117004394531,-0.2074633240699768,-0.10173743963241577,-0.17424672842025757,-0.18854659795761108,-0.23121017217636108,-0.10813558101654053,96.86727142333984,96.93755340576172,-0.218691885471344,12.852463722229004,-0.2149612307548523,-0.16379815340042114,-0.25017493963241577,-0.2317485213279724,-0.18302792310714722,-0.23996669054031372,-0.1240338683128357,-0.2807055115699768,-0.2097129225730896,-0.26178163290023804,-0.13226813077926636,-0.2867618203163147,-0.2875111699104309,-0.22927993535995483,33.211917877197266,-0.273762047290802,-0.22520452737808228,72.93549346923828,54.990325927734375,-0.25138622522354126,-0.16697078943252563,-0.012698233127593994,-0.19240981340408325,-0.2308599352836609,-0.30177468061447144,-0.1909765601158142,-0.24577027559280396,-0.25928252935409546,96.70844268798828,-0.23763030767440796,-0.2691037058830261,-0.24183791875839233,-0.051091015338897705,-0.17278820276260376,-0.15434974431991577,-0.21397799253463745,-0.029474973678588867,-0.28875070810317993,-0.31631773710250854,-0.07979094982147217,-0.2476077675819397,-0.18047016859054565,-0.2761121392250061,28.740827560424805,-0.250413715839386,-0.2661086916923523,-0.1711861491203308,-0.12164974212646484,-0.26890355348587036],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"S1\"}},\"zaxis\":{\"title\":{\"text\":\"Result\"}}},\"legend\":{\"title\":{\"text\":\"Sample\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"height\":400,\"width\":500},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('e01d82d6-7f25-4db9-adfd-a5bcfb949c4d');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["eqLossFn= 'calculateLossUsingKLMinMax'\n","sample_method= \"U\"\n","lr = 0.001\n","net10 = AmericanOptionNet( NL = 5 , NN = 100 )\n","net10.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","bsequation_net10 = AmericanBlackScholesSingleStock(net10)\n","bsequation_net10.beta = bsequation_net10.beta/1000.0\n","trainAmss_net10 = TrainAmericanBlackScholesSingleStock( net10 , bsequation_net10 , BATCH_SIZE = 2**9 , debug = True )\n","trainAmss_net10.hook_interval = 1000\n","trainAmss_net10.use_early_stop = True\n","trainAmss_net10.early_stop_patience = 2000\n","trainAmss_net10.train( epoch = 10000 , lr = lr, eqLossFn= eqLossFn , sample_method_X= sample_method )\n","torch.save(net10.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainAmss_net10.stop_epoch}_{str(lr).replace('.','p')}_{net10.NL}_{net10.NN}\")\n","df_net10 = trainAmss_net10.history_surfaces_hooks[trainAmss_net10.history_surfaces_hooks.Epoch == max(trainAmss_net10.history_surfaces_hooks.Epoch)]\n","fig_net10 = px.scatter_3d(df_net10, x='Time', y='S1', z='Result',color='Sample', width=500, height=400)\n","fig_net10.show()"]},{"cell_type":"markdown","metadata":{"id":"UWku2GG_fgtM"},"source":["#### American Call - KLMinMax, Log Normal Sampling , NL=5, NN=100, lr = 0.0001\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":203075,"status":"ok","timestamp":1651763679308,"user":{"displayName":"Hans Roggeman","userId":"10574434403170915342"},"user_tz":-60},"id":"QxiXWvRgfgtN","outputId":"b13aa99c-7876-4632-f289-1561fe1b8609"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - lr 0.0001 -  rebalanced loss: -0.2519979248046875 - original loss: 152.40261840820312 - L1 loss 7.3386993408203125\n","Epoch 999 - lr 0.0001 -  rebalanced loss: -1048.997379989624 - original loss: 57.97100830078125 - L1 loss 4.216172218322754\n","Epoch 1999 - lr 0.0001 -  rebalanced loss: -1084.1251049804687 - original loss: 49.54441833496094 - L1 loss 3.2189278602600098\n","Epoch 2999 - lr 0.0001 -  rebalanced loss: -1090.7400979003905 - original loss: 45.40711212158203 - L1 loss 2.8686468601226807\n","Epoch 3999 - lr 0.0001 -  rebalanced loss: -1094.1747880859375 - original loss: 36.71575164794922 - L1 loss 3.642162561416626\n","Epoch 4999 - lr 0.0001 -  rebalanced loss: -1092.887148864746 - original loss: 15.451629638671875 - L1 loss 1.5028152465820312\n","Epoch 5999 - lr 0.0001 -  rebalanced loss: -1088.607067779541 - original loss: 11.846174240112305 - L1 loss 1.1222398281097412\n","Epoch 6999 - lr 0.0001 -  rebalanced loss: -1076.703608947754 - original loss: 9.138808250427246 - L1 loss 0.7189598083496094\n","Early Stop at epoch 7684, original loss: 4.941232681274414 with patience 1000\n"]},{"data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"e9964b1f-3576-4010-9a86-f5c56d294228\" class=\"plotly-graph-div\" style=\"height:400px; width:500px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e9964b1f-3576-4010-9a86-f5c56d294228\")) {                    Plotly.newPlot(                        \"e9964b1f-3576-4010-9a86-f5c56d294228\",                        [{\"hovertemplate\":\"Sample=ABOVE<br>Time=%{x}<br>S1=%{y}<br>Result=%{z}<extra></extra>\",\"legendgroup\":\"ABOVE\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"ABOVE\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.5135614275932312,0.4385460615158081,0.020239293575286865,0.01439744234085083,0.44421136379241943,0.2328040599822998,0.571111261844635,0.7564383149147034,0.6821102499961853,0.037828266620635986,0.42603373527526855,0.8998266458511353,0.2406579852104187,0.7697247266769409,0.5427888631820679,0.32865309715270996,0.05910736322402954,0.6742473244667053,0.5158394575119019,0.8748012781143188,0.687932550907135,0.5716802477836609,0.6143971681594849,0.5230656266212463,0.5750230550765991,0.8283519148826599,0.2564889192581177,0.44602131843566895,0.5882255434989929,0.17948174476623535,0.45164090394973755,0.36389297246932983,0.7266470789909363,0.20547062158584595,0.41001081466674805,0.08365386724472046,0.5483413934707642,0.8068056702613831,0.16725653409957886,0.296048104763031,0.05409198999404907,0.7979791164398193,0.18246954679489136,0.8376652598381042,0.8912240266799927,0.7789137363433838,0.21298503875732422,0.40207135677337646,0.34643852710723877,0.13950389623641968,0.2596312165260315,0.02724635601043701,0.09164714813232422,0.19302314519882202,0.41241997480392456,0.1371631622314453,0.5382238030433655,0.47967731952667236,0.4166834354400635,0.3762807250022888,0.3733235001564026,0.3503744602203369,0.642306923866272,0.695737898349762,0.6575746536254883,0.4078386425971985,0.8521405458450317,0.9178708791732788,0.6662275791168213,0.878654420375824,0.8059126138687134,0.6672756671905518,0.7953627705574036,0.8809894919395447,0.9495599865913391,0.7402951717376709,0.821735143661499,0.030546724796295166,0.401445209980011,0.22139030694961548,0.7675566673278809,0.9176382422447205,0.9429610967636108,0.41048306226730347,0.7348843812942505,0.4572172164916992,0.18857043981552124,0.39443981647491455,0.21679741144180298,0.7107325196266174,0.855614185333252,0.03958940505981445,0.1397342085838318,0.6572860479354858,0.5442613363265991,0.9207062125205994,0.14581799507141113,0.06524741649627686,0.7692850232124329,0.6653064489364624,0.7082798480987549,0.1687389612197876,0.6047409772872925,0.01004338264465332,0.45606672763824463,0.42951852083206177,0.4706000089645386,0.5806606411933899,0.4563690423965454,0.9609128832817078,0.25449836254119873,0.8924989104270935,0.6310573220252991,0.8345849514007568,0.13487190008163452,0.034380435943603516,0.5731569528579712,0.9636865258216858,0.6426913142204285,0.9995222687721252,0.01297992467880249,0.4163646101951599,0.7085998058319092,0.7292231321334839,0.625680685043335,0.31017833948135376,0.20995551347732544,0.7595477104187012,0.72906494140625,0.8606724739074707,0.18979573249816895,0.1388738751411438,0.09312784671783447,0.5215897560119629,0.06827205419540405,0.1454850435256958,0.4456753134727478,0.7492473125457764,0.9241143465042114,0.23097193241119385,0.5036708116531372,0.4289611577987671,0.2803112864494324,0.9760974645614624,0.09606653451919556,0.8910231590270996,0.035560548305511475,0.10096997022628784,0.20948517322540283,0.8294396996498108,0.34479260444641113,0.8415067791938782,0.8391851782798767,0.1961219310760498,0.7901508212089539,0.8526875972747803,0.5255178809165955,0.77210533618927,0.2325763702392578,0.9932026267051697,0.232005774974823,0.12363970279693604,0.27081459760665894,0.08595049381256104,0.2214915156364441,0.13347095251083374,0.14253181219100952,0.068805992603302,0.2750731110572815,0.4250146150588989,0.0025216341018676758,0.3757164478302002,0.025288879871368408,0.8378210663795471,0.8364049196243286,0.3418498635292053,0.270267128944397,0.8666030168533325,0.4069311022758484,0.8271103501319885,0.5597246289253235,0.7117714881896973,0.66187983751297,0.1642511487007141,0.5499483942985535,0.7376053929328918,0.7762075662612915,0.1697503924369812,0.06246352195739746,0.9960603713989258,0.050358355045318604,0.2819415330886841,0.8799623250961304,0.1693316102027893,0.6850530505180359,0.05636036396026611,0.6528052687644958,0.5207661986351013,0.9023649096488953,0.9489999413490295,0.9611814022064209,0.7403247356414795,0.720137357711792,0.7029491066932678,0.9137784838676453,0.7738860249519348,0.516122043132782,0.6538206934928894,0.6304298043251038,0.30363351106643677,0.8892946839332581,0.604100227355957,0.5200378894805908,0.32902801036834717,0.5483802556991577,0.6372259855270386,0.5782478451728821,0.2937415838241577,0.13691431283950806,0.9081477522850037,0.9140977263450623,0.19127517938613892,0.7413738369941711,0.008417785167694092,0.6392834782600403,0.1956499218940735,0.5613327622413635,0.32727038860321045,0.9753802418708801,0.791491687297821,0.21764463186264038,0.3907591700553894,0.16066551208496094,0.32321441173553467,0.5920155644416809,0.658522367477417,0.8735620379447937,0.14012736082077026,0.441534161567688,0.4295990467071533,0.2748871445655823,0.44642120599746704,0.7325599789619446,0.07152140140533447,0.7036307454109192,0.16857510805130005,0.8009802103042603,0.6066542267799377,0.19689035415649414,0.9251107573509216,0.3661119341850281,0.31711578369140625,0.26471465826034546,0.10932803153991699,0.25167757272720337,0.4102431535720825,0.9773088693618774,0.5429402589797974,0.8508079051971436,0.20912820100784302,0.5466542840003967,0.9771486520767212,0.6808663010597229,0.8005297183990479,0.10369735956192017,0.6939228177070618,0.5376673340797424,0.813817024230957,0.6742563843727112,0.2572603225708008,0.4741222858428955,0.8472596406936646,0.7636732459068298,0.018342673778533936,0.482552170753479,0.07335841655731201,0.09838497638702393,0.5761566162109375,0.1039089560508728,0.3817914128303528,0.06713944673538208,0.6377872228622437,0.497885525226593,0.6854082345962524,0.15081584453582764,0.23165637254714966,0.6035133004188538,0.9211255311965942,0.04190856218338013,0.3741886615753174,0.0020638704299926758,0.7434590458869934,0.945656955242157,0.10589176416397095,0.3537474274635315,0.5712305903434753,0.8045474290847778,0.542087972164154,0.3206533193588257,0.9113065600395203,0.921184241771698,0.4975791573524475,0.5680044889450073,0.7352944612503052,0.8300705552101135,0.28620803356170654,0.28068387508392334,0.7452805042266846,0.6735242009162903,0.4235309958457947,0.17747122049331665,0.3468356728553772,0.7481211423873901,0.6433751583099365,0.8136520981788635,0.7872453927993774,0.9203413128852844,0.9769536852836609,0.440112829208374,0.4215623140335083,0.09466129541397095,0.2296244502067566,0.49062997102737427,0.8437134623527527,0.4225189685821533,0.8294879794120789,0.7288223505020142,0.8858516216278076,0.10298889875411987,0.6541124582290649,0.3028782606124878,0.16094934940338135,0.5931974053382874,0.8439139127731323,0.5144275426864624,0.29303646087646484,0.9633072018623352,0.4274834394454956,0.17103427648544312,0.3024370074272156,0.8719239234924316,0.4264577031135559,0.47678321599960327,0.2568194270133972,0.21370524168014526,0.6741265058517456,0.7640024423599243,0.5193564295768738,0.842025637626648,0.42161935567855835,0.7151557207107544,0.3761965036392212,0.10719597339630127,0.014085948467254639,0.2207554578781128,0.9027002453804016,0.051772117614746094,0.09634333848953247,0.6670564413070679,0.49571460485458374,0.5988247990608215,0.1309812068939209,0.8647363781929016,0.4685715436935425,0.5430962443351746,0.8677518367767334,0.178902268409729,0.5974042415618896,0.314106822013855,0.16346794366836548,0.6868712306022644,0.11277788877487183,0.1734241247177124,0.20348811149597168,0.08521789312362671,0.7231228947639465,0.03709036111831665,0.4085763096809387,0.6593757271766663,0.7918210029602051,0.29846107959747314,0.6443923711776733,0.5766584277153015,0.9551035761833191,0.19607692956924438,0.878984808921814,0.015742778778076172,0.5247107744216919,0.774647057056427,0.642823338508606,0.6772156357765198,0.7552734613418579,0.9605447053909302,0.7218698263168335,0.6339549422264099,0.4868096709251404,0.4911520481109619,0.04791301488876343,0.6449074149131775,0.4428257942199707,0.258503794670105,0.9958516359329224,0.1296980381011963,0.25661230087280273,0.9394587874412537,0.2921931743621826,0.28154194355010986,0.48174864053726196,0.4958862066268921,0.014362812042236328,0.5315305590629578,0.6887131333351135,0.40695899724960327,0.3119041919708252,0.01842653751373291,0.8442698121070862,0.8350254893302917,0.5003796815872192,0.4002869129180908,0.17628508806228638,0.3523193597793579,0.265034556388855,0.32749032974243164,0.9366379380226135,0.038082778453826904,0.054493069648742676,0.33150243759155273,0.6791636943817139,0.7208967208862305,0.8978974223136902,0.6265392303466797,0.8439652919769287,0.00816500186920166,0.473082959651947,0.49086785316467285,0.020043492317199707,0.04957515001296997,0.7639451622962952,0.564397394657135,0.07431477308273315,0.09396058320999146,0.7506147623062134,0.10241299867630005,0.7318394184112549,0.22524809837341309,0.7385975122451782,0.825689971446991,0.6891594529151917,0.021581292152404785,0.7665455937385559,0.08215987682342529,0.939172089099884,0.33453458547592163,0.5819448232650757,0.14426261186599731,0.8933697938919067,0.735200047492981,0.6955142617225647,0.12715578079223633,0.4350733160972595,0.3741708993911743,0.9026482701301575,0.519707441329956,0.1917998194694519,0.32643944025039673,0.266803503036499,0.3362298607826233,0.31236231327056885,0.39965349435806274,0.5527027249336243,0.20730936527252197,0.3274722695350647,0.2054218053817749,0.4547181725502014,0.9542849659919739,0.12094295024871826,0.2890205383300781,0.3761315941810608,0.606181263923645,0.6776906847953796,0.405409038066864,0.3109590411186218,0.4129652976989746,0.41849011182785034],\"y\":[37.610389709472656,55.18802261352539,55.41781234741211,52.02663040161133,68.65225219726562,48.142364501953125,53.16816711425781,61.951595306396484,49.514869689941406,65.95623779296875,54.61416244506836,65.30392456054688,54.704803466796875,48.05168914794922,57.731075286865234,42.75463104248047,33.514862060546875,46.83163070678711,44.95749282836914,42.32366180419922,45.189727783203125,41.45449447631836,48.14808654785156,53.841453552246094,49.18669891357422,55.3278694152832,49.89905548095703,62.56162643432617,42.79433059692383,77.13156127929688,30.854955673217773,54.20262908935547,60.98762130737305,53.865074157714844,69.37789154052734,59.348297119140625,66.63185119628906,39.090789794921875,33.08673858642578,45.766483306884766,40.94868087768555,55.08192825317383,45.877464294433594,43.53144454956055,48.37309646606445,56.58555603027344,38.54079055786133,59.79890060424805,40.06016540527344,53.19206619262695,44.257240295410156,56.196075439453125,69.1642837524414,56.13325881958008,85.13292694091797,69.22393035888672,70.7206802368164,56.79652786254883,40.8687629699707,61.77279281616211,59.131980895996094,55.66834259033203,51.33491897583008,41.75343322753906,40.79466247558594,63.683502197265625,51.57928466796875,48.04303741455078,80.95198822021484,61.94929504394531,58.713523864746094,50.98469924926758,61.66315841674805,52.31251525878906,50.65535354614258,49.38425064086914,50.72416687011719,48.77534484863281,45.635841369628906,49.48957443237305,48.542503356933594,47.6119270324707,49.02025604248047,61.43962097167969,37.13850021362305,46.276519775390625,42.151771545410156,35.839263916015625,36.325408935546875,68.50051879882812,36.904396057128906,67.04090881347656,63.751060485839844,48.19009017944336,41.595794677734375,50.6761474609375,34.44304656982422,46.97134017944336,57.562477111816406,39.306846618652344,36.025611877441406,41.87489318847656,48.2698860168457,59.793556213378906,61.73044967651367,74.0176773071289,87.82181549072266,46.46282958984375,37.18223190307617,58.74671936035156,41.8988151550293,77.76171875,68.42881774902344,51.453346252441406,56.88406753540039,67.32794189453125,62.627471923828125,85.02912902832031,39.217864990234375,50.311649322509766,50.17432403564453,44.43070983886719,26.51802635192871,66.1667709350586,38.997310638427734,50.03435134887695,38.78530502319336,52.79351043701172,57.66961669921875,40.140560150146484,48.73029708862305,51.20238494873047,48.17072296142578,42.87432098388672,51.7461051940918,48.45780944824219,54.31627655029297,59.7600212097168,39.96867752075195,72.60208892822266,40.42337417602539,35.20690155029297,52.36638259887695,64.70805358886719,47.368797302246094,39.4944953918457,38.42444610595703,39.3033447265625,53.1273193359375,59.66239547729492,60.947723388671875,38.28887176513672,54.40106201171875,66.12149810791016,34.04029083251953,73.64110565185547,58.133583068847656,64.74092864990234,42.724510192871094,52.030967712402344,32.85475540161133,40.797359466552734,63.324005126953125,65.42758178710938,50.00478744506836,55.236427307128906,63.56328582763672,55.55971908569336,71.80464172363281,40.75730895996094,33.37141418457031,39.35923385620117,41.8535041809082,28.360977172851562,57.62059783935547,65.46478271484375,55.37041473388672,44.66092300415039,35.36328125,31.68750762939453,63.9120979309082,64.74888610839844,65.03347778320312,56.54218673706055,79.68657684326172,61.443321228027344,77.6759033203125,30.48674201965332,39.308170318603516,49.43943786621094,62.767539978027344,71.89057159423828,69.36468505859375,51.58674621582031,74.7018051147461,61.609344482421875,48.584739685058594,30.71198081970215,70.8664779663086,48.83160400390625,65.63475799560547,45.30594253540039,48.41455078125,47.450687408447266,38.512290954589844,68.84088134765625,68.7322769165039,51.69766616821289,49.03019714355469,41.469173431396484,61.71778869628906,29.549983978271484,52.415950775146484,47.33791732788086,28.447786331176758,59.60919189453125,41.77538299560547,45.38736343383789,41.1019401550293,56.98875045776367,55.234222412109375,43.545745849609375,37.13616180419922,60.23392105102539,50.826690673828125,48.94846725463867,44.92935562133789,42.3089485168457,79.48603057861328,57.41000747680664,55.471004486083984,48.04304122924805,55.41713333129883,57.79785919189453,78.01934814453125,57.90693283081055,45.626224517822266,49.9803352355957,78.98653411865234,69.00419616699219,74.32610321044922,63.41985321044922,53.59458923339844,55.394805908203125,34.43576431274414,48.33423614501953,49.66122055053711,54.00975799560547,83.5851058959961,89.26140594482422,66.91558837890625,44.48247528076172,33.0000114440918,49.244327545166016,51.366085052490234,54.31437683105469,38.19519805908203,45.10248565673828,47.63959884643555,36.32870101928711,54.86160659790039,72.24610137939453,46.91377639770508,74.98189544677734,70.46844482421875,34.00408172607422,57.355926513671875,46.8879280090332,59.853816986083984,68.39226531982422,61.534297943115234,71.74954223632812,61.83714294433594,45.09467315673828,67.30035400390625,27.581035614013672,41.53066635131836,55.70811462402344,32.016014099121094,52.98508834838867,34.83958053588867,37.305686950683594,59.489261627197266,56.84292984008789,32.11053466796875,63.976375579833984,44.779659271240234,73.36542510986328,78.49961853027344,69.22872161865234,57.729942321777344,54.11485290527344,56.506378173828125,49.44182586669922,50.4814453125,52.286415100097656,40.27267837524414,63.50091552734375,24.32936668395996,58.330101013183594,69.53758239746094,44.83413314819336,50.722755432128906,40.966041564941406,72.77650451660156,35.33495330810547,41.35416030883789,39.685909271240234,52.619903564453125,77.44037628173828,34.85348129272461,37.53620910644531,59.43064880371094,64.32117462158203,56.84783935546875,62.34825134277344,66.57353973388672,52.969879150390625,46.74248504638672,59.44424819946289,81.46074676513672,61.691070556640625,48.91262435913086,40.473114013671875,49.3306770324707,50.812767028808594,34.571868896484375,38.36229705810547,71.61808013916016,45.446128845214844,58.91984558105469,78.0416030883789,38.953121185302734,39.702571868896484,54.37468719482422,46.763916015625,53.479949951171875,50.80388641357422,39.184635162353516,43.76033401489258,73.5354995727539,77.91075897216797,52.89826965332031,84.93697357177734,43.537391662597656,58.87603759765625,46.08745193481445,58.53340530395508,59.45897674560547,48.0336799621582,57.428436279296875,36.34855270385742,56.212127685546875,67.45503234863281,63.54285430908203,46.06614303588867,37.93402099609375,44.88786315917969,57.00569152832031,30.74651336669922,32.188934326171875,38.926578521728516,54.2330436706543,65.45757293701172,32.427982330322266,41.783790588378906,55.88745880126953,67.73462677001953,25.640941619873047,63.53300094604492,34.75926208496094,37.26515197753906,36.172096252441406,70.1470947265625,54.41411209106445,57.878662109375,39.245384216308594,53.362361907958984,41.75334930419922,35.58827209472656,84.82405090332031,70.31837463378906,54.99169158935547,56.00355911254883,43.156803131103516,42.117942810058594,47.103084564208984,52.10953903198242,37.76306915283203,41.868751525878906,60.59239959716797,70.62218475341797,57.51103210449219,59.54150390625,36.23719024658203,57.35359191894531,51.74769592285156,45.7480354309082,69.43578338623047,49.7363166809082,45.9525146484375,86.66348266601562,58.989112854003906,38.025291442871094,55.85374069213867,80.30209350585938,33.62512969970703,70.88282012939453,58.24515151977539,49.880523681640625,63.46418380737305,79.12110900878906,53.6456413269043,48.2440071105957,52.127464294433594,40.630775451660156,60.87179183959961,34.942710876464844,47.66004943847656,77.0910415649414,49.43745803833008,81.56748962402344,63.471229553222656,78.92091369628906,48.482112884521484,68.98401641845703,32.10575485229492,64.5511245727539,76.94918060302734,45.88943862915039,50.06147766113281,75.06698608398438,56.60474395751953,47.916786193847656,59.109039306640625,82.60172271728516,70.94475555419922,30.302858352661133,30.542827606201172,59.108882904052734,45.194766998291016,48.16974639892578,61.08897399902344,44.36988830566406,38.74418258666992,28.585962295532227,56.07913589477539,54.42583465576172,49.71375274658203,47.55928039550781,82.06986236572266,48.45134735107422,41.651161193847656,71.38556671142578,55.78614044189453,58.351951599121094,59.194610595703125,38.01907730102539,49.33635330200195,53.66669464111328,53.40226364135742,39.82902526855469,54.84398651123047,58.07059860229492,55.42074966430664,85.29950714111328,53.246097564697266,60.574310302734375,73.92729949951172,48.525672912597656,35.60037612915039,60.153602600097656,61.54557418823242,49.4996337890625,54.613037109375,44.530303955078125,58.4705696105957,62.073699951171875,57.46706008911133,45.950443267822266,59.36092758178711,31.203022003173828,60.63016128540039,42.00279235839844],\"z\":[0.567168653011322,8.005877494812012,9.78941535949707,7.3984904289245605,20.190013885498047,4.127459526062012,5.691599369049072,12.865427017211914,2.44356107711792,18.502052307128906,7.585474967956543,15.610511779785156,8.445464134216309,1.2012379169464111,9.775421142578125,1.5214521884918213,1.0808136463165283,1.2117449045181274,1.3422651290893555,0.021585047245025635,0.7037826180458069,0.600938618183136,2.0844361782073975,6.487390041351318,2.867032289505005,6.149920463562012,4.997479438781738,14.520851135253906,0.6733388304710388,28.42729377746582,0.5571305155754089,7.530944347381592,12.055575370788574,7.939473628997803,20.943037033081055,12.743250846862793,18.029142379760742,0.1840108036994934,0.911387026309967,2.670682907104492,2.085702657699585,6.0958571434021,3.2884409427642822,0.09142929315567017,0.7026724815368652,7.604260444641113,1.1715216636657715,12.163473129272461,1.016493320465088,7.707812786102295,2.2587997913360596,10.365072250366211,21.272077560424805,9.762808799743652,35.70928955078125,21.263599395751953,21.953672409057617,9.209524154663086,0.9268351197242737,14.014235496520996,11.670788764953613,8.782611846923828,3.8842315673828125,0.3478235602378845,0.39744997024536133,15.659048080444336,2.789846897125244,0.45405811071395874,31.60380744934082,12.357354164123535,9.509683609008789,3.487661123275757,12.427754402160645,3.173802614212036,1.534363031387329,2.0280935764312744,2.3720285892486572,5.4253153800964355,2.096224308013916,4.927203178405762,1.432981014251709,0.3258645534515381,0.6971167922019958,13.609156608581543,0.29031461477279663,2.0730834007263184,1.90110182762146,0.693343997001648,0.9845665693283081,19.383665084838867,0.1700921654701233,19.453773498535156,16.37598419189453,1.8694381713867188,0.6761587262153625,1.726719617843628,0.9973356127738953,4.370985984802246,8.581259727478027,0.3681357502937317,0.32520049810409546,1.9027912616729736,2.198859930038452,13.301799774169922,13.72935676574707,25.285587310791016,38.18619155883789,1.5331931114196777,0.6403026580810547,8.755387306213379,1.5926761627197266,28.099363327026367,19.53058624267578,2.806950330734253,10.56735897064209,19.714914321899414,14.177083015441895,35.20768737792969,0.40084677934646606,1.0221991539001465,6.266809940338135,1.6162621974945068,0.3138626217842102,17.05320167541504,0.42416030168533325,4.812900066375732,1.205906867980957,4.319067001342773,8.879740715026855,0.09572428464889526,4.655620098114014,6.35382604598999,4.8258376121521,0.8814558982849121,7.0021843910217285,4.72492790222168,7.250865936279297,10.780923843383789,0.02629023790359497,24.226993560791016,0.6858640313148499,0.635166347026825,6.514193534851074,14.712637901306152,4.4128546714782715,0.07787114381790161,1.603635311126709,1.5684564113616943,7.374752521514893,10.323111534118652,13.370879173278809,0.16031545400619507,5.232767581939697,18.366851806640625,0.2562231421470642,24.083057403564453,10.210101127624512,15.5194091796875,1.8863310813903809,2.22233247756958,0.82268226146698,1.798525094985962,15.693967819213867,17.955198287963867,5.2309794425964355,9.264310836791992,16.205324172973633,9.738018989562988,23.433568954467773,0.8942049145698547,1.1591689586639404,0.8850371837615967,2.467600107192993,0.2174491286277771,8.295969009399414,17.458845138549805,8.858891487121582,0.06288784742355347,0.6659245491027832,0.22980409860610962,15.426444053649902,15.734319686889648,16.17161750793457,10.193116188049316,30.519649505615234,12.449583053588867,28.244958877563477,0.8365293145179749,1.6828899383544922,0.5957140326499939,15.712441444396973,23.503658294677734,19.736000061035156,6.471548557281494,25.504863739013672,14.71143913269043,2.093038320541382,0.4921630024909973,21.171960830688477,0.5863232612609863,15.707842826843262,0.5318842530250549,1.6326048374176025,1.300654411315918,0.08061116933822632,19.536439895629883,20.105022430419922,4.084270000457764,2.461606979370117,1.338484287261963,12.0809907913208,0.41259145736694336,5.37213659286499,3.2197108268737793,0.45409953594207764,11.112375259399414,0.6077119708061218,2.525503635406494,1.8239989280700684,7.290527820587158,5.554409503936768,2.3191957473754883,0.28332728147506714,13.669655799865723,3.541635513305664,4.746551513671875,1.1438581943511963,1.4255671501159668,29.648603439331055,8.323805809020996,9.141202926635742,3.250955820083618,9.307977676391602,10.686384201049805,28.873180389404297,9.431436538696289,0.12226468324661255,5.597405433654785,29.961591720581055,20.55209732055664,25.770315170288086,15.30781364440918,5.1500444412231445,9.60189437866211,0.33456891775131226,4.547853469848633,1.830958366394043,6.197912693023682,34.278507232666016,39.39665603637695,18.739107131958008,2.07437801361084,0.7869797348976135,5.319478511810303,5.953125,7.412559986114502,0.02782207727432251,1.2672123908996582,0.6408665776252747,0.9980233311653137,7.2259135246276855,22.33930206298828,1.2087774276733398,25.53784942626953,22.43144989013672,0.34423714876174927,9.45958137512207,0.585974931716919,11.190208435058594,20.315536499023438,13.491665840148926,22.212158203125,12.725752830505371,3.7512454986572266,18.8288631439209,0.8787966966629028,2.0779600143432617,7.818840980529785,0.9558953642845154,6.499608993530273,1.1460318565368652,0.4007936120033264,11.544665336608887,8.317845344543457,0.9004847407341003,16.370424270629883,0.9400848746299744,23.628328323364258,29.70404624938965,20.8734073638916,11.647886276245117,5.537835121154785,6.599327564239502,5.444962978363037,4.873913764953613,4.997293949127197,0.16571348905563354,15.095476150512695,0.6176679730415344,8.601356506347656,19.77909278869629,1.3853251934051514,3.8802597522735596,0.2642161250114441,23.279783248901367,0.8287512063980103,1.390838623046875,0.25418752431869507,4.684053421020508,28.52335548400879,0.9692680835723877,0.8270924091339111,10.469576835632324,15.548898696899414,7.670222282409668,13.128547668457031,16.80521583557129,3.0921080112457275,2.358081817626953,11.779823303222656,32.35179901123047,14.354996681213379,3.1824147701263428,0.1091499924659729,3.7988576889038086,2.3842334747314453,0.31044119596481323,0.11254960298538208,23.471397399902344,0.8928507566452026,11.723187446594238,29.26424217224121,0.47236889600753784,0.12702888250350952,6.971405029296875,3.1310510635375977,3.6244025230407715,4.703843116760254,1.3555335998535156,1.9027025699615479,23.928804397583008,28.963176727294922,5.96751070022583,35.513099670410156,2.2170450687408447,10.2687349319458,0.5974074006080627,10.59613037109375,10.064404487609863,3.080211877822876,8.719584465026855,0.7332735657691956,10.120404243469238,19.855710983276367,16.010194778442383,0.07938462495803833,1.4857702255249023,3.2850515842437744,8.556617736816406,0.5152647495269775,0.4290483593940735,1.4243526458740234,4.925671577453613,17.1357421875,0.48253941535949707,0.04413968324661255,9.615572929382324,18.95117950439453,0.6333795189857483,16.132640838623047,0.35019415616989136,1.2610986232757568,1.0506387948989868,22.008697509765625,8.810617446899414,9.105645179748535,1.747556209564209,6.6630377769470215,0.4230104088783264,0.2456873059272766,35.419891357421875,21.325836181640625,7.190680503845215,6.048339366912842,2.1687257289886475,0.01904386281967163,4.667877674102783,5.115221977233887,0.24072355031967163,0.4642294645309448,11.877652168273926,21.34249496459961,7.5132737159729,10.691352844238281,0.4042545557022095,9.670937538146973,5.0269341468811035,3.894564628601074,20.468807220458984,3.934209108352661,2.939514398574829,36.78165817260742,12.315731048583984,1.0283212661743164,5.99665641784668,31.28108024597168,0.7823440432548523,22.223331451416016,10.429398536682129,6.094010353088379,15.09394359588623,29.80160140991211,6.8928351402282715,3.774705410003662,7.448997497558594,0.10473817586898804,11.483924865722656,0.5468098521232605,3.0031628608703613,28.39203643798828,4.235325336456299,32.44951248168945,15.68161392211914,29.164051055908203,5.239875793457031,21.158836364746094,0.693109929561615,15.652122497558594,27.632858276367188,0.07363420724868774,3.105665683746338,25.523630142211914,10.741805076599121,2.741258144378662,11.226642608642578,33.339599609375,22.91968536376953,0.28115755319595337,0.4520515203475952,12.569950103759766,3.427476406097412,1.353585958480835,14.160054206848145,0.4253750443458557,1.1650407314300537,0.2975248694419861,6.872068881988525,6.113542079925537,5.9699296951293945,1.0223133563995361,32.8921012878418,0.49078285694122314,1.272648811340332,22.49945640563965,9.657148361206055,8.717094421386719,10.301234245300293,0.3295937180519104,5.288165092468262,6.776651859283447,6.855370044708252,0.05494171380996704,7.340968608856201,11.360397338867188,8.675819396972656,35.84345245361328,6.909663200378418,13.138830184936523,25.2430419921875,2.6222453117370605,0.959999680519104,12.721534729003906,14.291699409484863,3.725034713745117,4.71695613861084,3.0179812908172607,11.381933212280273,14.285820960998535,9.2610445022583,0.9228340983390808,11.762150764465332,0.7000676989555359,12.871448516845703,1.0688560009002686],\"type\":\"scatter3d\"},{\"hovertemplate\":\"Sample=TERMINAL<br>Time=%{x}<br>S1=%{y}<br>Result=%{z}<extra></extra>\",\"legendgroup\":\"TERMINAL\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"TERMINAL\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],\"y\":[42.03054428100586,53.75661087036133,54.165672302246094,74.97087097167969,45.57297897338867,48.29309844970703,64.41424560546875,53.09046936035156,59.5042839050293,52.99530029296875,53.31441879272461,62.963104248046875,28.356882095336914,64.90365600585938,53.284793853759766,68.16876220703125,46.8588981628418,38.681758880615234,68.32630920410156,88.78003692626953,28.316234588623047,45.19255065917969,47.572265625,59.720027923583984,34.932125091552734,46.9342041015625,53.89949035644531,45.743839263916016,53.225154876708984,35.47474670410156,72.25959014892578,70.52435302734375,52.525115966796875,70.55327606201172,39.03120040893555,82.09654235839844,48.5653076171875,80.84525299072266,38.586280822753906,64.88967895507812,46.327999114990234,47.911529541015625,34.5878791809082,41.24253463745117,41.92359924316406,52.45125198364258,59.90822982788086,45.325382232666016,65.61934661865234,61.30744171142578,59.415794372558594,36.62562561035156,49.567108154296875,56.1253547668457,51.237648010253906,51.79521560668945,77.10006713867188,56.52708435058594,70.22467803955078,85.22642517089844,51.670677185058594,30.781972885131836,49.66172409057617,35.40565490722656,37.61104965209961,63.98005676269531,58.13264083862305,74.44356536865234,64.04395294189453,70.9271240234375,43.112876892089844,53.28704071044922,48.7371940612793,41.76576232910156,62.65262222290039,46.25123596191406,56.749820709228516,57.465911865234375,43.9921989440918,55.85696029663086,71.00845336914062,56.97758102416992,51.67609405517578,39.91863250732422,60.98352813720703,66.40201568603516,103.04957580566406,53.74691390991211,73.26327514648438,54.230926513671875,59.910179138183594,61.54771423339844,42.44443893432617,71.35546112060547,84.40204620361328,59.2463493347168,58.17533874511719,51.091583251953125,42.6492805480957,40.389503479003906,73.95600891113281,47.5405387878418,57.016361236572266,47.1114501953125,45.335235595703125,34.59170913696289,44.55763244628906,32.046119689941406,54.885780334472656,66.21219635009766,58.359127044677734,64.69144439697266,42.50751495361328,40.85992431640625,52.55000686645508,44.29956817626953,55.22451400756836,51.11764144897461,60.61839294433594,71.65430450439453,41.43325424194336,61.281185150146484,52.701805114746094,36.563419342041016,61.95067596435547,44.17349624633789,46.664066314697266,64.01963806152344,51.87971496582031,50.24851608276367,38.22901916503906,68.02472686767578,51.493282318115234,54.647560119628906,40.31402587890625,45.83696365356445,44.89212417602539,36.89244842529297,58.26008987426758,54.9088249206543,45.96980667114258,60.763389587402344,44.61819076538086,69.32856750488281,64.126220703125,55.371116638183594,46.808441162109375,50.54050827026367,55.73965835571289,50.151710510253906,87.63131713867188,38.44554901123047,46.35514450073242,52.325164794921875,40.85552215576172,55.68578338623047,61.906593322753906,84.45275115966797,36.368873596191406,64.58776092529297,40.11103439331055,69.92572021484375,35.79189682006836,48.765560150146484,45.217124938964844,49.01841735839844,61.56349182128906,56.880367279052734,64.62126159667969,47.82460403442383,68.72069549560547,44.464576721191406,39.31340408325195,75.15000915527344,65.38550567626953,53.987144470214844,50.7781867980957,65.92253112792969,48.1428337097168,39.62038040161133,95.27815246582031,109.5927734375,70.05821228027344,74.9872817993164,55.64739227294922,38.82655715942383,46.79833221435547,34.29782485961914,44.3603630065918,46.56834411621094,63.009273529052734,97.75233459472656,51.707942962646484,58.615230560302734,39.19479751586914,60.635929107666016,58.807044982910156,45.483116149902344,56.866127014160156,53.79315185546875,48.28583908081055,55.69458770751953,48.53901672363281,49.92414855957031,61.01813507080078,44.64059066772461,77.46204376220703,47.031063079833984,65.66026306152344,57.34397888183594,51.860107421875,47.066139221191406,35.53136444091797,100.86396026611328,52.977989196777344,54.25483703613281,59.66088104248047,32.461483001708984,54.995174407958984,77.53321838378906,62.68490982055664,42.52897644042969,48.943809509277344,64.65755462646484,46.534568786621094,36.40339279174805,39.884422302246094,46.820556640625,45.83808135986328,64.16219329833984,71.14425659179688,42.97984313964844,36.26402282714844,56.562744140625,33.43602752685547,60.15988540649414,63.860626220703125,38.61968231201172,52.330047607421875,47.42745590209961,45.889400482177734,76.54644012451172,51.342254638671875,62.722103118896484,37.796356201171875,48.23294448852539,53.470726013183594,37.7442741394043,62.806129455566406,43.845577239990234,36.880760192871094,55.744140625,55.3759880065918,43.11191177368164,82.48002624511719,48.88467025756836,62.779998779296875,55.87607192993164,50.14546585083008,69.20864868164062,50.15025329589844,57.86330795288086,45.916900634765625,62.75506591796875,56.115360260009766,67.9741439819336,55.43070983886719,33.05607986450195,62.15498352050781,58.83476257324219,76.19310760498047,53.99045181274414,37.984519958496094,49.618919372558594,37.98226547241211,71.16937255859375,77.60911560058594,50.54792785644531,60.13404846191406,37.20912551879883,50.31459426879883,73.75959777832031,43.87172317504883,47.94780349731445,58.75676345825195,53.156776428222656,52.039939880371094,52.458919525146484,36.69466018676758,46.6901969909668,61.44267272949219,56.21116256713867,45.35778045654297,57.606834411621094,41.75485610961914,50.23906707763672,39.46037292480469,58.71843719482422,46.11268997192383,43.31032180786133,60.102264404296875,60.04695129394531,51.97452163696289,34.283958435058594,51.352447509765625,38.54269027709961,54.46915054321289,41.82475662231445,58.64527130126953,133.32058715820312,63.37541961669922,64.40909576416016,80.49237060546875,42.29798889160156,45.551918029785156,58.64207077026367,54.15353012084961,40.72332763671875,61.77884292602539,54.33716583251953,67.26642608642578,31.77332305908203,84.1271743774414,49.015350341796875,51.772586822509766,44.20182418823242,43.42491149902344,34.608116149902344,57.170318603515625,60.30308532714844,57.98832702636719,65.25495910644531,24.773502349853516,41.51874542236328,76.55767059326172,61.94920349121094,39.06846237182617,58.281253814697266,103.46251678466797,57.199073791503906,41.173831939697266,64.52070617675781,51.671226501464844,62.61274719238281,51.163333892822266,33.28252410888672,63.459205627441406,42.03043746948242,49.75855255126953,33.87678909301758,40.489471435546875,94.1547622680664,41.909820556640625,80.6759033203125,62.90170669555664,51.50768280029297,36.331600189208984,44.77228546142578,39.823787689208984,63.021934509277344,48.71062469482422,43.87824630737305,39.78566360473633,68.30339050292969,40.113128662109375,54.08673095703125,62.97312927246094,54.357295989990234,59.84648513793945,58.80709457397461,53.36397933959961,40.31679153442383,49.75254440307617,48.4269905090332,49.21276092529297,94.061767578125,49.636878967285156,74.04083251953125,55.252601623535156,75.9473876953125,51.985443115234375,36.61608123779297,47.367431640625,57.14219665527344,39.88589859008789,61.46709442138672,42.13227844238281,31.793365478515625,64.65348052978516,79.34335327148438,61.49569320678711,35.877601623535156,60.88163757324219,62.21703338623047,39.641632080078125,50.415863037109375,70.08184051513672,63.001644134521484,60.85783386230469,44.04283905029297,40.80708694458008,39.40364074707031,70.76811218261719,73.76496124267578,53.73542404174805,47.73683547973633,43.562294006347656,50.04396438598633,50.1534538269043,38.986297607421875,48.02615737915039,63.712852478027344,71.38984680175781,50.21168518066406,47.03620147705078,33.76743698120117,51.941375732421875,51.69998550415039,33.11211013793945,49.97377014160156,58.712650299072266,50.389488220214844,30.08354377746582,48.591800689697266,30.366653442382812,52.240570068359375,43.06589126586914,64.31008911132812,38.744388580322266,55.05829620361328,85.47222900390625,62.88003921508789,48.51046371459961,50.37665939331055,46.20451736450195,59.169620513916016,41.75631332397461,45.45209884643555,54.08396911621094,81.10053253173828,58.50157165527344,64.98835754394531,65.52408599853516,73.4247055053711,48.35314178466797,42.47159194946289,48.7474250793457,42.66303253173828,84.33734130859375,26.68878746032715,60.388938903808594,53.06879425048828,57.185791015625,58.84760665893555,73.55974578857422,54.06478500366211,60.297542572021484,46.41252899169922,38.871646881103516,34.42325973510742,63.81558609008789,50.34159469604492,51.232791900634766,54.56651306152344,59.007293701171875,44.2588005065918,45.16937255859375,79.64165496826172,62.559967041015625,44.645450592041016,54.694576263427734,60.382198333740234,70.97692108154297,59.47876739501953,40.336029052734375,62.07029342651367,36.351505279541016,69.9652328491211,48.301063537597656,59.41832733154297,50.849308013916016,27.802595138549805,42.00456237792969,82.31038665771484,45.69818115234375,46.2523078918457,65.79817199707031,59.61619567871094,62.12740707397461,48.187232971191406,43.83735656738281,33.5020751953125,53.22774124145508,49.886505126953125,74.42208862304688,55.40821075439453,45.21449661254883,51.196956634521484,59.72356033325195,50.370513916015625,75.16292572021484,36.016380310058594,80.76370239257812,52.465240478515625,51.54070281982422,51.97123718261719,43.755096435546875,50.710899353027344,42.11101150512695,50.24368667602539,43.240989685058594,39.93986129760742],\"z\":[-0.1507740616798401,3.62919545173645,4.003272533416748,25.03387451171875,-0.24761444330215454,0.14801651239395142,14.315910339355469,3.041857957839966,9.316740036010742,2.960451364517212,3.236002206802368,12.841670036315918,0.09019559621810913,14.812811851501465,3.210115671157837,18.128101348876953,-0.15021538734436035,-0.007731616497039795,18.28814125061035,38.85780334472656,0.09011894464492798,-0.2550423741340637,-0.031542420387268066,9.537005424499512,0.06918495893478394,-0.1401817798614502,3.7588043212890625,-0.24160319566726685,3.158191442489624,0.062489211559295654,22.283891677856445,20.521364212036133,2.56868577003479,20.550745010375977,-0.01929527521133423,32.2200813293457,0.23307263851165771,30.96316909790039,-0.004722893238067627,14.798623085021973,-0.20615899562835693,0.0449678897857666,0.07284337282180786,-0.11178010702133179,-0.1454119086265564,2.5088133811950684,9.729144096374512,-0.2533208727836609,15.539359092712402,11.156510353088379,9.226396560668945,0.04393976926803589,0.6328704357147217,5.893408298492432,1.6005878448486328,1.9991586208343506,27.188125610351562,6.294222831726074,20.21688461303711,35.35011291503906,1.9072201251983643,0.09141474962234497,0.6777929663658142,0.06340938806533813,0.022481262683868408,13.874985694885254,7.917751312255859,24.49956703186035,13.939881324768066,20.93056297302246,-0.20356649160385132,3.212078094482422,0.2918815612792969,-0.13752001523971558,12.525962829589844,-0.21225982904434204,6.51767110824585,7.24050760269165,-0.23851662874221802,5.627480983734131,21.01318359375,6.7469096183776855,1.9111824035644531,-0.05254858732223511,10.82631778717041,16.333925247192383,48.22897720336914,3.620441436767578,23.302627563476562,4.063772678375244,9.731132507324219,11.401330947875977,-0.17146939039230347,21.36568832397461,34.528289794921875,9.053414344787598,7.961213111877441,1.501945972442627,-0.1815672516822815,-0.07240575551986694,24.005287170410156,-0.03800302743911743,6.786013126373291,-0.11434072256088257,-0.2531566023826599,0.07280558347702026,-0.25244206190109253,0.08879536390304565,4.6818060874938965,16.141212463378906,8.148393630981445,14.597362518310547,-0.17459505796432495,-0.09363669157028198,2.5889692306518555,-0.24719291925430298,5.008248805999756,1.519362211227417,10.453914642333984,21.669221878051758,-0.12106114625930786,11.12975025177002,2.713798761367798,0.04511076211929321,11.81170654296875,-0.24392598867416382,-0.1736661195755005,13.91518497467041,2.0624494552612305,0.9836724996566772,0.005980789661407471,17.98179054260254,1.779104232788086,4.454822540283203,-0.0691261887550354,-0.23755484819412231,-0.2558653950691223,0.03868132829666138,8.04751205444336,4.703880786895752,-0.23077934980392456,10.601820945739746,-0.25336283445358276,19.3063907623291,14.023428916931152,5.150744915008545,-0.15663260221481323,1.1529884338378906,5.511791706085205,0.9300153851509094,37.73091125488281,-0.0004016757011413574,-0.20388662815093994,2.4077227115631104,-0.0934324860572815,5.458781719207764,11.766826629638672,34.57890319824219,0.048640549182891846,14.492090225219727,-0.060484468936920166,19.913127899169922,0.058002769947052,0.3019738793373108,-0.2547888159751892,0.39678120613098145,11.417407035827637,6.648977756500244,14.526105880737305,0.024043619632720947,18.688793182373047,-0.25078731775283813,-0.02926236391067505,25.2153263092041,15.30197525024414,3.8388824462890625,1.298886775970459,15.847148895263672,0.10522204637527466,-0.040751636028289795,45.183841705322266,48.28214645385742,20.047748565673828,25.050498962402344,5.421046733856201,-0.012419044971466064,-0.15788841247558594,0.07560139894485474,-0.24861103296279907,-0.1839141845703125,12.888603210449219,47.32037353515625,1.9345588684082031,8.409439086914062,-0.025004088878631592,10.4718017578125,8.605080604553223,-0.25008028745651245,6.634644031524658,3.662228584289551,0.1458815336227417,5.467437267303467,0.22442626953125,0.8088184595108032,10.861603736877441,-0.25367146730422974,27.553804397583008,-0.12645572423934937,15.580899238586426,7.117027759552002,2.047698974609375,-0.12124907970428467,0.06172102689743042,48.13564682006836,2.9457168579101562,4.085991382598877,9.476617813110352,0.08728784322738647,4.786777973175049,27.625682830810547,12.558801651000977,-0.17565542459487915,0.36789292097091675,14.562954902648926,-0.18733900785446167,0.04802757501602173,-0.05116528272628784,-0.15511411428451538,-0.23750323057174683,14.05996322631836,21.151142120361328,-0.1974155306816101,0.050464093685150146,6.329946041107178,0.08221811056137085,9.986030578613281,13.753680229187012,-0.0057683587074279785,2.411609649658203,-0.060093462467193604,-0.23502308130264282,26.62851333618164,1.6727454662322998,12.596626281738281,0.017793595790863037,0.13053494691848755,3.3735527992248535,0.019132673740386963,12.682068824768066,-0.23359936475753784,0.03891879320144653,5.516208171844482,5.155489921569824,-0.20352202653884888,32.60478591918945,0.34554141759872437,12.655497550964355,5.6463623046875,0.9265968203544617,19.18454933166504,0.9292153716087341,7.643810272216797,-0.2336224913597107,12.630146980285645,5.883476257324219,17.9304141998291,5.208863258361816,0.08447283506393433,12.019674301147461,8.6333589553833,26.271156311035156,3.8419127464294434,0.012807190418243408,0.6573169827461243,0.012867987155914307,21.176654815673828,27.702333450317383,1.1574344635009766,9.9596586227417,0.03191953897476196,1.0210143327713013,23.806110382080078,-0.234510600566864,0.053977131843566895,8.553786277770996,3.098966121673584,2.1844379901885986,2.5150039196014404,0.042616069316864014,-0.17072659730911255,11.294313430786133,5.978757381439209,-0.25276392698287964,7.383385181427002,-0.1369759440422058,0.9783826470375061,-0.034680068492889404,8.51469612121582,-0.22211027145385742,-0.21237808465957642,9.927214622497559,9.870756149291992,2.1343259811401367,0.07572656869888306,1.6798443794250488,-0.003369271755218506,4.28639554977417,-0.14046543836593628,8.44007682800293,48.29568099975586,13.260740280151367,14.31068229675293,30.608291625976562,-0.16417354345321655,-0.24823445081710815,8.436810493469238,3.992041826248169,-0.08734041452407837,11.63674545288086,4.162721157073975,17.21163558959961,0.08959728479385376,34.253780364990234,0.3955777883529663,1.9823341369628906,-0.24469536542892456,-0.21728307008743286,0.07264131307601929,6.941415309906006,10.132183074951172,7.770915985107422,15.169448852539062,0.07785767316818237,-0.12526553869247437,26.63986587524414,11.810209274291992,-0.020578205585479736,8.069064140319824,48.23748016357422,6.970472812652588,-0.10847288370132446,14.424007415771484,1.9076199531555176,12.4854097366333,1.5500917434692383,0.08317488431930542,13.345877647399902,-0.15076857805252075,0.7250409126281738,0.07911211252212524,-0.0768088698387146,44.08381652832031,-0.14472049474716187,30.792882919311523,12.77924633026123,1.789377212524414,0.049295246601104736,-0.2551228404045105,-0.04872864484786987,12.901474952697754,0.28253084421157837,-0.23473471403121948,-0.04721313714981079,18.264860153198242,-0.06057173013687134,3.9303767681121826,12.851861000061035,4.181529521942139,9.666107177734375,8.605134010314941,3.279435157775879,-0.06924492120742798,0.7220706939697266,0.18862861394882202,0.47563648223876953,43.992897033691406,0.6658747792243958,24.09129524230957,5.03549861907959,26.022550582885742,2.1426618099212646,0.044120728969573975,-0.07123255729675293,6.913009166717529,-0.051225483417510986,11.319195747375488,-0.15587478876113892,0.08954447507858276,14.558815002441406,29.45157241821289,11.348332405090332,0.05671161413192749,10.722418785095215,12.082822799682617,-0.041572749614715576,1.0793545246124268,20.071760177612305,12.880847930908203,10.698141098022461,-0.24010688066482544,-0.09118920564651489,-0.03257089853286743,20.76901626586914,23.81154441833496,3.610079765319824,0.0038508176803588867,-0.2229291796684265,0.8717605471611023,0.9309692978858948,-0.017759740352630615,0.07398635149002075,13.60357666015625,21.400609970092773,0.963112473487854,-0.12569940090179443,0.07993549108505249,2.109095335006714,1.9287078380584717,0.0841633677482605,0.8346481919288635,8.508794784545898,1.0640301704406738,0.09179717302322388,0.24187719821929932,0.09172254800796509,2.3406999111175537,-0.2014121413230896,14.210147857666016,-0.009741604328155518,4.847549915313721,35.594669342041016,12.757218360900879,0.2151450514793396,1.0566105842590332,-0.2157449722290039,8.975090980529785,-0.13704746961593628,-0.25082534551620483,3.9278292655944824,31.219789505004883,8.293560981750488,14.898801803588867,15.442660331726074,23.46640396118164,0.1659371256828308,-0.1728164553642273,0.2955096960067749,-0.1822386384010315,34.46369171142578,0.08585458993911743,10.21979808807373,3.023259162902832,6.9570488929748535,8.646462440490723,23.603397369384766,3.9101669788360596,10.126524925231934,-0.19888842105865479,-0.013907134532928467,0.07444339990615845,13.707930564880371,1.0364384651184082,1.597271203994751,4.37813663482666,8.80941104888916,-0.24618369340896606,-0.2552521824836731,29.75204086303711,12.43172550201416,-0.2537382245063782,4.499439716339111,10.212918281555176,20.981151580810547,9.290691375732422,-0.07008105516433716,11.933478355407715,0.04894524812698364,19.953279495239258,0.1503666639328003,9.228981018066406,1.3439204692840576,0.08903151750564575,-0.1494695544242859,32.43464279174805,-0.24338549375534058,-0.21217548847198486,15.72089958190918,9.430997848510742,11.991610527038574,0.11756265163421631,-0.2333109974861145,0.08178836107254028,3.160438060760498,0.7894467711448669,24.477800369262695,5.186905384063721,-0.25481992959976196,1.5728609561920166,9.540613174438477,1.0530641078948975,25.22840690612793,0.05455094575881958,30.88117218017578,2.5201165676116943,1.8130183219909668,2.1318185329437256,-0.23034971952438354,1.2568602561950684,-0.15480929613113403,0.9809679388999939,-0.20933407545089722,-0.05341225862503052],\"type\":\"scatter3d\"},{\"hovertemplate\":\"Sample=BELOW<br>Time=%{x}<br>S1=%{y}<br>Result=%{z}<extra></extra>\",\"legendgroup\":\"BELOW\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"BELOW\",\"scene\":\"scene\",\"showlegend\":true,\"x\":[0.9554253816604614,0.992508053779602,0.22984784841537476,0.9456807374954224,0.9800506234169006,0.9861540794372559,0.4849701523780823,0.9802257418632507,0.8180699944496155,0.9846072793006897,0.9971790313720703,0.9995572566986084,0.9378724694252014,0.45451587438583374,0.9516491889953613,0.9863740801811218,0.6269581913948059,0.945917546749115,0.9135097861289978,0.9450854063034058,0.9919009804725647,0.9476497173309326,0.9714334607124329],\"y\":[43.75529861450195,53.17341232299805,106.24311828613281,42.24937438964844,44.139774322509766,55.39015579223633,103.54342651367188,41.313087463378906,100.87120819091797,41.302825927734375,53.41758728027344,53.563880920410156,46.17255783081055,97.39656066894531,40.71611404418945,45.873191833496094,94.46240234375,44.002777099609375,42.35987854003906,42.506874084472656,53.95467758178711,45.70866394042969,59.13014221191406],\"z\":[-0.1498989462852478,3.1633083820343018,48.25578689575195,-0.08644217252731323,-0.20490163564682007,5.257205486297607,48.224708557128906,-0.09184616804122925,48.1285400390625,-0.0966067910194397,3.345411777496338,3.459242105484009,-0.024561941623687744,46.80840301513672,-0.03256303071975708,-0.19773989915847778,44.354766845703125,-0.1352025866508484,-0.04168826341629028,-0.09514659643173218,3.86273193359375,-0.09717363119125366,9.087257385253906],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"Time\"}},\"yaxis\":{\"title\":{\"text\":\"S1\"}},\"zaxis\":{\"title\":{\"text\":\"Result\"}}},\"legend\":{\"title\":{\"text\":\"Sample\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"height\":400,\"width\":500},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('e9964b1f-3576-4010-9a86-f5c56d294228');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{},"output_type":"display_data"}],"source":["eqLossFn= 'calculateLossUsingKLMinMax'\n","sample_method= \"LN\"\n","lr = 0.0001\n","net10 = AmericanOptionNet( NL = 5 , NN = 100 )\n","net10.to(torch.device(\"cuda:0\")) \n","## providing sampler with net so it can accept/reject based on net and other criterions\n","bsequation_net10 = AmericanBlackScholesSingleStock(net10)\n","bsequation_net10.beta = bsequation_net10.beta/10.0\n","bsequation_net10.gamma = bsequation_net10.gamma*10.0\n","trainAmss_net10 = TrainAmericanBlackScholesSingleStock( net10 , bsequation_net10 , BATCH_SIZE = 2**9 , debug = True )\n","trainAmss_net10.hook_interval = 1000\n","trainAmss_net10.use_early_stop = True\n","trainAmss_net10.early_stop_patience = 1000\n","trainAmss_net10.train( epoch = 10000 , lr = lr, eqLossFn= eqLossFn , sample_method_X= sample_method)\n","df_net10 = trainAmss_net10.history_surfaces_hooks[trainAmss_net10.history_surfaces_hooks.Epoch == max(trainAmss_net10.history_surfaces_hooks.Epoch)]\n","fig_net10 = px.scatter_3d(df_net10, x='Time', y='S1', z='Result',color='Sample', width=500, height=400)\n","fig_net10.show()\n","torch.save(net10.state_dict(), f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/AmCallSs_{datetime.datetime.now():%Y%m%d%H%M%S}_{eqLossFn}_{sample_method}_{trainAmss_net10.stop_epoch}_{str(lr).replace('.','p')}_{net10.NL}_{net10.NN}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amgSOhm2fgtN"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"ZTnJdh3HvtOT"},"source":["### Adapted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQ2jTD4hB5d3"},"outputs":[],"source":["\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["BErSeefeQwQi","bvy0WvxDGCxk","HrivvbmubiiY","wClW1g9rbm8o","65nooklCbsdy","lAgwEsfACPRQ","fPi9t3UzCW5Y","8IZo1GkJC8Ik","LzFe9rKsCeNT","pO55QGFSCkhN","D7d5U4kTYsbp","N8FXnhHAfgs6","1fqY3wltfgs8","riU7CRY1fgs_","Hsr6F15afgtE","G4xcnKTkfgtG","iEwlH_u2fgtK","yZacjndLfgtK","xex3RmRrfgtL","UWku2GG_fgtM","ZTnJdh3HvtOT"],"name":"Extract_data_dgm.ipynb","provenance":[{"file_id":"1wcwWr2z1kklVPaP-c1kaJPNarQBqkhBl","timestamp":1660064131894}],"mount_file_id":"1wcwWr2z1kklVPaP-c1kaJPNarQBqkhBl","authorship_tag":"ABX9TyOYa3Ux7+1Wftqx6UYnzhRS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}