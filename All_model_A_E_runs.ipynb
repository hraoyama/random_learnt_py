{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"All_model_A_E_runs.ipynb","provenance":[{"file_id":"1tAxx62ER2jywEt4SmHB5wvPfX02TtDv1","timestamp":1630343167671}],"collapsed_sections":[],"mount_file_id":"11SRV3eTsBrk7F-jwWOVB_wSl5LjAv2kp","authorship_tag":"ABX9TyOzoHflV08fBC88oug0AfU8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9wnfYeodC3s","executionInfo":{"status":"ok","timestamp":1636841341777,"user_tz":0,"elapsed":10638,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}},"outputId":"c790bfa5-412a-4316-dfe9-5b64e75d5ec9"},"source":["%pip install keras-tuner\n","%pip install keras_self_attention"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.1.0-py3-none-any.whl (98 kB)\n","\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.1 MB/s \n","\u001b[?25hCollecting kt-legacy\n","  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.7.0)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.41.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.1.0 kt-legacy-1.0.4\n","Collecting keras_self_attention\n","  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.7.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=474cd44aef3dbc817cd2e6b99047c16f863b2d3d88dac3742f36634734a36e2e\n","  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.50.0\n"]}]},{"cell_type":"code","metadata":{"id":"OQjO1NMIkHiZ","executionInfo":{"status":"ok","timestamp":1636841357805,"user_tz":0,"elapsed":3283,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"source":["import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n","from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.summary import create_file_writer\n","\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vToSXGfMkc6w","executionInfo":{"status":"ok","timestamp":1636841357807,"user_tz":0,"elapsed":9,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"source":["def coShuffled_vectors(X, Y):\n","    if tf.shape(X)[0] == tf.shape(Y)[0]:\n","        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n","        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n","        return (tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs))\n","    else:\n","        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n","\n","\n","def getNpArrayFromH5(hf_Data):\n","    X_train = hf_Data['Train_Data']  # Get train set\n","    X_train = np.array(X_train)\n","    Y_train = hf_Data['Label']  # Get train label\n","    Y_train = np.array(Y_train)\n","    return X_train, Y_train\n","\n","# data extraction\n","def getData(is500=True, shuffle=False, ise2e=False, include_secondary=False, validation_split=None, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Train_Data\" if ise2e else \"Fold_10_Train_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Test_Data\" if ise2e else \"Fold_10_Test_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    X_validation = Y_validation = None\n","    if validation_split is not None:\n","        # sklearn split shuffles anyway\n","        X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_split)\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eData(is500=True, shuffle=False, include_secondary=False, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Secondary_Data_1136.h5', 'r') if include_secondary else h5.File(\n","        f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eDataJustSecondary(shuffle=False,isColab=False):\n","    hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_just_Secondary_Data_1000.h5', 'r')\n","    hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_just_Secondary_Data_1000.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_just_Secondary_Data_1000.h5', 'r')\n","    \n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","def getE2eData88(shuffle=False):\n","  hf_Train = h5.File(f'./drive/MyDrive/data_papers/ncRNA/e2e_Train_Data_1000_88.h5', 'r')\n","  hf_Test = h5.File(f'./drive/MyDrive/data_papers/ncRNA/e2e_Test_Data_1000_88.h5', 'r')\n","\n","  X_train, Y_train = getNpArrayFromH5(hf_Train)\n","  X_test, Y_test = getNpArrayFromH5(hf_Test)\n","  Y_train = to_categorical(Y_train, 88)  # Process the label of tain\n","  Y_test = to_categorical(Y_test, 88)  # Process the label of te\n","\n","  if shuffle:\n","    X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","    X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","  hf_Val = h5.File(f'./drive/MyDrive/data_papers/ncRNA/e2e_Val_Data_1000_88.h5', 'r')\n","  X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","  Y_validation = to_categorical(Y_validation, 88)  # Process the label of tain\n","\n","  return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA4FUPc8ke1I","executionInfo":{"status":"ok","timestamp":1636841358959,"user_tz":0,"elapsed":1160,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"source":["def plot_history(history):\n","    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n","    for k, v in history.history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        else:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()\n","\n","def plot_history_df(history):\n","    acc_keys = [k for k in history.columns.values if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.columns.values if not k in acc_keys and not k in ['epoch']]\n","    for k, v in history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        if k in loss_keys:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()   \n","\n","def get_layer_by_name(layers, name, return_first=True):\n","    matching_named_layers = [l for l in layers if l.name == name]\n","    if not matching_named_layers:\n","        return None\n","    return matching_named_layers[0] if return_first else matching_named_layers\n","\n","\n","def get_combined_features_from_models(\n","        to_combine,\n","        X_train_combined, Y_train_combined,\n","        X_validation_combined, Y_validation_combined,\n","        X_test_combined, Y_test_combined,\n","        reverse_one_hot=False,\n","        normalize_X_func=None):\n","    models = []\n","    models_dict = {}\n","    X_trains_out = []\n","    X_test_out = []\n","    X_validation_out = []\n","    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n","\n","    if reverse_one_hot:\n","        Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train_combined) + 1\n","        Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test_combined) + 1\n","        Y_validation_new = np.apply_along_axis(np.argmax, 1, Y_validation_combined) + 1\n","    else:\n","        Y_train_new = Y_train_combined.copy()\n","        Y_test_new = Y_test_combined.copy()\n","        Y_validation_new = np.apply_along_axis(np.argmax, 1, Y_validation_combined) + 1\n","\n","    for model_file_name, layer_name, kwargs in to_combine:\n","        model_here = None\n","        if isinstance(model_file_name, tf.keras.models.Model):\n","            model_here = model_file_name\n","            model_file_name = model_here.name\n","        else:\n","            if model_file_name in models_dict.keys():\n","                model_here = models_dict[model_file_name]\n","            else:\n","                model_here = tf.keras.models.load_model(model_file_name,\n","                                                        **kwargs) if kwargs is not None else tf.keras.models.load_model \\\n","                    (model_file_name)\n","\n","        features_model = Model(model_here.input,\n","                               get_layer_by_name(model_here.layers, layer_name).output)\n","        if normalize_X_func is None:\n","            X_trains_out.append(np.array(features_model.predict(X_train_combined), dtype='float64'))\n","            X_test_out.append(np.array(features_model.predict(X_test_combined), dtype='float64'))\n","            X_validation_out.append(np.array(features_model.predict(X_validation_combined), dtype='float64'))\n","        else:\n","            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train_combined)), dtype='float64'))\n","            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test_combined)), dtype='float64'))\n","            X_validation_out.append(np.array(normalize_X_func(features_model.predict(X_validation_combined)), dtype='float64'))\n","            \n","        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n","        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n","        \n","        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n","        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n","        \n","        XY_dict[model_file_name][layer_name]['Validation']['X'] = X_validation_out[-1]\n","        XY_dict[model_file_name][layer_name]['Validation']['Y'] = Y_validation_new\n","        \n","        \n","        models.append(((model_file_name, layer_name), (model_here, features_model)))\n","        models_dict[model_file_name] = model_here\n","\n","    X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n","    X_test_new = np.concatenate(tuple(X_test_out), axis=1)\n","    X_validation_new = np.concatenate(tuple(X_validation_out), axis=1)\n","\n","\n","    data_train = (X_train_new, Y_train_new)\n","    data_test = (X_test_new, Y_test_new)\n","    data_validation = (X_validation_new, Y_validation_new)\n","\n","    return models, data_train, data_validation, data_test, XY_dict\n","\n","\n","\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","\n","def source_model(model_func, model_name, input_shape):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    return m\n","\n","\n","def compile_and_fit_model_with_tb(model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_every_epoch=True,\n","                                  save_final=False,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    tb_callback = TensorBoard(log_dir=f'{m.name}_logs', histogram_freq=kwargs.pop(\"histogram_freq\", 1))\n","    if save_every_epoch:\n","        tb_callback.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=[tb_callback], verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","    # m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n","\n","\n","def compile_model_and_fit_with_custom_loop(model_func,\n","                                           model_name,\n","                                           input_shape,\n","                                           X_train,\n","                                           Y_train,\n","                                           **kwargs):\n","    make_dir_if_not_exist(model_name)\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    train_writer = create_file_writer(f'{m.name}_logs/train/')\n","    test_writer = create_file_writer(f'{m.name}_logs/test/')\n","    train_step = test_step = 0\n","\n","    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","    optimizer = tf.keras.optimizers.Adam()\n","    num_epochs = kwargs.get(\"epochs\", 10)\n","\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    BATCH_SIZE = kwargs.get(\"batch_size\", 32)\n","    X_test, Y_test = kwargs.get(\"validation_data\", (None, None))\n","    if X_test is None:\n","        raise ValueError(\"Missing X validation data\")\n","    if Y_test is None:\n","        raise ValueError(\"Missing Y validation data\")\n","\n","    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n","    train_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    train_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n","    test_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    test_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","\n","    for epoch in range(num_epochs):\n","        # Iterate through training set\n","        for batch_idx, (x, y) in enumerate(train_dataset_tf):\n","            with tf.GradientTape() as tape:\n","                y_pred = m(x, training=True)\n","                loss = loss_fn(y, y_pred)\n","\n","            gradients = tape.gradient(loss, m.trainable_weights)\n","            optimizer.apply_gradients(zip(gradients, m.trainable_weights))\n","            acc_metric.update_state(y, y_pred)\n","\n","            with train_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=train_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=train_step,\n","                )\n","                train_step += 1\n","        # Reset accuracy in between epochs (and for testing and test)\n","        acc_metric.reset_states()\n","        # Iterate through test set\n","        for batch_idx, (x, y) in enumerate(test_dataset_tf):\n","            y_pred = m(x, training=False)\n","            loss = loss_fn(y, y_pred)\n","            acc_metric.update_state(y, y_pred)\n","            with test_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=test_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=test_step,\n","                )\n","                test_step += 1\n","\n","        acc_metric.reset_states()  # Reset accuracy in between epochs (and for testing and test)\n","\n","    return m\n","\n","\n","def reinitialize_weights(model):\n","    for ix, layer in enumerate(model.layers):\n","        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n","            weight_initializer = model.layers[ix].kernel_initializer\n","            bias_initializer = model.layers[ix].bias_initializer\n","\n","            old_weights, old_biases = model.layers[ix].get_weights()\n","\n","            model.layers[ix].set_weights([\n","                weight_initializer(shape=old_weights.shape),\n","                bias_initializer(shape=len(old_biases))])\n","    return model\n","\n","\n","def reverse_tensor(X):\n","    return tf.gather(X, tf.reverse(tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32), (0,)))\n","\n","\n","def run_mirrored_strategy(model_func, base_batch_size, nepochs, x_train, y_train, x_test, y_test, **kwargs):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        model = model_func()\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adam(),\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","            metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n","        )\n","    batch_size_mirr_strat = base_batch_size * strategy.num_replicas_in_sync\n","    history = model.fit(x_train, y_train, epochs=nepochs, batch_size=batch_size_mirr_strat,\n","                        validation_data=(x_test, y_test),\n","                        **kwargs)\n","    return model, history\n","\n","\n","def sparse_setdiff(a1, a2):\n","    a1a = a1.reshape(a1.shape[0], -1)\n","    a2a = a2.reshape(a2.shape[0], -1)\n","    spa2a = [np.where(x)[0].tolist() for x in a2a]\n","    spa1a = [np.where(x)[0].tolist() for x in a1a]\n","    idxs_to_keep = []\n","    for idx, sample in enumerate(spa1a):\n","        try:\n","            spa2a.index(sample)\n","        except ValueError:\n","            # not in list\n","            idxs_to_keep.append(idx)\n","    return a1[idxs_to_keep], idxs_to_keep\n","\n","\n","def unpacking_apply_along_axis(all_args):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but with arguments in a tuple\n","    instead.\n","\n","    This function is useful with multiprocessing.Pool().map(): (1)\n","    map() only handles functions that take a single argument, and (2)\n","    this function can generally be imported from a module, as required\n","    by map().\n","    \"\"\"\n","    (func1d, axis, arr, args, kwargs) = all_args\n","    # return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n","\n","\n","def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but takes advantage of multiple\n","    cores.\n","    \"\"\"\n","    # Effective axis where apply_along_axis() will be applied by each\n","    # worker (any non-zero axis number would work, so as to allow the use\n","    # of `np.array_split()`, which is only done on axis 0):\n","    effective_axis = 1 if axis == 0 else axis\n","    if effective_axis != axis:\n","        arr = arr.swapaxes(axis, effective_axis)\n","\n","    # Chunks for the mapping (only a few chunks):\n","    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n","              for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n","\n","    pool = multiprocessing.Pool()\n","    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n","    # Freeing the workers:\n","    pool.close()\n","    pool.join()\n","\n","    return np.concatenate(individual_results)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsPksEpvkwx3","executionInfo":{"status":"ok","timestamp":1636841358959,"user_tz":0,"elapsed":4,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"source":["def model_A_CNN_256(model_name, inshape, num_classes = 88):\n","\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same' ,input_shape=inshape))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    # check these 2\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Flatten())\n","\n","    model.add(tf.keras.layers.Dense(128))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(64))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n","    model._name = model_name\n","\n","    return model\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"BA-Gd8l6kxWn","executionInfo":{"status":"ok","timestamp":1636841358960,"user_tz":0,"elapsed":5,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"source":["def model_E_RNN(model_name, input_shape = (1000, 8,),num_classes = 88):\n","\n","    # RNN part\n","    inputs = Input(shape=input_shape)\n","    lstm_one = Bidirectional \\\n","        (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(inputs)\n","    lstm_two = Bidirectional \\\n","        (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(lstm_one)\n","    attention = SeqWeightedAttention()(lstm_two)\n","    attention = Flatten()(attention)\n","    rnnoutput = Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(attention)\n","    rnnoutput = BatchNormalization()(rnnoutput)\n","    rnnoutput = GaussianNoise(1)(rnnoutput)\n","    rnnoutput = Dropout(0.4)(rnnoutput)\n","\n","    # Dense Feed-forward\n","    dense_one = Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros')(rnnoutput)\n","    dense_one = LeakyReLU()(dense_one)\n","    dense_one = Dropout(0.5)(dense_one)\n","    dense_one = BatchNormalization()(dense_one)\n","    dense_two = Dense(64, kernel_initializer='RandomNormal', bias_initializer='zeros')(dense_one)\n","    dense_two = LeakyReLU()(dense_two)\n","    dense_two = Dropout(0.4)(dense_two)\n","\n","    # Output\n","    output = Dense(num_classes, activation='softmax')(dense_two)\n","    model = Model([inputs], output, name = model_name)\n","    return model\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YulEt03kzS_","executionInfo":{"status":"ok","timestamp":1636841358960,"user_tz":0,"elapsed":4,"user":{"displayName":"Hans Roggeman","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10574434403170915342"}}},"source":["def model_combination(model_name, input_shape,num_classes =88):\n","    model = Sequential([\n","        tf.keras.Input(shape=input_shape),\n","        BatchNormalization(),\n","        Dense(256, kernel_initializer='RandomNormal', bias_initializer='zeros'),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-3)),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(32, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-2)),\n","        LeakyReLU(),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='softmax')\n","    ], name=model_name)\n","    return model"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IdtyDzDlB8u"},"source":["def merged_model_A_E(model_name, input_shape, num_classes = 88):\n","  base_input = Input(shape=input_shape, name='base_input')\n","\n","  cnn_convblock_1 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(base_input))))\n","  cnn_noiseblock_1 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_1))\n","\n","  cnn_convblock_2 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_1))))\n","  cnn_noiseblock_2 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_2))\n","\n","  cnn_convblock_3 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_2))))\n","  cnn_noiseblock_3 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_3))\n","\n","  cnn_convblock_4 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_3))))\n","  cnn_noiseblock_4 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_4))\n","\n","  cnn_convblock_5 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_4))))\n","  cnn_noiseblock_5 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_5))\n","\n","  cnn_flatten = Flatten()(cnn_noiseblock_5)\n","\n","  cnn_denseblock_1  = LeakyReLU(alpha=0.5)(BatchNormalization()(Dense(128)(cnn_flatten)))\n","  cnn_denseblock_2  = LeakyReLU(alpha=0.5)(BatchNormalization()(Dense(64)(cnn_denseblock_1)))\n","\n","  rnn_lstm_one = Bidirectional \\\n","      (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(base_input)\n","  rnn_lstm_two = Bidirectional \\\n","      (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(rnn_lstm_one)\n","  \n","  rnn_attention = Flatten()(SeqWeightedAttention()(rnn_lstm_two))\n","\n","  rnn_denseblock_1 = BatchNormalization()(Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_attention))\n","  rnn_noiseblock_1 = Dropout(0.4)(GaussianNoise(1)(rnn_denseblock_1))\n","\n","  rnn_denseblock_2 = LeakyReLU()(Dense(128 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_1))\n","  rnn_noiseblock_2 = BatchNormalization()(Dropout(0.5)(rnn_denseblock_2))\n","\n","  rnn_denseblock_3 = LeakyReLU()(Dense(64 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_2))\n","  rnn_noiseblock_3 = Dropout(0.4)(rnn_denseblock_3)\n","\n","  merged_layer = tf.keras.layers.concatenate([cnn_denseblock_2,rnn_noiseblock_3])\n","\n","  # Output\n","  output = Dense(num_classes, activation='softmax')(merged_layer)\n","  model = Model(base_input, output, name = model_name)\n","\n","  return model\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FizqwASsouJD"},"source":["def sequential_model_A_E(model_name, input_shape, num_classes = 13):\n","  \n","  base_input = Input(shape=input_shape, name='base_input')\n","\n","  cnn_convblock_1 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(base_input))))\n","  cnn_noiseblock_1 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_1))\n","\n","  cnn_convblock_2 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_1))))\n","  cnn_noiseblock_2 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_2))\n","\n","  cnn_convblock_3 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_2))))\n","  cnn_noiseblock_3 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_3))\n","\n","  cnn_convblock_4 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_3))))\n","  cnn_noiseblock_4 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_4))\n","\n","  cnn_convblock_5 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_4))))\n","  cnn_noiseblock_5 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_5))\n","\n","  rnn_lstm_one = Bidirectional \\\n","      (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(cnn_noiseblock_5)\n","  rnn_lstm_two = Bidirectional \\\n","      (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(rnn_lstm_one)\n","  \n","  rnn_attention = Flatten()(SeqWeightedAttention()(rnn_lstm_two))\n","\n","  rnn_denseblock_1 = BatchNormalization()(Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_attention))\n","  rnn_noiseblock_1 = Dropout(0.4)(GaussianNoise(1)(rnn_denseblock_1))\n","\n","  rnn_denseblock_2 = LeakyReLU()(Dense(128 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_1))\n","  rnn_noiseblock_2 = BatchNormalization()(Dropout(0.5)(rnn_denseblock_2))\n","\n","  rnn_denseblock_3 = LeakyReLU()(Dense(64 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_2))\n","  rnn_noiseblock_3 = Dropout(0.4)(rnn_denseblock_3)\n","\n","  # merged_layer = tf.keras.layers.concatenate([cnn_denseblock_2,rnn_noiseblock_3])\n","\n","  # Output\n","  output = Dense(num_classes, activation='softmax')(rnn_noiseblock_3)\n","  model = Model(base_input, output, name = model_name)\n","\n","  return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRSlxXgfk3Qu"},"source":["from tensorflow.keras.callbacks import CSVLogger\n","import datetime\n","\n","def compile_and_fit_model_basic(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  early_stopping_obs = 'accuracy',\n","                                  log_history = True,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor = early_stopping_obs, # 'accuracy', # 'val_accuracy'\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=patience_count))\n","\n","    if log_history:\n","        callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{model_name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n","\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o68Af33BDKyg"},"source":["def compile_and_fit_model_collaborative(  \n","        m1, m1_layer_name,\n","        m2, m2_layer_name,\n","        input_shape,\n","        X_train,\n","        Y_train,\n","        X_validation,\n","        Y_validation,\n","        X_test,\n","        Y_test,\n","        save_max_epoch=True,\n","        save_final=False,\n","        patience_count = None,\n","        log_history = True,\n","        merged_name = None,\n","        **kwargs):\n","    \n","    to_combine_ld_no2nd_rNo2nd = [\n","        (m1 , m1_layer_name, None),   # change to the appropriate dense layer name\n","        (m2, m2_layer_name, None) # change to the appropriate dense layer name\n","    ]\n","    \n","    # models, data_train, data_validation, data_test, XY_dict\n","    combined_models_ld, data_train_ld, data_validation_ld, data_test_ld, data_access_ld = get_combined_features_from_models(\n","        to_combine_ld_no2nd_rNo2nd ,\n","        X_train,\n","        Y_train,\n","        X_validation,\n","        Y_validation,\n","        X_test, \n","        Y_test, \n","        reverse_one_hot=False)\n","        \n","    rcnn_combine_model = model_combination(f\"collaborative_model_{m1.name}_{m2.name}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                                           data_train_ld[0][0].shape  ) if merged_name is None else model_combination(merged_name, data_train_ld[0][0].shape  )\n","    rcnn_combine_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n","    if patience_count is None:\n","        patience_count  = 10000    \n","    callbacks_used_rcnn_combine = [\n","                        ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{rcnn_combine_model.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                                save_weights_only=False,\n","                                                monitor='accuracy',\n","                                                mode='max',\n","                                                save_best_only=True),\n","                        tf.keras.callbacks.EarlyStopping(patience=patience_count),\n","                        tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{rcnn_combine_model.name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True)\n","                        ]\n","    \n","    history_rcnn_combine = rcnn_combine_model.fit(data_train_ld[0], \n","                                                  data_train_ld[1], \n","                                                  validation_data=(data_validation_ld[0],to_categorical(data_validation_ld[1]-1)),\n","                                                  callbacks=callbacks_used_rcnn_combine, \n","                                                  verbose=2, \n","                                                  **kwargs)\n","    rcnn_combine_model.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{rcnn_combine_model.name}.h5\")    \n","    # rcnn_combine_model.evaluate(data_test_ld_no2nd_rNo2nd[0],data_test_ld_no2nd_rNo2nd[1][0]) \n","    \n","    return (rcnn_combine_model, history_rcnn_combine, data_access_ld )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtDSFGTdG-b0"},"source":["# 'new' data \n","X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData(is500=False,\n","                                                                                                    include_secondary=False,\n","                                                                                                    isColab=True)\n","X_train_1000e_w2nd, Y_train_1000e_w2nd, X_test_1000e_w2nd, Y_test_1000e_w2nd, X_val_1000e_w2nd, Y_val_1000e_w2nd = getE2eData(is500=False, include_secondary=True, isColab=True)\n","X_train_1000e_j2nd, Y_train_1000e_j2nd, X_test_1000e_j2nd, Y_test_1000e_j2nd, X_val_1000e_j2nd, Y_val_1000e_j2nd = getE2eDataJustSecondary(isColab=True)\n","# merge into a new train:\n","X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n","Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )    \n","\n","X_new_train_j2nd = np.concatenate( (X_train_1000e_j2nd, X_val_1000e_j2nd), axis=0 )\n","Y_new_train_j2nd = np.concatenate( (Y_train_1000e_j2nd, Y_val_1000e_j2nd), axis=0 )    \n","\n","X_new_train_w2nd = np.concatenate( (X_train_1000e_w2nd, X_val_1000e_w2nd), axis=0 )\n","Y_new_train_w2nd = np.concatenate( (Y_train_1000e_w2nd, Y_val_1000e_w2nd), axis=0 )  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoMBMjumG_DU"},"source":["# getE2eData88\n","X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData88()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uqJKLR30s-k","executionInfo":{"elapsed":185,"status":"ok","timestamp":1631104209779,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"},"user_tz":-60},"outputId":"720fd608-62f1-437a-d72c-e2ff385b76c4"},"source":["print(X_train_1000e.shape)\n","print(X_train_1000e[0].shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["(88433, 1000, 8)\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"qZ1VNN2mILPR"},"source":["# Model A\n","cnn_256, history_cnn_256 = compile_and_fit_model_basic(model_A_CNN_256, \n","                                                       f\"cnn_A_256_{'test88' if X_new_train.shape[0]>20000 else 'test13'}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                                                       X_train_1000e[0].shape, \n","                                                       X_train_1000e, \n","                                                       Y_train_1000e, \n","                                                       save_max_epoch=True, \n","                                                       save_final=True, \n","                                                       patience_count=60, \n","                                                       batch_size=1024, \n","                                                       epochs=500, \n","                                                       class_weight=None, \n","                                                       log_history=True,\n","                                                       validation_data=(X_val_1000e, Y_val_1000e)) \n","plot_history(history_cnn_256)\n","# validation_data=(X_val_1000e, Y_val_1000e),\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvjtInOEL5QR"},"source":["# Model E\n","rnn_E, history_rnn_E = compile_and_fit_model_basic(model_E_RNN,\n","                                                  f\"rnn_E_{'test88' if X_new_train.shape[0]>20000 else 'test13'}_{datetime.datetime.now():%Y%m%d%H%M%S}\",\n","                                                  X_train_1000e[0].shape,\n","                                                  X_train_1000e,\n","                                                  Y_train_1000e,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=60,\n","                                                  batch_size=128,\n","                                                  epochs=500,\n","                                                  class_weight=None,\n","                                                   validation_data=(X_val_1000e, Y_val_1000e)                                                  \n","                                                   )\n","plot_history(history_rnn_E)\n","# validation_data=(X_val_1000e, Y_val_1000e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_Htzco9Jbq6"},"source":["# # Continue model E run from before...\n","# rnn_E = load_model(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_119_0.979\")\n","# m = rnn_E\n","# patience_count=60\n","# callbacks_used = []\n","# callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","#                                       save_weights_only=False,\n","#                                       monitor = 'accuracy', # 'val_accuracy'\n","#                                       mode='max',\n","#                                       save_best_only=True))\n","# callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=patience_count))\n","# callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{m.name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n","# # check if continues from before..\n","\n","# m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# history_rnn_E = m.fit(X_train_1000e, Y_train_1000e, callbacks=callbacks_used, verbose=2,                                                   \n","#                       batch_size=128,\n","#                       epochs=500-118-6-119,\n","#                       class_weight=None,\n","#                       validation_data=(X_val_1000e, Y_val_1000e))\n","# m.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}_saved_model_after_fit\")  # Save the model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"hNyr0U9eMgUq"},"source":["# Merged Parallel model A&E\n","mcrnn_AE, history_mcrnn_AE = compile_and_fit_model_basic(merged_model_A_E,\n","                                                  f\"mcrnn_AE_{datetime.datetime.now():%Y%m%d%H%M%S}\",\n","                                                  X_train_1000e[0].shape,\n","                                                  X_train_1000e,\n","                                                  Y_train_1000e,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=60,\n","                                                  batch_size=128,\n","                                                  epochs=500,\n","                                                  class_weight=None,\n","                                                  validation_data=(X_val_1000e, Y_val_1000e)                                                  \n","                                                   )\n","plot_history(history_mcrnn_AE)\n","# evalutate model E on the test data\n","mcrnn_AE.evaluate(X_test_1000e, Y_test_1000e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"_tPjqRc0gKsL"},"source":["# Continue Merged Parallel model A&E run from before...\n","# mcrnn_AE = load_model(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/mcrnn_AE_20210830185445_model_092_0.990\")\n","# m = mcrnn_AE\n","# patience_count=60\n","# callbacks_used = []\n","# callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","#                                       save_weights_only=False,\n","#                                       monitor = 'accuracy', # 'val_accuracy'\n","#                                       mode='max',\n","#                                       save_best_only=True))\n","# callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=patience_count))\n","# callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{m.name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n","# # check if continues from before..\n","\n","# m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# history_rnn_E = m.fit(X_train_1000e, Y_train_1000e, callbacks=callbacks_used, verbose=2,                                                   \n","#                       batch_size=128,\n","#                       epochs=500-121-92,\n","#                       class_weight=None,\n","#                       validation_data=(X_val_1000e, Y_val_1000e))\n","# m.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}_saved_model_after_fit\")  # Save the model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apjkX6rg9yD_"},"source":["# Sequential Model A & E\n","seQcrnn_AE, history_seQcrnn_AE = compile_and_fit_model_basic(sequential_model_A_E,\n","                                                  f\"seQcrnn_AE_{datetime.datetime.now():%Y%m%d%H%M%S}\",\n","                                                  X_train_1000e[0].shape,\n","                                                  X_train_1000e,\n","                                                  Y_train_1000e,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=60,\n","                                                  batch_size=1024,\n","                                                  epochs=500,\n","                                                  class_weight=None,\n","                                                  validation_data=(X_val_1000e, Y_val_1000e)\n","                                                   )\n","plot_history(history_seQcrnn_AE)\n","# evalutate sequential model AE on the test data\n","seQcrnn_AE.evaluate(X_test_1000e, Y_test_1000e)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2s-UdKY6Co_q"},"source":["# Collaborative / Combined Model A&E \n","collabcrnn_AE, history_collabcrnn_AE, data_access_internals = compile_and_fit_model_collaborative(\n","    model_cnn_A, \"leaky_re_lu_6\",\n","    model_rnn_E, \"dropout_7\",\n","    X_train_1000e[0].shape,\n","    X_train_1000e,\n","    Y_train_1000e,\n","    X_val_1000e,\n","    Y_val_1000e,\n","    X_test_1000e,\n","    Y_test_1000e,\n","    log_history=True,\n","    save_max_epoch=True,\n","    save_final=True,\n","    patience_count=60,\n","    batch_size=1024,\n","    epochs=500,\n","    class_weight=None)\n","\n","plot_history(history_collabcrnn_AE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAGBWfIjL5FQ"},"source":["#  CNN A Get the dictionary containing each metric and the loss for each epoch\n","history_cnn_256_dict = history_cnn_256.history\n","# Save it under the form of a json file\n","json.dump(history_cnn_256_dict, open(f'/content/drive/MyDrive/ncRNA/data/history/history_cnn_256_dict_{cnn_256.name}.json', 'w'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3YAAFCmL415"},"source":["# RNN E Get the dictionary containing each metric and the loss for each epoch\n","history_rnn_E_dict = history_rnn_E.history\n","# Save it under the form of a json file\n","json.dump(history_rnn_E_dict, open(f'/content/drive/MyDrive/ncRNA/data/history/history_rnn_E_dict_{rnn_E.name}.json', 'w'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qPmrZseCABk"},"source":["# Sequential A&E Get the dictionary containing each metric and the loss for each epoch\n","history_seQcrnn_AE_dict = history_seQcrnn_AE.history\n","# Save it under the form of a json file\n","json.dump(history_seQcrnn_AE_dict, open(f'D:/GooDrive/ncRNA/data/history/history_seQcrnn_AE_dict_{seQcrnn_AE.name}.json', 'w'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FtYOPCpKjoJ"},"source":["# Merged Collaborative A&E Get the dictionary containing each metric and the loss for each epoch\n","history_mcrnn_AE_dict = history_mcrnn_AE.history\n","# Save it under the form of a json file\n","json.dump(history_mcrnn_AE_dict, open(f'/content/drive/MyDrive/ncRNA/data/history/history_mcrnn_AE_dict_{mcrnn_AE.name}.json', 'w'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggayZ7CTFMyv"},"source":["# Collaborative A&E Get the dictionary containing each metric and the loss for each epoch\n","history_collabcrnn_AE_dict = history_collabcrnn_AE.history\n","# Save it under the form of a json file\n","json.dump(history_collabcrnn_AE_dict, open(f'D:/GooDrive/ncRNA/data/history/history_collabcrnn_AE_dict_{collabcrnn_AE.name}.json', 'w'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoFgOdZjaEBy"},"source":["num_repeats = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoorsTIlfmzd"},"source":["# run CNN A in repeat and save outputs\n","cnn_A_data = [ compile_and_fit_model_basic(model_A_CNN_256, \n","                                          f\"cnn_A_repeat{repeat_counter+1}_256_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                                          X_train_1000e[0].shape, \n","                                          X_train_1000e, \n","                                          Y_train_1000e, \n","                                          save_max_epoch=True, \n","                                          save_final=True, \n","                                          patience_count=60, \n","                                          batch_size=1024, \n","                                          epochs=500, \n","                                          class_weight=None, \n","                                          log_history=True,\n","                                          validation_data=(X_val_1000e, Y_val_1000e))  for repeat_counter in range(num_repeats) ]\n","\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZ75O_X4nvpb","executionInfo":{"elapsed":18375,"status":"ok","timestamp":1630756481700,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"},"user_tz":-60},"outputId":"f755791a-4696-4935-d417-6456a11d2931"},"source":["# load model A and model E from disk\n","model_cnn_A = load_model(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/cnn_A_256_20210827212633_model_491_0.995\")\n","model_rnn_E = load_model(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_060_0.983\")\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"code","metadata":{"id":"m_PsfUFYlzSn"},"source":["# run Collaborative A&E in repeat and save outputs\n","collabcrnn_AE_data = [ compile_and_fit_model_collaborative(\n","      model_cnn_A, \"leaky_re_lu_6\",\n","      model_rnn_E, \"dropout_7\",\n","      X_train_1000e[0].shape,\n","      X_train_1000e,\n","      Y_train_1000e,\n","      X_val_1000e,\n","      Y_val_1000e,\n","      X_test_1000e,\n","      Y_test_1000e,\n","      log_history=True,\n","      save_max_epoch=True,\n","      save_final=True,\n","      merged_name = f\"collabcrnn_AE_repeat{repeat_counter+1}_{datetime.datetime.now():%Y%m%d%H%M%S}\",\n","      patience_count=60,\n","      batch_size=1024,\n","      epochs=500,\n","      class_weight=None) for repeat_counter in range(num_repeats) ]\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnECW9aWHiE_"},"source":["# run sequential A& in repeat and save outputs\n","num_repeats_here = 10\n","seQcrnn_AE_data = [ compile_and_fit_model_basic(sequential_model_A_E,\n","                                                  f\"seQcrnn_AE_repeat{repeat_counter+1}_{datetime.datetime.now():%Y%m%d%H%M%S}\",\n","                                                  X_train_1000e[0].shape,\n","                                                  X_train_1000e,\n","                                                  Y_train_1000e,\n","                                                  save_max_epoch=True,\n","                                                  save_final=True,\n","                                                  patience_count=60,\n","                                                  batch_size=1024,\n","                                                  epochs=500,\n","                                                  class_weight=None,\n","                                                  validation_data=(X_val_1000e, Y_val_1000e)\n","                                                   ) for repeat_counter in range(num_repeats_here) ]\n","\n","\n"],"execution_count":null,"outputs":[]}]}