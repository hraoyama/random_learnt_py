{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_merge_hyperband.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1ygmqkRNc3y4_CMOoFKdhyUnFVmqzAHmu","authorship_tag":"ABX9TyOkJU1MR27xiyAPitcE1yFA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mR2TvAiGQilA","executionInfo":{"status":"ok","timestamp":1621265230829,"user_tz":-60,"elapsed":10497,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"15fa6691-679a-4423-d4ca-c486ff2b8712"},"source":["%pip install keras_self_attention\n","%pip install keras-tuner"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting keras_self_attention\n","  Downloading https://files.pythonhosted.org/packages/c3/34/e21dc6adcdab2be03781bde78c6c5d2b2136d35a1dd3e692d7e160ba062a/keras-self-attention-0.49.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.4.3)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras_self_attention) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras_self_attention) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras_self_attention) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras_self_attention) (1.15.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.49.0-cp37-none-any.whl size=19468 sha256=a5ca961498a3ea2e51c8835ad68843ce390a4cdc73cff2db3028a395f036e5f6\n","  Stored in directory: /root/.cache/pip/wheels/6f/9d/c5/26693a5092d9313daeae94db04818fc0a2b7a48ea381989f34\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.49.0\n","Collecting keras-tuner\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n","\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n","Collecting terminaltables\n","  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n","Building wheels for collected packages: keras-tuner, terminaltables\n","  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=4c47024cbc473c60663250abb9c80051d9a82519d8a2317efde772a40c51d46c\n","  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n","  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=09743d4af5172fdde2fdf88ebc3f19b70c3c4cb0164e918ea229d868b355c787\n","  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n","Successfully built keras-tuner terminaltables\n","Installing collected packages: terminaltables, colorama, keras-tuner\n","Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dWN1YdD7Qvc2","executionInfo":{"status":"ok","timestamp":1621265232290,"user_tz":-60,"elapsed":11951,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.layers import BatchNormalization, Dense, LeakyReLU, Dropout\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","import site\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","import os"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmcD3FuxQw7Q","executionInfo":{"status":"ok","timestamp":1621265232291,"user_tz":-60,"elapsed":11948,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfyld79IQ0j0","executionInfo":{"status":"ok","timestamp":1621265233351,"user_tz":-60,"elapsed":13004,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Model\n","import errno\n","import os\n","from collections import defaultdict\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.summary import create_file_writer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import kerastuner as kt\n","from kerastuner import HyperModel\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.layers import BatchNormalization, Dense, LeakyReLU, Dropout\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","import site\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","import os\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DZ2zSmhRDJf","executionInfo":{"status":"ok","timestamp":1621265233353,"user_tz":-60,"elapsed":13001,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def get_features_model(model_name, combined_models):\n","    for ids, models in combined_models:\n","        if models[0].name == model_name:\n","            return models[1]\n","    return None\n","\n","def get_features_from_combined_models(combined_models, X_input):\n","    data_features = []\n","    for ids, models in combined_models:\n","        data_features.append(np.array(models[1].predict(X_input), dtype='float64'))\n","    return np.concatenate(tuple(data_features), axis=1)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHCMi9z-xlo-","executionInfo":{"status":"ok","timestamp":1621265235127,"user_tz":-60,"elapsed":14769,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def coShuffled_vectors(X, Y):\n","    if tf.shape(X)[0] == tf.shape(Y)[0]:\n","        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n","        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n","        return (tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs))\n","    else:\n","        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n","\n","def reverse_one_hot(Y_input):\n","    return np.apply_along_axis(np.argmax, 1, Y_input) + 1\n","\n","def getNpArrayFromH5(hf_Data):\n","    X_train = hf_Data['Train_Data']  # Get train set\n","    X_train = np.array(X_train)\n","    Y_train = hf_Data['Label']  # Get train label\n","    Y_train = np.array(Y_train)\n","    return X_train, Y_train\n","\n","\n","# data extraction\n","def getData(is500=True, shuffle=False, ise2e=False, include_secondary=False, validation_split=None, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Train_Data\" if ise2e else \"Fold_10_Train_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Test_Data\" if ise2e else \"Fold_10_Test_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    X_validation = Y_validation = None\n","    if validation_split is not None:\n","        # sklearn split shuffles anyway\n","        X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_split)\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eData(is500=True, shuffle=False, include_secondary=False, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Secondary_Data_1136.h5', 'r') if include_secondary else h5.File(\n","        f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def plot_history(history):\n","    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n","    for k, v in history.history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        else:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()\n","\n","\n","def get_layer_by_name(layers, name, return_first=True):\n","    matching_named_layers = [l for l in layers if l.name == name]\n","    if not matching_named_layers:\n","        return None\n","    return matching_named_layers[0] if return_first else matching_named_layers\n","\n","\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","\n","def source_model(model_func, model_name, input_shape):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    return m\n","\n","\n","def compile_and_fit_model_with_tb(model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_every_epoch=True,\n","                                  save_final=False,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    tb_callback = TensorBoard(log_dir=f'{m.name}_logs', histogram_freq=kwargs.pop(\"histogram_freq\", 1))\n","    if save_every_epoch:\n","        tb_callback.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=[tb_callback], verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","    # m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n","\n","\n","def compile_and_fit_model(model_func,\n","                          model_name,\n","                          input_shape,\n","                          X_train,\n","                          Y_train,\n","                          save_every_epoch=True,\n","                          save_final=False,\n","                          **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_every_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","\n","\n","def compile_model_and_fit_with_custom_loop(model_func,\n","                                           model_name,\n","                                           input_shape,\n","                                           X_train,\n","                                           Y_train,\n","                                           **kwargs):\n","    make_dir_if_not_exist(model_name)\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    train_writer = create_file_writer(f'{m.name}_logs/train/')\n","    test_writer = create_file_writer(f'{m.name}_logs/test/')\n","    train_step = test_step = 0\n","\n","    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","    optimizer = tf.keras.optimizers.Adam()\n","    num_epochs = kwargs.get(\"epochs\", 10)\n","\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    BATCH_SIZE = kwargs.get(\"batch_size\", 32)\n","    X_test, Y_test = kwargs.get(\"validation_data\", (None, None))\n","    if X_test is None:\n","        raise ValueError(\"Missing X validation data\")\n","    if Y_test is None:\n","        raise ValueError(\"Missing Y validation data\")\n","\n","    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n","    train_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    train_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n","    test_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    test_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","\n","    for epoch in range(num_epochs):\n","        # Iterate through training set\n","        for batch_idx, (x, y) in enumerate(train_dataset_tf):\n","            with tf.GradientTape() as tape:\n","                y_pred = m(x, training=True)\n","                loss = loss_fn(y, y_pred)\n","\n","            gradients = tape.gradient(loss, m.trainable_weights)\n","            optimizer.apply_gradients(zip(gradients, m.trainable_weights))\n","            acc_metric.update_state(y, y_pred)\n","\n","            with train_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=train_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=train_step,\n","                )\n","                train_step += 1\n","        # Reset accuracy in between epochs (and for testing and test)\n","        acc_metric.reset_states()\n","        # Iterate through test set\n","        for batch_idx, (x, y) in enumerate(test_dataset_tf):\n","            y_pred = m(x, training=False)\n","            loss = loss_fn(y, y_pred)\n","            acc_metric.update_state(y, y_pred)\n","            with test_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=test_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=test_step,\n","                )\n","                test_step += 1\n","\n","        acc_metric.reset_states()  # Reset accuracy in between epochs (and for testing and test)\n","\n","    return m\n","\n","\n","\n","def run_mirrored_strategy(model_func, base_batch_size, nepochs, x_train, y_train, x_test, y_test, **kwargs):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        model = model_func()\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adam(),\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","            metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n","        )\n","    batch_size_mirr_strat = base_batch_size * strategy.num_replicas_in_sync\n","    history = model.fit(x_train, y_train, epochs=nepochs, batch_size=batch_size_mirr_strat,\n","                        validation_data=(x_test, y_test),\n","                        **kwargs)\n","    return model, history\n","\n","\n","def sparse_setdiff(a1, a2):\n","    a1a = a1.reshape(a1.shape[0], -1)\n","    a2a = a2.reshape(a2.shape[0], -1)\n","    spa2a = [np.where(x)[0].tolist() for x in a2a]\n","    spa1a = [np.where(x)[0].tolist() for x in a1a]\n","    idxs_to_keep = []\n","    for idx, sample in enumerate(spa1a):\n","        try:\n","            spa2a.index(sample)\n","        except ValueError:\n","            # not in list\n","            idxs_to_keep.append(idx)\n","    return a1[idxs_to_keep], idxs_to_keep\n","\n","def get_combined_features_from_models(\n","    \n","        to_combine,\n","        X_train, Y_train,\n","        X_test, Y_test,\n","        reverse_one_hot=False,\n","        normalize_X_func=None):\n","    \n","    models = []\n","    models_dict = {}\n","    X_trains_out = []\n","    X_test_out = []\n","    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n","\n","    models_have_different_inputs = isinstance(Y_train,list)\n","\n","    if reverse_one_hot:\n","        if models_have_different_inputs:\n","            Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train) + 1\n","            Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test) + 1\n","        else:\n","            Y_train_new = [ np.apply_along_axis(np.argmax, 1, y_train) + 1 for y_train in Y_train ]  \n","            Y_test_new = [ np.apply_along_axis(np.argmax, 1, y_test) + 1 for y_test in Y_test ]              \n","    else:\n","        if models_have_different_inputs:\n","            Y_train_new = Y_train.copy()\n","            Y_test_new = Y_test.copy()\n","        else:\n","            Y_train_new = [ y_train.copy() for y_train in Y_train ] \n","            Y_test_new = [ y_test.copy() for y_test in Y_train ] \n","            \n","\n","    extraction_counter =0\n","    for model_file_name, layer_name, kwargs in to_combine:\n","        model_here = None\n","        if isinstance(model_file_name, tf.keras.models.Model):\n","            model_here = model_file_name\n","            model_file_name = model_here.name\n","        else:\n","            if model_file_name in models_dict.keys():\n","                model_here = models_dict[model_file_name]\n","            else:\n","                model_here = tf.keras.models.load_model(model_file_name,\n","                                                        **kwargs) if kwargs is not None else tf.keras.models.load_model \\\n","                    (model_file_name)\n","\n","        features_model = Model(model_here.input,\n","                               get_layer_by_name(model_here.layers, layer_name).output)\n","        \n","        if normalize_X_func is None:\n","            X_trains_out.append(np.array(features_model.predict(X_train if not models_have_different_inputs else X_train[extraction_counter]), dtype='float64'))\n","            X_test_out.append(np.array(features_model.predict(X_test if not models_have_different_inputs else X_test[extraction_counter]), dtype='float64'))\n","        else:\n","            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train if not models_have_different_inputs else X_train[extraction_counter])), dtype='float64'))\n","            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test if not models_have_different_inputs else X_test[extraction_counter])), dtype='float64'))\n","        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n","        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n","        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n","        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n","        models.append(((model_file_name, layer_name), (model_here, features_model)))\n","        models_dict[model_file_name] = model_here\n","        extraction_counter += 1\n","\n","    X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n","    X_test_new = np.concatenate(tuple(X_test_out), axis=1)\n","\n","    data_train = (X_train_new, Y_train_new)\n","    data_test = (X_test_new, Y_test_new)\n","\n","    return models, data_train, data_test, XY_dict\n","\n","class CNNHyperModel(HyperModel):\n","    def __init__(self, model_name, input_shape, num_classes):\n","        self.input_shape = input_shape\n","        self.num_classes = num_classes\n","        self.model_name = model_name\n","\n","    def build(self, hp):\n","        inputs = tf.keras.Input(shape=self.input_shape)\n","        x = inputs\n","\n","        for idx, i in enumerate(range(hp.Int('conv128_blocks_with_normalizations', 1, 6, default=4))):\n","            x = Conv1D(128, 3, padding='same', name=f\"conv1D_128_{idx}\")(x)\n","            if hp.Boolean(f'conv128_has_leaky_relu_{idx}', default=True):\n","                x = LeakyReLU()(x)\n","            if hp.Boolean(f'conv128_has_max_pooling_{idx}', default=True):\n","                x = MaxPooling1D()(x)\n","            if hp.Boolean(f'conv128_has_batchnorm_{idx}', default=True):\n","                x = BatchNormalization()(x)\n","            if hp.Boolean(f'conv128_has_gaussiannoise_{idx}', default=True):\n","                x = GaussianNoise(hp.Float(f'conv128_gaussiannoise_{idx}',\n","                                       min_value=1e-5,\n","                                       max_value=1e1,\n","                                       sampling='LOG',\n","                                       default=0.05\n","                                       ))(x)\n","        for idx, i in enumerate(range(hp.Int('conv256_blocks_with_normalizations', 1, 4, default=2))):\n","            x = Conv1D(256, 3, padding='same', name=f\"conv1D_256_{idx}\")(x)\n","            if hp.Boolean(f'conv256_has_leaky_relu_{idx}', default=True):\n","                x = LeakyReLU()(x)\n","            if hp.Boolean(f'conv256_has_max_pooling_{idx}', default=True):\n","                x = MaxPooling1D()(x)\n","            if hp.Boolean(f'conv256_has_batchnorm_{idx}', default=True):\n","                x = BatchNormalization()(x)\n","            if hp.Boolean(f'conv256_has_gaussiannoise_{idx}', default=True):\n","                x = GaussianNoise(hp.Float(f'conv256_gaussiannoise_{idx}',\n","                                       min_value=1e-5,\n","                                       max_value=1e1,\n","                                       sampling='LOG',\n","                                       default=0.05\n","                                       ))(x)\n","        x = Flatten(name=\"last_flatten\")(x)\n","        for idx, i in enumerate(range(hp.Int('final_dense', 1, 5, default=2))):\n","            x = Dense(units=hp.Choice(f'final_dense_num_nodes_{idx}', values=[16, 32, 64, 128], default=128),\n","                  activation=hp.Choice(f'final_dense_kernel_activation_{idx}',\n","                                       values=['exponential', 'gelu', 'elu', 'relu', 'tanh'], default='relu'),\n","                  kernel_initializer='RandomNormal',\n","                  bias_initializer='zeros',\n","                  name=f\"final_dense_{idx}\")(x)\n","            if hp.Boolean(f'final_dense_has_dropout_{idx}', default=True):\n","                x = Dropout(hp.Float(f'final_dense_dropout_{idx}',\n","                                 min_value=0.05,\n","                                 max_value=0.75,\n","                                 step=0.05,\n","                                 default=0.2\n","                                 ), name=f\"final_dense_dropout_{idx}\")(x)\n","        outputs = Dense(self.num_classes, activation='softmax', name=\"last_softmax\")(x)\n","        model = tf.keras.Model(inputs, outputs, name=self.model_name)\n","        #  m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        if hp.Boolean('optimize_adam', default=True):\n","            model.compile(\n","                optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-5, 1e-1, sampling='log')),\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n","        else:\n","            model.compile(\n","                optimizer=hp.Choice('final_optimizer',\n","                                    values=['adam', 'SGD', 'RMSprop', 'Adadelta', 'Nadam', 'Adamax', 'Adagrad'],\n","                                    default='adam'),\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n","        return model\n","    \n","    \n","    \n","\n","def reinitialize_weights(model):\n","    for ix, layer in enumerate(model.layers):\n","        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n","            weight_initializer = model.layers[ix].kernel_initializer\n","            bias_initializer = model.layers[ix].bias_initializer\n","    \n","            old_weights, old_biases = model.layers[ix].get_weights()\n","    \n","            model.layers[ix].set_weights([\n","                weight_initializer(shape=old_weights.shape),\n","                bias_initializer(shape=len(old_biases))])            \n","    return model\n","\n","def reverse_tensor(X):\n","    return tf.gather(X, tf.reverse(tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32),(0,)) )\n","\n","   \n","def get_confusion_matrix_classification(model, X, Y_true):\n","    y_pred = model.predict(X)\n","    y_true = np.apply_along_axis(np.argmax, 1, Y_true)\n","    y_pred = np.apply_along_axis(np.argmax, 1, y_pred)\n","    return (confusion_matrix(y_true, y_pred), y_pred, y_true)\n","\n","def misclass_perc_to_weight(input_confusion, add_base=True, func=None):\n","    perc_misclassified = 1.0 - np.array([ input_confusion[x,x] for x in np.arange(input_confusion.shape[0]).tolist() ])/input_confusion.sum(axis=1)\n","    \n","    base_val = min(perc_misclassified[perc_misclassified>0.0])\n","    if add_base:        \n","        perc_misclassified = perc_misclassified + base_val\n","    \n","    perc_misclassified = [ x/base_val for x in perc_misclassified]\n","    \n","    return dict([ (idx, func(perc_val)) if func is not None else (idx, perc_val) for idx, perc_val in enumerate(perc_misclassified) ])\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rFUtecuyVcM","executionInfo":{"status":"ok","timestamp":1621266326813,"user_tz":-60,"elapsed":533,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["class DNNFeatureMergeModel(HyperModel):\n","    \n","    def __init__(self, model_name, input_shape, num_classes):\n","        self.input_shape = input_shape\n","        self.num_classes = num_classes\n","        self.model_name = model_name\n","\n","    def build(self, hp):\n","        inputs = tf.keras.Input(shape=self.input_shape)\n","        x = inputs\n","        for idx, i in enumerate(range(hp.Int('dense_blocks_with_normalizations', 1, 10, default=1))):\n","            if hp.Boolean(f'has_batchnormalization_{idx}', default=True):\n","                x = BatchNormalization()(x)\n","            x = Dense(units= hp.Choice(f'dense_block_nunits_{idx}', values=[16,32,64,128,256,512],default=128), \n","                      activation=hp.Choice(f'dense_activation_{idx}',\n","                                       values=['selu', 'gelu', 'elu', 'relu', 'tanh', 'linear'], default='relu'),\n","                  kernel_initializer=hp.Choice(f'dense_kernel_init_{idx}',\n","                                       values=['HeNormal', 'HeUniform','VarianceScaling', 'LecunNormal','LecunUniform', 'GlorotUniform', 'GlorotNormal', 'RandomNormal', 'Ones', 'Orthogonal'], default='RandomNormal'),\n","                  bias_initializer='zeros',\n","                  name=f'dense_{idx}')(x)\n","            if hp.Boolean(f'has_leakyrelu_{idx}', default=True):\n","                x = LeakyReLU()(x)\n","            if hp.Boolean(f'has_dropout_{idx}', default=True):\n","                x = Dropout(hp.Float(f'dense_dropout_value_{idx}',\n","                                 min_value=0.0,\n","                                 max_value=0.99,\n","                                 step=0.025,\n","                                 default=0.6\n","                                 ), name=f\"dense_dropout_{idx}\")(x)\n","        outputs = Dense(self.num_classes, activation='softmax', name=\"last_softmax\")(x)\n","        model = tf.keras.Model(inputs, outputs, name=self.model_name)\n","        if hp.Boolean('optimize_adam', default=True):\n","            model.compile(\n","                optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-7, 0.5e-1, sampling='log')),\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n","        else:\n","            model.compile(\n","                optimizer=hp.Choice('final_optimizer',\n","                                    values=['adam', 'SGD', 'RMSprop', 'Adadelta', 'Nadam', 'Adamax', 'Adagrad'],\n","                                    default='adam'),\n","                loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n","        return model\n","    \n","\n","class BatchSizeTuner(kt.tuners.Hyperband):\n","   def run_trial(self, trial, *args, **kwargs):\n","#     # You can add additional HyperParameters for preprocessing and custom training loops via overriding `run_trial`\n","     kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 16, 256, step=32)\n","     super(BatchSizeTuner, self).run_trial(trial, *args, **kwargs)\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"VM12SQkLykKU","executionInfo":{"status":"ok","timestamp":1621265241398,"user_tz":-60,"elapsed":21028,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["    # 'new' data\n","    X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData(is500=False,\n","                                                                                                    include_secondary=False,\n","                                                                                                    isColab=True)\n","    X_train_1000e_w2nd, Y_train_1000e_w2nd, X_test_1000e_w2nd, Y_test_1000e_w2nd, X_val_1000e_w2nd, Y_val_1000e_w2nd = getE2eData(is500=False,\n","                                                                                                    include_secondary=True,\n","                                                                                                    isColab=True)\n","    "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRzhAOuY0B33","executionInfo":{"status":"ok","timestamp":1621265241399,"user_tz":-60,"elapsed":21024,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"4ff4e52c-2cc9-4f00-cbd5-6b43debdd558"},"source":["print (X_train_1000e.shape)\n","print (X_train_1000e_w2nd.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(6858, 1000, 8)\n","(6858, 1136, 12)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuiFwxcoziU9","executionInfo":{"status":"ok","timestamp":1621265272717,"user_tz":-60,"elapsed":52336,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"3ea1026f-3a90-47c7-d35c-f84f1cb883b6"},"source":["    # merge into a new train:\n","    X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n","    Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )    \n","    \n","    X_new_train_w2nd = np.concatenate( (X_train_1000e_w2nd, X_val_1000e_w2nd), axis=0 )\n","    Y_new_train_w2nd = np.concatenate( (Y_train_1000e_w2nd, Y_val_1000e_w2nd), axis=0 )    \n","\n","    # CNNs no secondary\n","    mCNN1_1000 = load_model(\"./drive/MyDrive/data_papers/ncRNA/CNN_baseline_May16_e2e1000_256.h5\") # CNN on 256 1st dens\n","    mCNN1_1000._name = \"cnn_merged_newdata_finalist_1\"\n","\n","    mCNN2_1000 = load_model(\"./drive/MyDrive/data_papers/ncRNA/CNN_baseline_May16_e2e.h5\")   # CNN on 128 1st dens\n","    mCNN2_1000._name = \"cnn_merged_newdata_finalist_2\"\n","    \n","    mCNN_1000 = load_model(\"./drive/MyDrive/data_papers/ncRNA/cnn_noTest_20210516_model_445_0.998\")   # CNN on 128 1st dens\n","    mCNN_1000._name = \"cnn_merged_newdata_colab_finalist\"\n","    \n","    # RNN no secondary\n","    mCNN_1000_w2nd = load_model(\"./drive/MyDrive/data_papers/ncRNA/CNN_baseline_May16_e2e_secondary.h5\", custom_objects=SeqWeightedAttention.get_custom_objects())\n","    mCNN_1000_w2nd._name = \"cnn_merged_newdata_w_secondary_finalist\"\n","\n","    \n","    mCNN1_1000.evaluate(X_test_1000e, Y_test_1000e)  # 96.15% \n","    mCNN2_1000.evaluate(X_test_1000e, Y_test_1000e)  # 95.80 %  \n","    mCNN_1000.evaluate(X_test_1000e, Y_test_1000e)  # 95.57% \n","    mCNN_1000_w2nd.evaluate(X_test_1000e_w2nd, Y_test_1000e_w2nd)  # 94.64 %\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["27/27 [==============================] - 16s 10ms/step - loss: 0.2015 - accuracy: 0.9615\n","27/27 [==============================] - 0s 6ms/step - loss: 0.2149 - accuracy: 0.9580\n","27/27 [==============================] - 0s 6ms/step - loss: 0.2126 - accuracy: 0.9557\n","27/27 [==============================] - 0s 7ms/step - loss: 0.2043 - accuracy: 0.9464\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.20432178676128387, 0.9463869333267212]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"yBTr_vs33xmj","executionInfo":{"status":"ok","timestamp":1621265276923,"user_tz":-60,"elapsed":56537,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["    to_combine_last_layers_no2nd = [\n","        (mCNN_1000, \"dense_2\", None),\n","        (mCNN1_1000, \"dense_26\", None),\n","        (mCNN2_1000, \"dense_14\", None)\n","    ]\n","\n","    combined_models_no2nd, data_train_ll_no2nd, data_test_ll_no2nd, data_access_ll_no2nd = get_combined_features_from_models(\n","        to_combine_last_layers_no2nd,\n","        [ X_new_train, X_new_train, X_new_train],\n","        [ Y_new_train, Y_new_train, Y_new_train], \n","        [ X_test_1000e, X_test_1000e, X_test_1000e],\n","        [ Y_test_1000e, Y_test_1000e, Y_test_1000e],\n","        reverse_one_hot=False)\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGiH8Wwl339i","executionInfo":{"status":"ok","timestamp":1621266332505,"user_tz":-60,"elapsed":571,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["hypermodel_lastlayers_merge_1 = DNNFeatureMergeModel(model_name=\"combine_last_layers_no2nd\", input_shape=(data_train_ll_no2nd[0].shape[1],), num_classes=data_train_ll_no2nd[1][0].shape[-1] )\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EogtPah4JLc","executionInfo":{"status":"ok","timestamp":1621266860282,"user_tz":-60,"elapsed":525820,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"e2fe7173-6f94-4b0a-b517-0d83bc0e7685"},"source":["tuner = BatchSizeTuner(\n","    hypermodel_lastlayers_merge_1,\n","    max_epochs=4,\n","    objective='loss',\n","    executions_per_trial=4,\n","    hyperband_iterations=6,\n","    seed=123,\n","    directory=f'{hypermodel_lastlayers_merge_1.model_name}_hyperband'\n",")\n","\n","tuner.search(data_train_ll_no2nd[0], data_train_ll_no2nd[1][0],\n","             epochs=50,\n","             callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n","\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Trial 60 Complete [00h 00m 06s]\n","loss: 1.444082885980606\n","\n","Best loss So Far: 0.002559129789005965\n","Total elapsed time: 00h 08m 45s\n","INFO:tensorflow:Oracle triggered exit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aaw6v-2q4fiT","executionInfo":{"status":"ok","timestamp":1621268373118,"user_tz":-60,"elapsed":712,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"6e2e3358-f8d0-46d1-da16-8b9b2dcf647d"},"source":["# Show a summary of the search\n","tuner.results_summary()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Results summary\n","Results in combine_last_layers_no2nd_hyperband/untitled_project\n","Showing 10 best trials\n","Objective(name='loss', direction='min')\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 3\n","has_batchnormalization_0: True\n","dense_block_nunits_0: 512\n","dense_activation_0: relu\n","dense_kernel_init_0: HeNormal\n","has_leakyrelu_0: False\n","has_dropout_0: False\n","dense_dropout_value_0: 0.925\n","optimize_adam: True\n","learning_rate: 0.00014814338860744293\n","batch_size: 176\n","final_optimizer: RMSprop\n","has_batchnormalization_1: False\n","dense_block_nunits_1: 256\n","dense_activation_1: gelu\n","dense_kernel_init_1: GlorotUniform\n","has_leakyrelu_1: False\n","has_dropout_1: True\n","dense_dropout_value_1: 0.17500000000000002\n","has_batchnormalization_2: False\n","dense_block_nunits_2: 512\n","dense_activation_2: gelu\n","dense_kernel_init_2: GlorotNormal\n","has_leakyrelu_2: False\n","has_dropout_2: False\n","dense_dropout_value_2: 0.875\n","has_batchnormalization_3: True\n","dense_block_nunits_3: 512\n","dense_activation_3: linear\n","dense_kernel_init_3: HeUniform\n","has_leakyrelu_3: True\n","has_dropout_3: True\n","dense_dropout_value_3: 0.35000000000000003\n","has_batchnormalization_4: False\n","dense_block_nunits_4: 16\n","dense_activation_4: elu\n","dense_kernel_init_4: LecunNormal\n","has_leakyrelu_4: False\n","has_dropout_4: False\n","dense_dropout_value_4: 0.2\n","has_batchnormalization_5: False\n","dense_block_nunits_5: 16\n","dense_activation_5: relu\n","dense_kernel_init_5: HeNormal\n","has_leakyrelu_5: False\n","has_dropout_5: True\n","dense_dropout_value_5: 0.9750000000000001\n","has_batchnormalization_6: False\n","dense_block_nunits_6: 16\n","dense_activation_6: linear\n","dense_kernel_init_6: VarianceScaling\n","has_leakyrelu_6: False\n","has_dropout_6: False\n","dense_dropout_value_6: 0.25\n","has_batchnormalization_7: False\n","dense_block_nunits_7: 512\n","dense_activation_7: relu\n","dense_kernel_init_7: HeNormal\n","has_leakyrelu_7: False\n","has_dropout_7: True\n","dense_dropout_value_7: 0.07500000000000001\n","has_batchnormalization_8: True\n","dense_block_nunits_8: 128\n","dense_activation_8: elu\n","dense_kernel_init_8: GlorotUniform\n","has_leakyrelu_8: False\n","has_dropout_8: True\n","dense_dropout_value_8: 0.17500000000000002\n","has_batchnormalization_9: True\n","dense_block_nunits_9: 128\n","dense_activation_9: selu\n","dense_kernel_init_9: LecunUniform\n","has_leakyrelu_9: False\n","has_dropout_9: True\n","dense_dropout_value_9: 0.35000000000000003\n","tuner/epochs: 4\n","tuner/initial_epoch: 0\n","tuner/bracket: 0\n","tuner/round: 0\n","Score: 0.002559129789005965\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 1\n","has_batchnormalization_0: False\n","dense_block_nunits_0: 32\n","dense_activation_0: tanh\n","dense_kernel_init_0: HeNormal\n","has_leakyrelu_0: True\n","has_dropout_0: False\n","dense_dropout_value_0: 0.525\n","optimize_adam: False\n","learning_rate: 2.2408847208615633e-05\n","tuner/epochs: 2\n","tuner/initial_epoch: 0\n","tuner/bracket: 1\n","tuner/round: 0\n","batch_size: 16\n","final_optimizer: adam\n","Score: 0.03953492920845747\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 1\n","has_batchnormalization_0: False\n","dense_block_nunits_0: 32\n","dense_activation_0: tanh\n","dense_kernel_init_0: HeNormal\n","has_leakyrelu_0: True\n","has_dropout_0: False\n","dense_dropout_value_0: 0.525\n","optimize_adam: False\n","learning_rate: 2.2408847208615633e-05\n","tuner/epochs: 4\n","tuner/initial_epoch: 2\n","tuner/bracket: 1\n","tuner/round: 1\n","batch_size: 16\n","final_optimizer: adam\n","tuner/trial_id: 9a5341e8bdecdbafa91e86081af77fb2\n","Score: 0.04224065225571394\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 2\n","has_batchnormalization_0: True\n","dense_block_nunits_0: 32\n","dense_activation_0: elu\n","dense_kernel_init_0: Orthogonal\n","has_leakyrelu_0: True\n","has_dropout_0: False\n","dense_dropout_value_0: 0.17500000000000002\n","optimize_adam: False\n","learning_rate: 0.012669725212085219\n","batch_size: 112\n","final_optimizer: Adamax\n","has_batchnormalization_1: True\n","dense_block_nunits_1: 64\n","dense_activation_1: gelu\n","dense_kernel_init_1: HeUniform\n","has_leakyrelu_1: True\n","has_dropout_1: True\n","dense_dropout_value_1: 0.225\n","has_batchnormalization_2: False\n","dense_block_nunits_2: 128\n","dense_activation_2: selu\n","dense_kernel_init_2: RandomNormal\n","has_leakyrelu_2: True\n","has_dropout_2: True\n","dense_dropout_value_2: 0.5750000000000001\n","has_batchnormalization_3: False\n","dense_block_nunits_3: 512\n","dense_activation_3: tanh\n","dense_kernel_init_3: GlorotNormal\n","has_leakyrelu_3: True\n","has_dropout_3: True\n","dense_dropout_value_3: 0.875\n","has_batchnormalization_4: False\n","dense_block_nunits_4: 32\n","dense_activation_4: gelu\n","dense_kernel_init_4: GlorotNormal\n","has_leakyrelu_4: True\n","has_dropout_4: False\n","dense_dropout_value_4: 0.6000000000000001\n","has_batchnormalization_5: False\n","dense_block_nunits_5: 16\n","dense_activation_5: linear\n","dense_kernel_init_5: HeNormal\n","has_leakyrelu_5: False\n","has_dropout_5: False\n","dense_dropout_value_5: 0.55\n","has_batchnormalization_6: False\n","dense_block_nunits_6: 64\n","dense_activation_6: linear\n","dense_kernel_init_6: Ones\n","has_leakyrelu_6: False\n","has_dropout_6: False\n","dense_dropout_value_6: 0.9\n","has_batchnormalization_7: True\n","dense_block_nunits_7: 256\n","dense_activation_7: selu\n","dense_kernel_init_7: GlorotUniform\n","has_leakyrelu_7: False\n","has_dropout_7: True\n","dense_dropout_value_7: 0.9750000000000001\n","has_batchnormalization_8: True\n","dense_block_nunits_8: 16\n","dense_activation_8: elu\n","dense_kernel_init_8: VarianceScaling\n","has_leakyrelu_8: True\n","has_dropout_8: True\n","dense_dropout_value_8: 0.35000000000000003\n","has_batchnormalization_9: True\n","dense_block_nunits_9: 32\n","dense_activation_9: elu\n","dense_kernel_init_9: Ones\n","has_leakyrelu_9: True\n","has_dropout_9: False\n","dense_dropout_value_9: 0.1\n","tuner/epochs: 4\n","tuner/initial_epoch: 0\n","tuner/bracket: 0\n","tuner/round: 0\n","Score: 0.057986726984381676\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 7\n","has_batchnormalization_0: False\n","dense_block_nunits_0: 16\n","dense_activation_0: selu\n","dense_kernel_init_0: Orthogonal\n","has_leakyrelu_0: True\n","has_dropout_0: True\n","dense_dropout_value_0: 0.05\n","optimize_adam: False\n","learning_rate: 2.4485383549839913e-06\n","batch_size: 176\n","final_optimizer: Adamax\n","has_batchnormalization_1: True\n","dense_block_nunits_1: 512\n","dense_activation_1: relu\n","dense_kernel_init_1: GlorotUniform\n","has_leakyrelu_1: False\n","has_dropout_1: False\n","dense_dropout_value_1: 0.125\n","has_batchnormalization_2: True\n","dense_block_nunits_2: 256\n","dense_activation_2: gelu\n","dense_kernel_init_2: HeUniform\n","has_leakyrelu_2: False\n","has_dropout_2: True\n","dense_dropout_value_2: 0.625\n","has_batchnormalization_3: True\n","dense_block_nunits_3: 256\n","dense_activation_3: linear\n","dense_kernel_init_3: RandomNormal\n","has_leakyrelu_3: False\n","has_dropout_3: True\n","dense_dropout_value_3: 0.25\n","has_batchnormalization_4: True\n","dense_block_nunits_4: 128\n","dense_activation_4: relu\n","dense_kernel_init_4: LecunUniform\n","has_leakyrelu_4: True\n","has_dropout_4: True\n","dense_dropout_value_4: 0.17500000000000002\n","has_batchnormalization_5: True\n","dense_block_nunits_5: 128\n","dense_activation_5: elu\n","dense_kernel_init_5: GlorotNormal\n","has_leakyrelu_5: False\n","has_dropout_5: True\n","dense_dropout_value_5: 0.75\n","has_batchnormalization_6: False\n","dense_block_nunits_6: 32\n","dense_activation_6: tanh\n","dense_kernel_init_6: LecunUniform\n","has_leakyrelu_6: False\n","has_dropout_6: False\n","dense_dropout_value_6: 0.75\n","has_batchnormalization_7: False\n","dense_block_nunits_7: 16\n","dense_activation_7: tanh\n","dense_kernel_init_7: RandomNormal\n","has_leakyrelu_7: True\n","has_dropout_7: False\n","dense_dropout_value_7: 0.7000000000000001\n","has_batchnormalization_8: True\n","dense_block_nunits_8: 128\n","dense_activation_8: selu\n","dense_kernel_init_8: GlorotUniform\n","has_leakyrelu_8: True\n","has_dropout_8: True\n","dense_dropout_value_8: 0.1\n","has_batchnormalization_9: False\n","dense_block_nunits_9: 256\n","dense_activation_9: linear\n","dense_kernel_init_9: Orthogonal\n","has_leakyrelu_9: False\n","has_dropout_9: True\n","dense_dropout_value_9: 0.325\n","tuner/epochs: 4\n","tuner/initial_epoch: 0\n","tuner/bracket: 0\n","tuner/round: 0\n","Score: 0.32357245683670044\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 3\n","has_batchnormalization_0: False\n","dense_block_nunits_0: 16\n","dense_activation_0: linear\n","dense_kernel_init_0: LecunUniform\n","has_leakyrelu_0: False\n","has_dropout_0: False\n","dense_dropout_value_0: 0.35000000000000003\n","optimize_adam: False\n","learning_rate: 0.0002893937911498327\n","batch_size: 112\n","final_optimizer: Nadam\n","has_batchnormalization_1: True\n","dense_block_nunits_1: 16\n","dense_activation_1: tanh\n","dense_kernel_init_1: VarianceScaling\n","has_leakyrelu_1: True\n","has_dropout_1: True\n","dense_dropout_value_1: 0.225\n","has_batchnormalization_2: False\n","dense_block_nunits_2: 32\n","dense_activation_2: linear\n","dense_kernel_init_2: Orthogonal\n","has_leakyrelu_2: False\n","has_dropout_2: False\n","dense_dropout_value_2: 0.625\n","has_batchnormalization_3: True\n","dense_block_nunits_3: 16\n","dense_activation_3: tanh\n","dense_kernel_init_3: HeNormal\n","has_leakyrelu_3: False\n","has_dropout_3: False\n","dense_dropout_value_3: 0.42500000000000004\n","has_batchnormalization_4: False\n","dense_block_nunits_4: 16\n","dense_activation_4: elu\n","dense_kernel_init_4: VarianceScaling\n","has_leakyrelu_4: False\n","has_dropout_4: False\n","dense_dropout_value_4: 0.8250000000000001\n","has_batchnormalization_5: True\n","dense_block_nunits_5: 128\n","dense_activation_5: selu\n","dense_kernel_init_5: Orthogonal\n","has_leakyrelu_5: False\n","has_dropout_5: False\n","dense_dropout_value_5: 0.525\n","has_batchnormalization_6: True\n","dense_block_nunits_6: 32\n","dense_activation_6: gelu\n","dense_kernel_init_6: GlorotNormal\n","has_leakyrelu_6: False\n","has_dropout_6: True\n","dense_dropout_value_6: 0.6000000000000001\n","has_batchnormalization_7: True\n","dense_block_nunits_7: 32\n","dense_activation_7: elu\n","dense_kernel_init_7: VarianceScaling\n","has_leakyrelu_7: False\n","has_dropout_7: True\n","dense_dropout_value_7: 0.15000000000000002\n","has_batchnormalization_8: True\n","dense_block_nunits_8: 64\n","dense_activation_8: tanh\n","dense_kernel_init_8: Orthogonal\n","has_leakyrelu_8: False\n","has_dropout_8: True\n","dense_dropout_value_8: 0.5\n","has_batchnormalization_9: False\n","dense_block_nunits_9: 16\n","dense_activation_9: linear\n","dense_kernel_init_9: GlorotNormal\n","has_leakyrelu_9: True\n","has_dropout_9: True\n","dense_dropout_value_9: 0.9\n","tuner/epochs: 4\n","tuner/initial_epoch: 2\n","tuner/bracket: 1\n","tuner/round: 1\n","tuner/trial_id: 22be5f61d9f65bdaaea13d848e6aa501\n","Score: 0.6980619430541992\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 3\n","has_batchnormalization_0: False\n","dense_block_nunits_0: 16\n","dense_activation_0: linear\n","dense_kernel_init_0: LecunUniform\n","has_leakyrelu_0: False\n","has_dropout_0: False\n","dense_dropout_value_0: 0.35000000000000003\n","optimize_adam: False\n","learning_rate: 0.0002893937911498327\n","batch_size: 112\n","final_optimizer: Nadam\n","has_batchnormalization_1: True\n","dense_block_nunits_1: 16\n","dense_activation_1: tanh\n","dense_kernel_init_1: VarianceScaling\n","has_leakyrelu_1: True\n","has_dropout_1: True\n","dense_dropout_value_1: 0.225\n","has_batchnormalization_2: False\n","dense_block_nunits_2: 32\n","dense_activation_2: linear\n","dense_kernel_init_2: Orthogonal\n","has_leakyrelu_2: False\n","has_dropout_2: False\n","dense_dropout_value_2: 0.625\n","has_batchnormalization_3: True\n","dense_block_nunits_3: 16\n","dense_activation_3: tanh\n","dense_kernel_init_3: HeNormal\n","has_leakyrelu_3: False\n","has_dropout_3: False\n","dense_dropout_value_3: 0.42500000000000004\n","has_batchnormalization_4: False\n","dense_block_nunits_4: 16\n","dense_activation_4: elu\n","dense_kernel_init_4: VarianceScaling\n","has_leakyrelu_4: False\n","has_dropout_4: False\n","dense_dropout_value_4: 0.8250000000000001\n","has_batchnormalization_5: True\n","dense_block_nunits_5: 128\n","dense_activation_5: selu\n","dense_kernel_init_5: Orthogonal\n","has_leakyrelu_5: False\n","has_dropout_5: False\n","dense_dropout_value_5: 0.525\n","has_batchnormalization_6: True\n","dense_block_nunits_6: 32\n","dense_activation_6: gelu\n","dense_kernel_init_6: GlorotNormal\n","has_leakyrelu_6: False\n","has_dropout_6: True\n","dense_dropout_value_6: 0.6000000000000001\n","has_batchnormalization_7: True\n","dense_block_nunits_7: 32\n","dense_activation_7: elu\n","dense_kernel_init_7: VarianceScaling\n","has_leakyrelu_7: False\n","has_dropout_7: True\n","dense_dropout_value_7: 0.15000000000000002\n","has_batchnormalization_8: True\n","dense_block_nunits_8: 64\n","dense_activation_8: tanh\n","dense_kernel_init_8: Orthogonal\n","has_leakyrelu_8: False\n","has_dropout_8: True\n","dense_dropout_value_8: 0.5\n","has_batchnormalization_9: False\n","dense_block_nunits_9: 16\n","dense_activation_9: linear\n","dense_kernel_init_9: GlorotNormal\n","has_leakyrelu_9: True\n","has_dropout_9: True\n","dense_dropout_value_9: 0.9\n","tuner/epochs: 2\n","tuner/initial_epoch: 0\n","tuner/bracket: 1\n","tuner/round: 0\n","Score: 0.7129609137773514\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 8\n","has_batchnormalization_0: False\n","dense_block_nunits_0: 128\n","dense_activation_0: relu\n","dense_kernel_init_0: RandomNormal\n","has_leakyrelu_0: False\n","has_dropout_0: True\n","dense_dropout_value_0: 0.525\n","optimize_adam: True\n","learning_rate: 0.0006668683198856941\n","batch_size: 112\n","final_optimizer: RMSprop\n","has_batchnormalization_1: False\n","dense_block_nunits_1: 256\n","dense_activation_1: selu\n","dense_kernel_init_1: GlorotUniform\n","has_leakyrelu_1: False\n","has_dropout_1: False\n","dense_dropout_value_1: 0.025\n","has_batchnormalization_2: True\n","dense_block_nunits_2: 256\n","dense_activation_2: gelu\n","dense_kernel_init_2: RandomNormal\n","has_leakyrelu_2: True\n","has_dropout_2: True\n","dense_dropout_value_2: 0.025\n","has_batchnormalization_3: True\n","dense_block_nunits_3: 128\n","dense_activation_3: tanh\n","dense_kernel_init_3: HeNormal\n","has_leakyrelu_3: True\n","has_dropout_3: False\n","dense_dropout_value_3: 0.9500000000000001\n","has_batchnormalization_4: True\n","dense_block_nunits_4: 512\n","dense_activation_4: relu\n","dense_kernel_init_4: HeUniform\n","has_leakyrelu_4: True\n","has_dropout_4: True\n","dense_dropout_value_4: 0.30000000000000004\n","has_batchnormalization_5: False\n","dense_block_nunits_5: 32\n","dense_activation_5: tanh\n","dense_kernel_init_5: LecunNormal\n","has_leakyrelu_5: True\n","has_dropout_5: False\n","dense_dropout_value_5: 0.07500000000000001\n","has_batchnormalization_6: False\n","dense_block_nunits_6: 512\n","dense_activation_6: elu\n","dense_kernel_init_6: HeNormal\n","has_leakyrelu_6: True\n","has_dropout_6: False\n","dense_dropout_value_6: 0.125\n","has_batchnormalization_7: True\n","dense_block_nunits_7: 16\n","dense_activation_7: tanh\n","dense_kernel_init_7: LecunUniform\n","has_leakyrelu_7: True\n","has_dropout_7: True\n","dense_dropout_value_7: 0.525\n","has_batchnormalization_8: True\n","dense_block_nunits_8: 32\n","dense_activation_8: gelu\n","dense_kernel_init_8: HeUniform\n","has_leakyrelu_8: False\n","has_dropout_8: True\n","dense_dropout_value_8: 0.42500000000000004\n","tuner/epochs: 4\n","tuner/initial_epoch: 0\n","tuner/bracket: 0\n","tuner/round: 0\n","Score: 0.7192111313343048\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 2\n","has_batchnormalization_0: True\n","dense_block_nunits_0: 32\n","dense_activation_0: linear\n","dense_kernel_init_0: VarianceScaling\n","has_leakyrelu_0: True\n","has_dropout_0: True\n","dense_dropout_value_0: 0.30000000000000004\n","optimize_adam: False\n","learning_rate: 2.027751592235015e-06\n","batch_size: 112\n","final_optimizer: Nadam\n","has_batchnormalization_1: True\n","dense_block_nunits_1: 64\n","dense_activation_1: selu\n","dense_kernel_init_1: LecunNormal\n","has_leakyrelu_1: True\n","has_dropout_1: True\n","dense_dropout_value_1: 0.75\n","has_batchnormalization_2: False\n","dense_block_nunits_2: 512\n","dense_activation_2: gelu\n","dense_kernel_init_2: RandomNormal\n","has_leakyrelu_2: True\n","has_dropout_2: True\n","dense_dropout_value_2: 0.25\n","has_batchnormalization_3: False\n","dense_block_nunits_3: 32\n","dense_activation_3: gelu\n","dense_kernel_init_3: LecunNormal\n","has_leakyrelu_3: False\n","has_dropout_3: True\n","dense_dropout_value_3: 0.025\n","has_batchnormalization_4: False\n","dense_block_nunits_4: 32\n","dense_activation_4: gelu\n","dense_kernel_init_4: GlorotNormal\n","has_leakyrelu_4: False\n","has_dropout_4: False\n","dense_dropout_value_4: 0.625\n","has_batchnormalization_5: False\n","dense_block_nunits_5: 16\n","dense_activation_5: tanh\n","dense_kernel_init_5: Ones\n","has_leakyrelu_5: True\n","has_dropout_5: False\n","dense_dropout_value_5: 0.47500000000000003\n","has_batchnormalization_6: True\n","dense_block_nunits_6: 256\n","dense_activation_6: tanh\n","dense_kernel_init_6: VarianceScaling\n","has_leakyrelu_6: True\n","has_dropout_6: True\n","dense_dropout_value_6: 0.17500000000000002\n","has_batchnormalization_7: False\n","dense_block_nunits_7: 512\n","dense_activation_7: selu\n","dense_kernel_init_7: GlorotUniform\n","has_leakyrelu_7: True\n","has_dropout_7: False\n","dense_dropout_value_7: 0.30000000000000004\n","has_batchnormalization_8: False\n","dense_block_nunits_8: 32\n","dense_activation_8: elu\n","dense_kernel_init_8: HeUniform\n","has_leakyrelu_8: False\n","has_dropout_8: False\n","dense_dropout_value_8: 0.625\n","has_batchnormalization_9: False\n","dense_block_nunits_9: 16\n","dense_activation_9: tanh\n","dense_kernel_init_9: HeNormal\n","has_leakyrelu_9: False\n","has_dropout_9: True\n","dense_dropout_value_9: 0.55\n","tuner/epochs: 4\n","tuner/initial_epoch: 2\n","tuner/bracket: 1\n","tuner/round: 1\n","tuner/trial_id: 055850e205ae2e745a224eb1631c171a\n","Score: 0.7607351243495941\n","Trial summary\n","Hyperparameters:\n","dense_blocks_with_normalizations: 2\n","has_batchnormalization_0: True\n","dense_block_nunits_0: 32\n","dense_activation_0: linear\n","dense_kernel_init_0: VarianceScaling\n","has_leakyrelu_0: True\n","has_dropout_0: True\n","dense_dropout_value_0: 0.30000000000000004\n","optimize_adam: False\n","learning_rate: 2.027751592235015e-06\n","batch_size: 112\n","final_optimizer: Nadam\n","has_batchnormalization_1: True\n","dense_block_nunits_1: 64\n","dense_activation_1: selu\n","dense_kernel_init_1: LecunNormal\n","has_leakyrelu_1: True\n","has_dropout_1: True\n","dense_dropout_value_1: 0.75\n","has_batchnormalization_2: False\n","dense_block_nunits_2: 512\n","dense_activation_2: gelu\n","dense_kernel_init_2: RandomNormal\n","has_leakyrelu_2: True\n","has_dropout_2: True\n","dense_dropout_value_2: 0.25\n","has_batchnormalization_3: False\n","dense_block_nunits_3: 32\n","dense_activation_3: gelu\n","dense_kernel_init_3: LecunNormal\n","has_leakyrelu_3: False\n","has_dropout_3: True\n","dense_dropout_value_3: 0.025\n","has_batchnormalization_4: False\n","dense_block_nunits_4: 32\n","dense_activation_4: gelu\n","dense_kernel_init_4: GlorotNormal\n","has_leakyrelu_4: False\n","has_dropout_4: False\n","dense_dropout_value_4: 0.625\n","has_batchnormalization_5: False\n","dense_block_nunits_5: 16\n","dense_activation_5: tanh\n","dense_kernel_init_5: Ones\n","has_leakyrelu_5: True\n","has_dropout_5: False\n","dense_dropout_value_5: 0.47500000000000003\n","has_batchnormalization_6: True\n","dense_block_nunits_6: 256\n","dense_activation_6: tanh\n","dense_kernel_init_6: VarianceScaling\n","has_leakyrelu_6: True\n","has_dropout_6: True\n","dense_dropout_value_6: 0.17500000000000002\n","has_batchnormalization_7: False\n","dense_block_nunits_7: 512\n","dense_activation_7: selu\n","dense_kernel_init_7: GlorotUniform\n","has_leakyrelu_7: True\n","has_dropout_7: False\n","dense_dropout_value_7: 0.30000000000000004\n","has_batchnormalization_8: False\n","dense_block_nunits_8: 32\n","dense_activation_8: elu\n","dense_kernel_init_8: HeUniform\n","has_leakyrelu_8: False\n","has_dropout_8: False\n","dense_dropout_value_8: 0.625\n","has_batchnormalization_9: False\n","dense_block_nunits_9: 16\n","dense_activation_9: tanh\n","dense_kernel_init_9: HeNormal\n","has_leakyrelu_9: False\n","has_dropout_9: True\n","dense_dropout_value_9: 0.55\n","tuner/epochs: 2\n","tuner/initial_epoch: 0\n","tuner/bracket: 1\n","tuner/round: 0\n","Score: 0.775242954492569\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-pmEiMW6EPY","executionInfo":{"status":"ok","timestamp":1621268377171,"user_tz":-60,"elapsed":1264,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"be32dbf9-61f7-406b-9217-08b9f4a00438"},"source":["# Retrieve the best model.\n","tuner.get_best_models()\n","\n","best_model = tuner.get_best_models(num_models=1)[0]\n","best_model.save(\"hypermodel_merge_3lastlayers_m1.h5\")\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9B08Bbp6OmS","executionInfo":{"status":"ok","timestamp":1621268382367,"user_tz":-60,"elapsed":839,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"7af5b920-4068-4659-dc4e-71acd2d94ff7"},"source":["best_model.evaluate(data_test_ll_no2nd[0], data_test_ll_no2nd[1][0])  # 95.57% "],"execution_count":25,"outputs":[{"output_type":"stream","text":["27/27 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.9648\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.23474499583244324, 0.9650349617004395]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xfok3o8ZbxDH","executionInfo":{"status":"ok","timestamp":1621268432122,"user_tz":-60,"elapsed":19062,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"ee8768ab-bc8f-432f-ea82-558003202d6f"},"source":["best_models = tuner.get_best_models(num_models=30)\n","\n","for model in best_models:\n","  model.evaluate(data_test_ll_no2nd[0], data_test_ll_no2nd[1][0])\n","\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["27/27 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.9648\n","27/27 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9598\n","27/27 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9625\n","27/27 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9565\n","27/27 [==============================] - 0s 2ms/step - loss: 1.6775 - accuracy: 0.2159\n","27/27 [==============================] - 0s 2ms/step - loss: 0.7516 - accuracy: 0.9466\n","27/27 [==============================] - 0s 2ms/step - loss: 0.8175 - accuracy: 0.9511\n","27/27 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.9609\n","27/27 [==============================] - 0s 2ms/step - loss: 0.7869 - accuracy: 0.9632\n","27/27 [==============================] - 0s 2ms/step - loss: 0.7812 - accuracy: 0.9583\n","27/27 [==============================] - 0s 2ms/step - loss: 1.8962 - accuracy: 0.9412\n","27/27 [==============================] - 0s 2ms/step - loss: 2.0179 - accuracy: 0.9417\n","27/27 [==============================] - 0s 2ms/step - loss: 2.3929 - accuracy: 0.9546\n","27/27 [==============================] - 0s 2ms/step - loss: 2.3906 - accuracy: 0.9534\n","27/27 [==============================] - 0s 2ms/step - loss: 1.3083 - accuracy: 0.9633\n","27/27 [==============================] - 0s 2ms/step - loss: 1.5586 - accuracy: 0.4433\n","27/27 [==============================] - 0s 2ms/step - loss: 1.0881 - accuracy: 0.9611\n","27/27 [==============================] - 0s 2ms/step - loss: 1.6788 - accuracy: 0.8942\n","27/27 [==============================] - 0s 2ms/step - loss: 1.9277 - accuracy: 0.8344\n","27/27 [==============================] - 0s 2ms/step - loss: 2.0930 - accuracy: 0.7320\n","27/27 [==============================] - 0s 2ms/step - loss: 1.8928 - accuracy: 0.3839\n","27/27 [==============================] - 0s 2ms/step - loss: 1.7275 - accuracy: 0.5736\n","27/27 [==============================] - 0s 2ms/step - loss: 3.3875 - accuracy: 0.0762\n","27/27 [==============================] - 0s 2ms/step - loss: 1.3576 - accuracy: 0.9597\n","27/27 [==============================] - 0s 2ms/step - loss: 3.2587 - accuracy: 0.1007\n","27/27 [==============================] - 0s 1ms/step - loss: 1.3529 - accuracy: 0.8837\n","27/27 [==============================] - 0s 2ms/step - loss: 1.3350 - accuracy: 0.9133\n","27/27 [==============================] - 0s 2ms/step - loss: 1.9933 - accuracy: 0.1602\n","27/27 [==============================] - 0s 2ms/step - loss: 1.5232 - accuracy: 0.3919\n","27/27 [==============================] - 0s 2ms/step - loss: 2.3908 - accuracy: 0.2323\n"],"name":"stdout"}]}]}