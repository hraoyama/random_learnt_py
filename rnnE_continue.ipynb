{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnnE_continue.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1QrwzrCDpTgHK-GVmIsbasRMJ9SQo1pRB","authorship_tag":"ABX9TyPN0uaMAijfzorUpwY46XpH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKUNpW9_UrtC","executionInfo":{"status":"ok","timestamp":1630448858636,"user_tz":-60,"elapsed":8180,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"0e788851-dde4-4e70-e4c3-17ee718ad475"},"source":["%pip install keras-tuner\n","%pip install keras_self_attention"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-tuner\n","  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n","\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 31.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n","Collecting kt-legacy\n","  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.0.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n","Installing collected packages: kt-legacy, keras-tuner\n","Successfully installed keras-tuner-1.0.4 kt-legacy-1.0.4\n","Collecting keras_self_attention\n","  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.6.0)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=91edd5fb637cb34958d63cae1784362aa6758723925cd62043094f29c51dd5e9\n","  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.50.0\n"]}]},{"cell_type":"code","metadata":{"id":"OQjO1NMIkHiZ","executionInfo":{"status":"ok","timestamp":1630448861456,"user_tz":-60,"elapsed":2824,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["import site\n","import os\n","import tensorflow as tf\n","import pandas as pd\n","import h5py as h5\n","import matplotlib.pyplot as plt\n","import errno\n","import numpy as np\n","import itertools\n","import multiprocessing\n","import json\n","import datetime\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.keras.models import Model, Sequential, load_model\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n","from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.summary import create_file_writer\n","\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","\n","pd.set_option('display.width', 400)\n","pd.set_option('display.max_columns', 40)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vToSXGfMkc6w","executionInfo":{"status":"ok","timestamp":1630448861801,"user_tz":-60,"elapsed":349,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def coShuffled_vectors(X, Y):\n","    if tf.shape(X)[0] == tf.shape(Y)[0]:\n","        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n","        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n","        return (tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs))\n","    else:\n","        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n","\n","\n","def getNpArrayFromH5(hf_Data):\n","    X_train = hf_Data['Train_Data']  # Get train set\n","    X_train = np.array(X_train)\n","    Y_train = hf_Data['Label']  # Get train label\n","    Y_train = np.array(Y_train)\n","    return X_train, Y_train\n","\n","# data extraction\n","def getData(is500=True, shuffle=False, ise2e=False, include_secondary=False, validation_split=None, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Train_Data\" if ise2e else \"Fold_10_Train_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/{\"e2e_Test_Data\" if ise2e else \"Fold_10_Test_Data\"}_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    X_validation = Y_validation = None\n","    if validation_split is not None:\n","        # sklearn split shuffles anyway\n","        X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_split)\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eData(is500=True, shuffle=False, include_secondary=False, isColab=False):\n","    if not include_secondary:\n","        hf_Train = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","        hf_Test = h5.File(\n","            f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    else:\n","        hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_Secondary_Data_1136.h5', 'r')\n","        hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_Secondary_Data_1136.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Secondary_Data_1136.h5', 'r') if include_secondary else h5.File(\n","        f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_Data_{str(500) if is500 else str(1000)}.h5', 'r')\n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n","\n","\n","def getE2eDataJustSecondary(shuffle=False,isColab=False):\n","    hf_Train = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Train_just_Secondary_Data_1000.h5', 'r')\n","    hf_Test = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Test_just_Secondary_Data_1000.h5', 'r')\n","\n","    X_train, Y_train = getNpArrayFromH5(hf_Train)\n","    X_test, Y_test = getNpArrayFromH5(hf_Test)\n","    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n","    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n","\n","    if shuffle:\n","        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n","        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n","\n","    hf_Val = h5.File(f'./{\"data\" if not isColab else \"drive/MyDrive/data_papers/ncRNA\"}/e2e_Val_just_Secondary_Data_1000.h5', 'r')\n","    \n","    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n","    Y_validation = to_categorical(Y_validation, 13)  # Process the label of tain\n","\n","    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA4FUPc8ke1I","executionInfo":{"status":"ok","timestamp":1630448862717,"user_tz":-60,"elapsed":921,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def plot_history(history):\n","    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n","    for k, v in history.history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        else:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()\n","\n","def plot_history_df(history):\n","    acc_keys = [k for k in history.columns.values if k in ('accuracy', 'val_accuracy')]\n","    loss_keys = [k for k in history.columns.values if not k in acc_keys and not k in ['epoch']]\n","    for k, v in history.items():\n","        if k in acc_keys:\n","            plt.figure(1)\n","            plt.plot(v)\n","        if k in loss_keys:\n","            plt.figure(2)\n","            plt.plot(v)\n","    plt.figure(1)\n","    plt.title('Accuracy vs. epochs')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(acc_keys, loc='lower right')\n","    plt.figure(2)\n","    plt.title('Loss vs. epochs')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(loss_keys, loc='upper right')\n","    plt.show()   \n","\n","def get_layer_by_name(layers, name, return_first=True):\n","    matching_named_layers = [l for l in layers if l.name == name]\n","    if not matching_named_layers:\n","        return None\n","    return matching_named_layers[0] if return_first else matching_named_layers\n","\n","\n","def get_combined_features_from_models(\n","        to_combine,\n","        X_train_combined, Y_train_combined,\n","        X_validation_combined, Y_validation_combined,\n","        X_test_combined, Y_test_combined,\n","        reverse_one_hot=False,\n","        normalize_X_func=None):\n","    models = []\n","    models_dict = {}\n","    X_trains_out = []\n","    X_test_out = []\n","    X_validation_out = []\n","    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n","\n","    if reverse_one_hot:\n","        Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train_combined) + 1\n","        Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test_combined) + 1\n","        Y_validation_new = np.apply_along_axis(np.argmax, 1, Y_validation_combined) + 1\n","    else:\n","        Y_train_new = Y_train_combined.copy()\n","        Y_test_new = Y_test_combined.copy()\n","        Y_validation_new = np.apply_along_axis(np.argmax, 1, Y_validation_combined) + 1\n","\n","    for model_file_name, layer_name, kwargs in to_combine:\n","        model_here = None\n","        if isinstance(model_file_name, tf.keras.models.Model):\n","            model_here = model_file_name\n","            model_file_name = model_here.name\n","        else:\n","            if model_file_name in models_dict.keys():\n","                model_here = models_dict[model_file_name]\n","            else:\n","                model_here = tf.keras.models.load_model(model_file_name,\n","                                                        **kwargs) if kwargs is not None else tf.keras.models.load_model \\\n","                    (model_file_name)\n","\n","        features_model = Model(model_here.input,\n","                               get_layer_by_name(model_here.layers, layer_name).output)\n","        if normalize_X_func is None:\n","            X_trains_out.append(np.array(features_model.predict(X_train_combined), dtype='float64'))\n","            X_test_out.append(np.array(features_model.predict(X_test_combined), dtype='float64'))\n","            X_validation_out.append(np.array(features_model.predict(X_validation_combined), dtype='float64'))\n","        else:\n","            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train_combined)), dtype='float64'))\n","            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test_combined)), dtype='float64'))\n","            X_validation_out.append(np.array(normalize_X_func(features_model.predict(X_validation_combined)), dtype='float64'))\n","            \n","        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n","        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n","        \n","        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n","        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n","        \n","        XY_dict[model_file_name][layer_name]['Validation']['X'] = X_validation_out[-1]\n","        XY_dict[model_file_name][layer_name]['Validation']['Y'] = Y_validation_new\n","        \n","        \n","        models.append(((model_file_name, layer_name), (model_here, features_model)))\n","        models_dict[model_file_name] = model_here\n","\n","    X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n","    X_test_new = np.concatenate(tuple(X_test_out), axis=1)\n","    X_validation_new = np.concatenate(tuple(X_validation_out), axis=1)\n","\n","\n","    data_train = (X_train_new, Y_train_new)\n","    data_test = (X_test_new, Y_test_new)\n","    data_validation = (X_validation_new, Y_validation_new)\n","\n","    return models, data_train, data_validation, data_test, XY_dict\n","\n","\n","\n","def make_dir_if_not_exist(used_path):\n","    if not os.path.isdir(used_path):\n","        try:\n","            os.mkdir(used_path)\n","        except OSError as exc:\n","            if exc.errno != errno.EEXIST:\n","                raise exc\n","            else:\n","                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n","\n","\n","def source_model(model_func, model_name, input_shape):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    return m\n","\n","\n","def compile_and_fit_model_with_tb(model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_every_epoch=True,\n","                                  save_final=False,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","    tb_callback = TensorBoard(log_dir=f'{m.name}_logs', histogram_freq=kwargs.pop(\"histogram_freq\", 1))\n","    if save_every_epoch:\n","        tb_callback.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=[tb_callback], verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n","    # m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n","\n","\n","def compile_model_and_fit_with_custom_loop(model_func,\n","                                           model_name,\n","                                           input_shape,\n","                                           X_train,\n","                                           Y_train,\n","                                           **kwargs):\n","    make_dir_if_not_exist(model_name)\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    train_writer = create_file_writer(f'{m.name}_logs/train/')\n","    test_writer = create_file_writer(f'{m.name}_logs/test/')\n","    train_step = test_step = 0\n","\n","    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n","    optimizer = tf.keras.optimizers.Adam()\n","    num_epochs = kwargs.get(\"epochs\", 10)\n","\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    BATCH_SIZE = kwargs.get(\"batch_size\", 32)\n","    X_test, Y_test = kwargs.get(\"validation_data\", (None, None))\n","    if X_test is None:\n","        raise ValueError(\"Missing X validation data\")\n","    if Y_test is None:\n","        raise ValueError(\"Missing Y validation data\")\n","\n","    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n","    train_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    train_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n","    test_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n","    test_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n","\n","    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n","\n","    for epoch in range(num_epochs):\n","        # Iterate through training set\n","        for batch_idx, (x, y) in enumerate(train_dataset_tf):\n","            with tf.GradientTape() as tape:\n","                y_pred = m(x, training=True)\n","                loss = loss_fn(y, y_pred)\n","\n","            gradients = tape.gradient(loss, m.trainable_weights)\n","            optimizer.apply_gradients(zip(gradients, m.trainable_weights))\n","            acc_metric.update_state(y, y_pred)\n","\n","            with train_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=train_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=train_step,\n","                )\n","                train_step += 1\n","        # Reset accuracy in between epochs (and for testing and test)\n","        acc_metric.reset_states()\n","        # Iterate through test set\n","        for batch_idx, (x, y) in enumerate(test_dataset_tf):\n","            y_pred = m(x, training=False)\n","            loss = loss_fn(y, y_pred)\n","            acc_metric.update_state(y, y_pred)\n","            with test_writer.as_default():\n","                tf.summary.scalar(\"Loss\", loss, step=test_step)\n","                tf.summary.scalar(\n","                    \"Accuracy\", acc_metric.result(), step=test_step,\n","                )\n","                test_step += 1\n","\n","        acc_metric.reset_states()  # Reset accuracy in between epochs (and for testing and test)\n","\n","    return m\n","\n","\n","def reinitialize_weights(model):\n","    for ix, layer in enumerate(model.layers):\n","        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n","            weight_initializer = model.layers[ix].kernel_initializer\n","            bias_initializer = model.layers[ix].bias_initializer\n","\n","            old_weights, old_biases = model.layers[ix].get_weights()\n","\n","            model.layers[ix].set_weights([\n","                weight_initializer(shape=old_weights.shape),\n","                bias_initializer(shape=len(old_biases))])\n","    return model\n","\n","\n","def reverse_tensor(X):\n","    return tf.gather(X, tf.reverse(tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32), (0,)))\n","\n","\n","def run_mirrored_strategy(model_func, base_batch_size, nepochs, x_train, y_train, x_test, y_test, **kwargs):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        model = model_func()\n","        model.compile(\n","            optimizer=tf.keras.optimizers.Adam(),\n","            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","            metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n","        )\n","    batch_size_mirr_strat = base_batch_size * strategy.num_replicas_in_sync\n","    history = model.fit(x_train, y_train, epochs=nepochs, batch_size=batch_size_mirr_strat,\n","                        validation_data=(x_test, y_test),\n","                        **kwargs)\n","    return model, history\n","\n","\n","def sparse_setdiff(a1, a2):\n","    a1a = a1.reshape(a1.shape[0], -1)\n","    a2a = a2.reshape(a2.shape[0], -1)\n","    spa2a = [np.where(x)[0].tolist() for x in a2a]\n","    spa1a = [np.where(x)[0].tolist() for x in a1a]\n","    idxs_to_keep = []\n","    for idx, sample in enumerate(spa1a):\n","        try:\n","            spa2a.index(sample)\n","        except ValueError:\n","            # not in list\n","            idxs_to_keep.append(idx)\n","    return a1[idxs_to_keep], idxs_to_keep\n","\n","\n","def unpacking_apply_along_axis(all_args):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but with arguments in a tuple\n","    instead.\n","\n","    This function is useful with multiprocessing.Pool().map(): (1)\n","    map() only handles functions that take a single argument, and (2)\n","    this function can generally be imported from a module, as required\n","    by map().\n","    \"\"\"\n","    (func1d, axis, arr, args, kwargs) = all_args\n","    # return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n","\n","\n","def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n","    \"\"\"\n","    Like numpy.apply_along_axis(), but takes advantage of multiple\n","    cores.\n","    \"\"\"\n","    # Effective axis where apply_along_axis() will be applied by each\n","    # worker (any non-zero axis number would work, so as to allow the use\n","    # of `np.array_split()`, which is only done on axis 0):\n","    effective_axis = 1 if axis == 0 else axis\n","    if effective_axis != axis:\n","        arr = arr.swapaxes(axis, effective_axis)\n","\n","    # Chunks for the mapping (only a few chunks):\n","    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n","              for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n","\n","    pool = multiprocessing.Pool()\n","    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n","    # Freeing the workers:\n","    pool.close()\n","    pool.join()\n","\n","    return np.concatenate(individual_results)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsPksEpvkwx3","executionInfo":{"status":"ok","timestamp":1630448867350,"user_tz":-60,"elapsed":150,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def model_A_CNN_256(model_name, inshape, num_classes = 13):\n","\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same' ,input_shape=inshape))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(2))\n","\n","    # check these 2\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Conv1D(256 ,10 ,padding='same'))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","    model.add(tf.keras.layers.MaxPooling1D(4))\n","\n","    model.add(tf.keras.layers.GaussianNoise(1))\n","    model.add(tf.keras.layers.Dropout(rate=0.5))\n","\n","    model.add(tf.keras.layers.Flatten())\n","\n","    model.add(tf.keras.layers.Dense(128))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(64))\n","    model.add(tf.keras.layers.BatchNormalization())\n","    model.add(tf.keras.layers.LeakyReLU(alpha=0.5))\n","\n","    model.add(tf.keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n","    model._name = model_name\n","\n","    return model\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"BA-Gd8l6kxWn","executionInfo":{"status":"ok","timestamp":1630448869892,"user_tz":-60,"elapsed":160,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def model_E_RNN(model_name, input_shape = (1000, 8,),num_classes = 13):\n","\n","    # RNN part\n","    inputs = Input(shape=input_shape)\n","    lstm_one = Bidirectional \\\n","        (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(inputs)\n","    lstm_two = Bidirectional \\\n","        (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(lstm_one)\n","    attention = SeqWeightedAttention()(lstm_two)\n","    attention = Flatten()(attention)\n","    rnnoutput = Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(attention)\n","    rnnoutput = BatchNormalization()(rnnoutput)\n","    rnnoutput = GaussianNoise(1)(rnnoutput)\n","    rnnoutput = Dropout(0.4)(rnnoutput)\n","\n","    # Dense Feed-forward\n","    dense_one = Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros')(rnnoutput)\n","    dense_one = LeakyReLU()(dense_one)\n","    dense_one = Dropout(0.5)(dense_one)\n","    dense_one = BatchNormalization()(dense_one)\n","    dense_two = Dense(64, kernel_initializer='RandomNormal', bias_initializer='zeros')(dense_one)\n","    dense_two = LeakyReLU()(dense_two)\n","    dense_two = Dropout(0.4)(dense_two)\n","\n","    # Output\n","    output = Dense(num_classes, activation='softmax')(dense_two)\n","    model = Model([inputs], output, name = model_name)\n","    return model\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YulEt03kzS_","executionInfo":{"status":"ok","timestamp":1630448871601,"user_tz":-60,"elapsed":230,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def model_combination(model_name, input_shape,num_classes = 13):\n","    model = Sequential([\n","        tf.keras.Input(shape=input_shape),\n","        BatchNormalization(),\n","        Dense(256, kernel_initializer='RandomNormal', bias_initializer='zeros'),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(128, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-3)),\n","        LeakyReLU(),\n","        Dropout(0.6),\n","        Dense(32, kernel_initializer='RandomNormal', bias_initializer='zeros', kernel_regularizer = tf.keras.regularizers.l1(1e-2)),\n","        LeakyReLU(),\n","        Dropout(0.5),\n","        Dense(num_classes, activation='softmax')\n","    ], name=model_name)\n","    return model"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IdtyDzDlB8u","executionInfo":{"status":"ok","timestamp":1630448877181,"user_tz":-60,"elapsed":190,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def merged_model_A_E(model_name, input_shape, num_classes = 13):\n","  base_input = Input(shape=input_shape, name='base_input')\n","\n","  cnn_convblock_1 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(base_input))))\n","  cnn_noiseblock_1 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_1))\n","\n","  cnn_convblock_2 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_1))))\n","  cnn_noiseblock_2 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_2))\n","\n","  cnn_convblock_3 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_2))))\n","  cnn_noiseblock_3 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_3))\n","\n","  cnn_convblock_4 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_3))))\n","  cnn_noiseblock_4 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_4))\n","\n","  cnn_convblock_5 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_4))))\n","  cnn_noiseblock_5 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_5))\n","\n","  cnn_flatten = Flatten()(cnn_noiseblock_5)\n","\n","  cnn_denseblock_1  = LeakyReLU(alpha=0.5)(BatchNormalization()(Dense(128)(cnn_flatten)))\n","  cnn_denseblock_2  = LeakyReLU(alpha=0.5)(BatchNormalization()(Dense(64)(cnn_denseblock_1)))\n","\n","  rnn_lstm_one = Bidirectional \\\n","      (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(base_input)\n","  rnn_lstm_two = Bidirectional \\\n","      (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(rnn_lstm_one)\n","  \n","  rnn_attention = Flatten()(SeqWeightedAttention()(rnn_lstm_two))\n","\n","  rnn_denseblock_1 = BatchNormalization()(Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_attention))\n","  rnn_noiseblock_1 = Dropout(0.4)(GaussianNoise(1)(rnn_denseblock_1))\n","\n","  rnn_denseblock_2 = LeakyReLU()(Dense(128 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_1))\n","  rnn_noiseblock_2 = BatchNormalization()(Dropout(0.5)(rnn_denseblock_2))\n","\n","  rnn_denseblock_3 = LeakyReLU()(Dense(64 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_2))\n","  rnn_noiseblock_3 = Dropout(0.4)(rnn_denseblock_3)\n","\n","  merged_layer = tf.keras.layers.concatenate([cnn_denseblock_2,rnn_noiseblock_3])\n","\n","  # Output\n","  output = Dense(num_classes, activation='softmax')(merged_layer)\n","  model = Model(base_input, output, name = model_name)\n","\n","  return model\n","\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"FizqwASsouJD","executionInfo":{"status":"ok","timestamp":1630448874535,"user_tz":-60,"elapsed":160,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def sequential_model_A_E(model_name, input_shape, num_classes = 13):\n","  \n","  base_input = Input(shape=input_shape, name='base_input')\n","\n","  cnn_convblock_1 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(base_input))))\n","  cnn_noiseblock_1 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_1))\n","\n","  cnn_convblock_2 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_1))))\n","  cnn_noiseblock_2 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_2))\n","\n","  cnn_convblock_3 = MaxPooling1D(2)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_2))))\n","  cnn_noiseblock_3 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_3))\n","\n","  cnn_convblock_4 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_3))))\n","  cnn_noiseblock_4 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_4))\n","\n","  cnn_convblock_5 = MaxPooling1D(4)(LeakyReLU(alpha=0.5)(BatchNormalization()(Conv1D(256 ,10 ,padding='same')(cnn_noiseblock_4))))\n","  cnn_noiseblock_5 = Dropout(rate=0.5)(GaussianNoise(1)(cnn_convblock_5))\n","\n","  rnn_lstm_one = Bidirectional \\\n","      (GRU(256, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(cnn_noiseblock_5)\n","  rnn_lstm_two = Bidirectional \\\n","      (GRU(128, return_sequences=True, kernel_initializer='RandomNormal', dropout= 0.5, recurrent_dropout = 0.5, recurrent_initializer='RandomNormal', bias_initializer='zero'))(rnn_lstm_one)\n","  \n","  rnn_attention = Flatten()(SeqWeightedAttention()(rnn_lstm_two))\n","\n","  rnn_denseblock_1 = BatchNormalization()(Dense(256 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_attention))\n","  rnn_noiseblock_1 = Dropout(0.4)(GaussianNoise(1)(rnn_denseblock_1))\n","\n","  rnn_denseblock_2 = LeakyReLU()(Dense(128 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_1))\n","  rnn_noiseblock_2 = BatchNormalization()(Dropout(0.5)(rnn_denseblock_2))\n","\n","  rnn_denseblock_3 = LeakyReLU()(Dense(64 ,kernel_initializer='RandomNormal', bias_initializer='zeros')(rnn_noiseblock_2))\n","  rnn_noiseblock_3 = Dropout(0.4)(rnn_denseblock_3)\n","\n","  # merged_layer = tf.keras.layers.concatenate([cnn_denseblock_2,rnn_noiseblock_3])\n","\n","  # Output\n","  output = Dense(num_classes, activation='softmax')(rnn_noiseblock_3)\n","  model = Model(base_input, output, name = model_name)\n","\n","  return model\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRSlxXgfk3Qu","executionInfo":{"status":"ok","timestamp":1630448888190,"user_tz":-60,"elapsed":161,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["from tensorflow.keras.callbacks import CSVLogger\n","import datetime\n","\n","def compile_and_fit_model_basic(  model_func,\n","                                  model_name,\n","                                  input_shape,\n","                                  X_train,\n","                                  Y_train,\n","                                  save_max_epoch=True,\n","                                  save_final=False,\n","                                  patience_count = None,\n","                                  early_stopping_obs = 'accuracy',\n","                                  log_history = True,\n","                                  **kwargs):\n","    m = None\n","    if isinstance(model_func, tf.keras.models.Model):\n","        m = model_func\n","        m._name = model_name\n","    else:\n","        m = model_func(model_name, input_shape)\n","\n","    callbacks_used = []\n","    if save_max_epoch:\n","        callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                              save_weights_only=False,\n","                                              monitor = early_stopping_obs, # 'accuracy', # 'val_accuracy'\n","                                              mode='max',\n","                                              save_best_only=True))\n","    if patience_count is not None:\n","        callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=patience_count))\n","\n","    if log_history:\n","        callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{model_name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n","\n","    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=2, **kwargs)\n","    if save_final:\n","        make_dir_if_not_exist(model_name)\n","        m.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}_saved_model_after_fit\")  # Save the model\n","    return (m, history)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"o68Af33BDKyg","executionInfo":{"status":"ok","timestamp":1630448891201,"user_tz":-60,"elapsed":164,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["def compile_and_fit_model_collaborative(  \n","        m1, m1_layer_name,\n","        m2, m2_layer_name,\n","        input_shape,\n","        X_train,\n","        Y_train,\n","        X_validation,\n","        Y_validation,\n","        X_test,\n","        Y_test,\n","        save_max_epoch=True,\n","        save_final=False,\n","        patience_count = None,\n","        log_history = True,\n","        **kwargs):\n","    \n","    to_combine_ld_no2nd_rNo2nd = [\n","        (m1 , m1_layer_name, None),   # change to the appropriate dense layer name\n","        (m2, m2_layer_name, None) # change to the appropriate dense layer name\n","    ]\n","    \n","    # models, data_train, data_validation, data_test, XY_dict\n","    combined_models_ld, data_train_ld, data_validation_ld, data_test_ld, data_access_ld = get_combined_features_from_models(\n","        to_combine_ld_no2nd_rNo2nd ,\n","        X_train,\n","        Y_train,\n","        X_validation,\n","        Y_validation,\n","        X_test, \n","        Y_test, \n","        reverse_one_hot=False)\n","        \n","    rcnn_combine_model = model_combination(f\"collaborative_model_{m1.name}_{m2.name}_{datetime.datetime.now():%Y%m%d%H%M%S}\", \n","                                           data_train_ld[0][0].shape  )\n","    rcnn_combine_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n","    if patience_count is None:\n","        patience_count  = 10000    \n","    callbacks_used_rcnn_combine = [\n","                        ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{rcnn_combine_model.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                                save_weights_only=False,\n","                                                monitor='accuracy',\n","                                                mode='max',\n","                                                save_best_only=True),\n","                        tf.keras.callbacks.EarlyStopping(patience=patience_count),\n","                        tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{rcnn_combine_model.name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True)\n","                        ]\n","    \n","    history_rcnn_combine = rcnn_combine_model.fit(data_train_ld[0], \n","                                                  data_train_ld[1], \n","                                                  validation_data=(data_validation_ld[0],to_categorical(data_validation_ld[1]-1)),\n","                                                  callbacks=callbacks_used_rcnn_combine, \n","                                                  verbose=2, \n","                                                  **kwargs)\n","    rcnn_combine_model.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{rcnn_combine_model.name}.h5\")    \n","    # rcnn_combine_model.evaluate(data_test_ld_no2nd_rNo2nd[0],data_test_ld_no2nd_rNo2nd[1][0]) \n","    \n","    return (rcnn_combine_model, history_rcnn_combine, data_access_ld )"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtDSFGTdG-b0","executionInfo":{"status":"ok","timestamp":1630448900977,"user_tz":-60,"elapsed":5831,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["# 'new' data \n","X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData(is500=False,\n","                                                                                                    include_secondary=False,\n","                                                                                                    isColab=True)\n","X_train_1000e_w2nd, Y_train_1000e_w2nd, X_test_1000e_w2nd, Y_test_1000e_w2nd, X_val_1000e_w2nd, Y_val_1000e_w2nd = getE2eData(is500=False, include_secondary=True, isColab=True)\n","X_train_1000e_j2nd, Y_train_1000e_j2nd, X_test_1000e_j2nd, Y_test_1000e_j2nd, X_val_1000e_j2nd, Y_val_1000e_j2nd = getE2eDataJustSecondary(isColab=True)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoMBMjumG_DU","executionInfo":{"status":"ok","timestamp":1630448901251,"user_tz":-60,"elapsed":275,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}}},"source":["# merge into a new train:\n","X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n","Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )    \n","\n","X_new_train_j2nd = np.concatenate( (X_train_1000e_j2nd, X_val_1000e_j2nd), axis=0 )\n","Y_new_train_j2nd = np.concatenate( (Y_train_1000e_j2nd, Y_val_1000e_j2nd), axis=0 )    \n","\n","X_new_train_w2nd = np.concatenate( (X_train_1000e_w2nd, X_val_1000e_w2nd), axis=0 )\n","Y_new_train_w2nd = np.concatenate( (Y_train_1000e_w2nd, Y_val_1000e_w2nd), axis=0 )  "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uqJKLR30s-k","executionInfo":{"status":"ok","timestamp":1630448901251,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hans Roggeman","photoUrl":"","userId":"10574434403170915342"}},"outputId":"7d4d167a-e52a-4565-a147-a82bd765d720"},"source":["print(X_new_train.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["(7715, 1000, 8)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCpaXFdLVA3o","outputId":"c44dee22-1871-43bb-8336-8586520d50a8"},"source":["# # Continue model E run from before...\n","rnn_E = load_model(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_119_0.979\")\n","m = rnn_E\n","patience_count=60\n","callbacks_used = []\n","callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n","                                      save_weights_only=False,\n","                                      monitor = 'accuracy', # 'val_accuracy'\n","                                      mode='max',\n","                                      save_best_only=True))\n","callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=patience_count))\n","callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/ncRNA/data/history/history_log_{m.name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n","# check if continues from before..\n","\n","m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","history_rnn_E = m.fit(X_train_1000e, Y_train_1000e, callbacks=callbacks_used, verbose=2,                                                   \n","                      batch_size=128,\n","                      epochs=500-118-6-119,\n","                      class_weight=None,\n","                      validation_data=(X_val_1000e, Y_val_1000e))\n","m.save(f\"/content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/{m.name}_saved_model_after_fit\")  # Save the model"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/257\n","54/54 - 786s - loss: 0.0862 - accuracy: 0.9736 - val_loss: 0.4591 - val_accuracy: 0.9032\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_001_0.974/assets\n","Epoch 2/257\n","54/54 - 768s - loss: 0.0854 - accuracy: 0.9752 - val_loss: 0.4057 - val_accuracy: 0.9148\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_002_0.975/assets\n","Epoch 3/257\n","54/54 - 742s - loss: 0.0934 - accuracy: 0.9720 - val_loss: 0.5057 - val_accuracy: 0.9043\n","Epoch 4/257\n","54/54 - 758s - loss: 0.0867 - accuracy: 0.9711 - val_loss: 0.3987 - val_accuracy: 0.9265\n","Epoch 5/257\n","54/54 - 720s - loss: 0.0892 - accuracy: 0.9726 - val_loss: 0.4499 - val_accuracy: 0.9137\n","Epoch 6/257\n","54/54 - 751s - loss: 0.0795 - accuracy: 0.9742 - val_loss: 0.2980 - val_accuracy: 0.9347\n","Epoch 7/257\n","54/54 - 773s - loss: 0.0895 - accuracy: 0.9717 - val_loss: 0.4820 - val_accuracy: 0.9067\n","Epoch 8/257\n","54/54 - 784s - loss: 0.0751 - accuracy: 0.9748 - val_loss: 0.3893 - val_accuracy: 0.9277\n","Epoch 9/257\n","54/54 - 751s - loss: 0.0692 - accuracy: 0.9786 - val_loss: 0.3199 - val_accuracy: 0.9370\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_009_0.979/assets\n","Epoch 10/257\n","54/54 - 760s - loss: 0.0730 - accuracy: 0.9740 - val_loss: 0.4566 - val_accuracy: 0.9137\n","Epoch 11/257\n","54/54 - 761s - loss: 0.0869 - accuracy: 0.9732 - val_loss: 0.3406 - val_accuracy: 0.9300\n","Epoch 12/257\n","54/54 - 715s - loss: 0.0760 - accuracy: 0.9754 - val_loss: 0.3501 - val_accuracy: 0.9172\n","Epoch 13/257\n","54/54 - 701s - loss: 0.0789 - accuracy: 0.9758 - val_loss: 0.4114 - val_accuracy: 0.9160\n","Epoch 14/257\n","54/54 - 714s - loss: 0.0789 - accuracy: 0.9742 - val_loss: 0.3762 - val_accuracy: 0.9172\n","Epoch 15/257\n","54/54 - 691s - loss: 0.0779 - accuracy: 0.9762 - val_loss: 0.4068 - val_accuracy: 0.9195\n","Epoch 16/257\n","54/54 - 743s - loss: 0.0853 - accuracy: 0.9745 - val_loss: 0.3808 - val_accuracy: 0.9277\n","Epoch 17/257\n","54/54 - 753s - loss: 0.0775 - accuracy: 0.9724 - val_loss: 0.3368 - val_accuracy: 0.9312\n","Epoch 18/257\n","54/54 - 742s - loss: 0.0813 - accuracy: 0.9754 - val_loss: 0.3675 - val_accuracy: 0.9218\n","Epoch 19/257\n","54/54 - 713s - loss: 0.0754 - accuracy: 0.9759 - val_loss: 0.5696 - val_accuracy: 0.8880\n","Epoch 20/257\n","54/54 - 680s - loss: 0.0844 - accuracy: 0.9736 - val_loss: 0.2688 - val_accuracy: 0.9440\n","Epoch 21/257\n","54/54 - 678s - loss: 0.0822 - accuracy: 0.9736 - val_loss: 0.2585 - val_accuracy: 0.9393\n","Epoch 22/257\n","54/54 - 678s - loss: 0.0695 - accuracy: 0.9784 - val_loss: 0.3929 - val_accuracy: 0.9265\n","Epoch 23/257\n","54/54 - 669s - loss: 0.0656 - accuracy: 0.9803 - val_loss: 0.3852 - val_accuracy: 0.9195\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_023_0.980/assets\n","Epoch 24/257\n","54/54 - 670s - loss: 0.0750 - accuracy: 0.9768 - val_loss: 0.5684 - val_accuracy: 0.8961\n","Epoch 25/257\n","54/54 - 669s - loss: 0.0793 - accuracy: 0.9752 - val_loss: 0.4438 - val_accuracy: 0.9078\n","Epoch 26/257\n","54/54 - 673s - loss: 0.0703 - accuracy: 0.9783 - val_loss: 0.3790 - val_accuracy: 0.9113\n","Epoch 27/257\n","54/54 - 674s - loss: 0.0714 - accuracy: 0.9755 - val_loss: 0.2804 - val_accuracy: 0.9323\n","Epoch 28/257\n","54/54 - 670s - loss: 0.0701 - accuracy: 0.9774 - val_loss: 0.4006 - val_accuracy: 0.9160\n","Epoch 29/257\n","54/54 - 673s - loss: 0.0668 - accuracy: 0.9775 - val_loss: 0.4556 - val_accuracy: 0.9160\n","Epoch 30/257\n","54/54 - 673s - loss: 0.0780 - accuracy: 0.9762 - val_loss: 0.3394 - val_accuracy: 0.9347\n","Epoch 31/257\n","54/54 - 662s - loss: 0.0734 - accuracy: 0.9762 - val_loss: 0.3951 - val_accuracy: 0.9253\n","Epoch 32/257\n","54/54 - 669s - loss: 0.0775 - accuracy: 0.9754 - val_loss: 0.2807 - val_accuracy: 0.9347\n","Epoch 33/257\n","54/54 - 660s - loss: 0.0887 - accuracy: 0.9724 - val_loss: 0.4324 - val_accuracy: 0.9183\n","Epoch 34/257\n","54/54 - 667s - loss: 0.0700 - accuracy: 0.9791 - val_loss: 0.4180 - val_accuracy: 0.9113\n","Epoch 35/257\n","54/54 - 666s - loss: 0.0653 - accuracy: 0.9813 - val_loss: 0.3822 - val_accuracy: 0.9183\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_035_0.981/assets\n","Epoch 36/257\n","54/54 - 667s - loss: 0.0716 - accuracy: 0.9781 - val_loss: 0.4700 - val_accuracy: 0.9125\n","Epoch 37/257\n","54/54 - 657s - loss: 0.0717 - accuracy: 0.9789 - val_loss: 0.3406 - val_accuracy: 0.9265\n","Epoch 38/257\n","54/54 - 666s - loss: 0.0711 - accuracy: 0.9749 - val_loss: 0.3419 - val_accuracy: 0.9242\n","Epoch 39/257\n","54/54 - 673s - loss: 0.0742 - accuracy: 0.9778 - val_loss: 0.2746 - val_accuracy: 0.9417\n","Epoch 40/257\n","54/54 - 676s - loss: 0.0763 - accuracy: 0.9764 - val_loss: 0.3527 - val_accuracy: 0.9265\n","Epoch 41/257\n","54/54 - 662s - loss: 0.0625 - accuracy: 0.9796 - val_loss: 0.3634 - val_accuracy: 0.9207\n","Epoch 42/257\n","54/54 - 662s - loss: 0.0705 - accuracy: 0.9765 - val_loss: 0.4069 - val_accuracy: 0.9207\n","Epoch 43/257\n","54/54 - 660s - loss: 0.0700 - accuracy: 0.9799 - val_loss: 0.3615 - val_accuracy: 0.9265\n","Epoch 44/257\n","54/54 - 663s - loss: 0.0606 - accuracy: 0.9806 - val_loss: 0.3505 - val_accuracy: 0.9288\n","Epoch 45/257\n","54/54 - 666s - loss: 0.0715 - accuracy: 0.9789 - val_loss: 0.3815 - val_accuracy: 0.9207\n","Epoch 46/257\n","54/54 - 659s - loss: 0.0655 - accuracy: 0.9778 - val_loss: 0.3142 - val_accuracy: 0.9312\n","Epoch 47/257\n","54/54 - 659s - loss: 0.0586 - accuracy: 0.9802 - val_loss: 0.4104 - val_accuracy: 0.9218\n","Epoch 48/257\n","54/54 - 662s - loss: 0.0686 - accuracy: 0.9778 - val_loss: 0.3496 - val_accuracy: 0.9288\n","Epoch 49/257\n","54/54 - 659s - loss: 0.0631 - accuracy: 0.9796 - val_loss: 0.3407 - val_accuracy: 0.9393\n","Epoch 50/257\n","54/54 - 672s - loss: 0.0665 - accuracy: 0.9796 - val_loss: 0.3250 - val_accuracy: 0.9277\n","Epoch 51/257\n","54/54 - 688s - loss: 0.0629 - accuracy: 0.9805 - val_loss: 0.3437 - val_accuracy: 0.9288\n","Epoch 52/257\n","54/54 - 689s - loss: 0.0678 - accuracy: 0.9774 - val_loss: 0.3265 - val_accuracy: 0.9323\n","Epoch 53/257\n","54/54 - 677s - loss: 0.0590 - accuracy: 0.9805 - val_loss: 0.4618 - val_accuracy: 0.9090\n","Epoch 54/257\n","54/54 - 666s - loss: 0.0736 - accuracy: 0.9756 - val_loss: 0.3657 - val_accuracy: 0.9253\n","Epoch 55/257\n","54/54 - 666s - loss: 0.0643 - accuracy: 0.9809 - val_loss: 0.3399 - val_accuracy: 0.9347\n","Epoch 56/257\n","54/54 - 737s - loss: 0.0643 - accuracy: 0.9787 - val_loss: 0.3594 - val_accuracy: 0.9300\n","Epoch 57/257\n","54/54 - 663s - loss: 0.0626 - accuracy: 0.9805 - val_loss: 0.3756 - val_accuracy: 0.9265\n","Epoch 58/257\n","54/54 - 679s - loss: 0.0630 - accuracy: 0.9784 - val_loss: 0.3839 - val_accuracy: 0.9265\n","Epoch 59/257\n","54/54 - 677s - loss: 0.0626 - accuracy: 0.9794 - val_loss: 0.4147 - val_accuracy: 0.9218\n","Epoch 60/257\n","54/54 - 698s - loss: 0.0579 - accuracy: 0.9826 - val_loss: 0.4144 - val_accuracy: 0.9253\n","INFO:tensorflow:Assets written to: /content/drive/MyDrive/data_papers/ncRNA/model_checkpoints/rnn_E_20210827221034_model_060_0.983/assets\n","Epoch 61/257\n","54/54 - 714s - loss: 0.0612 - accuracy: 0.9818 - val_loss: 0.4477 - val_accuracy: 0.9113\n","Epoch 62/257\n"]}]}]}